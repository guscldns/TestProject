{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guscldns/TestProject/blob/main/0625/%EB%AA%A8%EB%8D%B8%EB%A7%81_%EC%98%88%EC%A0%9C(commit_no).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vHoZfaXbCPi",
        "outputId": "1dad2abf-cb39-4eef-a2df-b670c0f5a5a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymysql\n",
            "  Downloading PyMySQL-1.0.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymysql\n",
            "Successfully installed pymysql-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pymysql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcWVxz_YPBZS",
        "outputId": "750e761c-c3a1-4a16-d0db-1bb06f4b1133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ],
      "source": [
        "# hugging face transformers 라이브러리 설치\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uhAELnk-2DEM"
      },
      "outputs": [],
      "source": [
        "# core\n",
        "import os, shutil, unicodedata\n",
        "import pickle, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# visualize\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm  # 폰트\n",
        "from matplotlib import rc # 폰트\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow as tf\n",
        "## keras\n",
        "from tensorflow.keras import layers, models, metrics, callbacks\n",
        "## layers & models\n",
        "from tensorflow.keras.layers import Layer, Input, Embedding, Concatenate, Flatten, Normalization\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU, Attention\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "## optimizers & callbacks\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "## preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "## utils\n",
        "from tensorflow.keras.utils import plot_model\n",
        "## saving\n",
        "from tensorflow.keras.saving import save_model\n",
        "\n",
        "# Huggingface\n",
        "from transformers import TFBertModel, BertForMaskedLM, FillMaskPipeline, TFBertForNextSentencePrediction, TFRobertaModel, AutoModel, TFBertForSequenceClassification\n",
        "\n",
        "from transformers import TFBertTokenizer, AutoTokenizer, AutoConfig\n",
        "from transformers import TFGPT2LMHeadModel\n",
        "# scikit-learn\n",
        "## models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "# Time Check\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# customization\n",
        "## font\n",
        "# plt.rc('font', family='NanumBarunGothic')\n",
        "plt.rcParams['axes.unicode_minus'] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eutb4ESx2DEO"
      },
      "source": [
        "## preprocessed Data 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CVCrNCp52jXq"
      },
      "outputs": [],
      "source": [
        "# # 데이터 불러오기\n",
        "# import pandas as pd\n",
        "# import pymysql\n",
        "\n",
        "# # Connect to the MariaDB database\n",
        "# conn = pymysql.connect(\n",
        "#     host='private.dotge.site',\n",
        "#     user='ade345',\n",
        "#     password='dbslwms123',\n",
        "#     database='books'\n",
        "# )\n",
        "# cursor = conn.cursor()\n",
        "# # Execute the SQL query to fetch the data\n",
        "# sql = \"SELECT * FROM devcon\" # SELECT * FROM devcon_test : test 파일\n",
        "# cursor.execute(sql)\n",
        "# # Fetch all the rows returned by the query\n",
        "# rows = cursor.fetchall()\n",
        "# data = []\n",
        "# # Iterate over the rows and append data to the list as dictionaries\n",
        "# for row in rows:\n",
        "#     raw_id = row[1]\n",
        "#     raw_first_party = row[2]\n",
        "#     raw_second_party = row[3]\n",
        "#     raw_facts = row[4]\n",
        "#     raw_winner = row[5]\n",
        "#     # Create a dictionary for the row\n",
        "#     row_dict = {\n",
        "#         '_id': raw_id,\n",
        "#         'raw_first_party': raw_first_party,\n",
        "#         'raw_second_party': raw_second_party,\n",
        "#         'raw_facts': raw_facts,\n",
        "#         'raw_winner': raw_winner\n",
        "#     }\n",
        "#     # Append the dictionary to the data list\n",
        "#     data.append(row_dict)\n",
        "# # Create a DataFrame from the data list\n",
        "# df = pd.DataFrame(data)\n",
        "# # Close the cursor and connection\n",
        "# cursor.close()\n",
        "# conn.close()\n",
        "# # Print the DataFrame\n",
        "# df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Kv3puEU22DEQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "5dabd2ad-f3ba-4391-efdc-efaca3bf7f10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Unnamed: 0          ID                                   first_party  \\\n",
              "0              0  TRAIN_0000                             Phil A. St. Amant   \n",
              "1              1  TRAIN_0001                                Stephen Duncan   \n",
              "2              2  TRAIN_0002                             Billy Joe Magwood   \n",
              "3              3  TRAIN_0003                                    Linkletter   \n",
              "4              4  TRAIN_0004                            William Earl Fikes   \n",
              "...          ...         ...                                           ...   \n",
              "2473        2473  TRAIN_2473  HollyFrontier Cheyenne Refining, LLC, et al.   \n",
              "2474        2474  TRAIN_2474           Grupo Mexicano de Desarrollo, S. A.   \n",
              "2475        2475  TRAIN_2475                                       Peguero   \n",
              "2476        2476  TRAIN_2476        Immigration and Naturalization Service   \n",
              "2477        2477  TRAIN_2477                                       Markman   \n",
              "\n",
              "                             second_party  \\\n",
              "0                      Herman A. Thompson   \n",
              "1                          Lawrence Owens   \n",
              "2          Tony Patterson, Warden, et al.   \n",
              "3                                  Walker   \n",
              "4                                 Alabama   \n",
              "...                                   ...   \n",
              "2473  Renewable Fuels Association, et al.   \n",
              "2474             Alliance Bond Fund, Inc.   \n",
              "2475                        United States   \n",
              "2476                              St. Cyr   \n",
              "2477           Westview Instruments, Inc.   \n",
              "\n",
              "                                                  facts  first_party_winner  \\\n",
              "0     On June 27, 1962, Phil St. Amant, a candidate ...                   1   \n",
              "1     Ramon Nelson was riding his bike when he suffe...                   0   \n",
              "2     An Alabama state court convicted Billy Joe Mag...                   1   \n",
              "3     Victor Linkletter was convicted in state court...                   0   \n",
              "4     On April 24, 1953 in Selma, Alabama, an intrud...                   1   \n",
              "...                                                 ...                 ...   \n",
              "2473  Congress amended the Clean Air Act through the...                   1   \n",
              "2474  Alliance Bond Fund, Inc., an investment fund, ...                   1   \n",
              "2475  In 1992, the District Court sentenced Manuel D...                   0   \n",
              "2476  On March 8, 1996, Enrico St. Cyr, a lawful per...                   0   \n",
              "2477  Herbert Markman owns the patent to a system th...                   0   \n",
              "\n",
              "                                            facts_split  \n",
              "0     ['On', 'June', '27,', '1962,', 'Phil', 'St.', ...  \n",
              "1     ['Ramon', 'Nelson', 'was', 'riding', 'his', 'b...  \n",
              "2     ['An', 'Alabama', 'state', 'court', 'convicted...  \n",
              "3     ['Victor', 'Linkletter', 'was', 'convicted', '...  \n",
              "4     ['On', 'April', '24,', '1953', 'in', 'Selma,',...  \n",
              "...                                                 ...  \n",
              "2473  ['Congress', 'amended', 'the', 'Clean', 'Air',...  \n",
              "2474  ['Alliance', 'Bond', 'Fund,', 'Inc.,', 'an', '...  \n",
              "2475  ['In', '1992,', 'the', 'District', 'Court', 's...  \n",
              "2476  ['On', 'March', '8,', '1996,', 'Enrico', 'St.'...  \n",
              "2477  ['Herbert', 'Markman', 'owns', 'the', 'patent'...  \n",
              "\n",
              "[2478 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86b634b6-0d28-42e6-b606-3517ba28c7df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ID</th>\n",
              "      <th>first_party</th>\n",
              "      <th>second_party</th>\n",
              "      <th>facts</th>\n",
              "      <th>first_party_winner</th>\n",
              "      <th>facts_split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>TRAIN_0000</td>\n",
              "      <td>Phil A. St. Amant</td>\n",
              "      <td>Herman A. Thompson</td>\n",
              "      <td>On June 27, 1962, Phil St. Amant, a candidate ...</td>\n",
              "      <td>1</td>\n",
              "      <td>['On', 'June', '27,', '1962,', 'Phil', 'St.', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>TRAIN_0001</td>\n",
              "      <td>Stephen Duncan</td>\n",
              "      <td>Lawrence Owens</td>\n",
              "      <td>Ramon Nelson was riding his bike when he suffe...</td>\n",
              "      <td>0</td>\n",
              "      <td>['Ramon', 'Nelson', 'was', 'riding', 'his', 'b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>TRAIN_0002</td>\n",
              "      <td>Billy Joe Magwood</td>\n",
              "      <td>Tony Patterson, Warden, et al.</td>\n",
              "      <td>An Alabama state court convicted Billy Joe Mag...</td>\n",
              "      <td>1</td>\n",
              "      <td>['An', 'Alabama', 'state', 'court', 'convicted...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>TRAIN_0003</td>\n",
              "      <td>Linkletter</td>\n",
              "      <td>Walker</td>\n",
              "      <td>Victor Linkletter was convicted in state court...</td>\n",
              "      <td>0</td>\n",
              "      <td>['Victor', 'Linkletter', 'was', 'convicted', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>TRAIN_0004</td>\n",
              "      <td>William Earl Fikes</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>On April 24, 1953 in Selma, Alabama, an intrud...</td>\n",
              "      <td>1</td>\n",
              "      <td>['On', 'April', '24,', '1953', 'in', 'Selma,',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2473</th>\n",
              "      <td>2473</td>\n",
              "      <td>TRAIN_2473</td>\n",
              "      <td>HollyFrontier Cheyenne Refining, LLC, et al.</td>\n",
              "      <td>Renewable Fuels Association, et al.</td>\n",
              "      <td>Congress amended the Clean Air Act through the...</td>\n",
              "      <td>1</td>\n",
              "      <td>['Congress', 'amended', 'the', 'Clean', 'Air',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2474</th>\n",
              "      <td>2474</td>\n",
              "      <td>TRAIN_2474</td>\n",
              "      <td>Grupo Mexicano de Desarrollo, S. A.</td>\n",
              "      <td>Alliance Bond Fund, Inc.</td>\n",
              "      <td>Alliance Bond Fund, Inc., an investment fund, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>['Alliance', 'Bond', 'Fund,', 'Inc.,', 'an', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2475</th>\n",
              "      <td>2475</td>\n",
              "      <td>TRAIN_2475</td>\n",
              "      <td>Peguero</td>\n",
              "      <td>United States</td>\n",
              "      <td>In 1992, the District Court sentenced Manuel D...</td>\n",
              "      <td>0</td>\n",
              "      <td>['In', '1992,', 'the', 'District', 'Court', 's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2476</th>\n",
              "      <td>2476</td>\n",
              "      <td>TRAIN_2476</td>\n",
              "      <td>Immigration and Naturalization Service</td>\n",
              "      <td>St. Cyr</td>\n",
              "      <td>On March 8, 1996, Enrico St. Cyr, a lawful per...</td>\n",
              "      <td>0</td>\n",
              "      <td>['On', 'March', '8,', '1996,', 'Enrico', 'St.'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2477</th>\n",
              "      <td>2477</td>\n",
              "      <td>TRAIN_2477</td>\n",
              "      <td>Markman</td>\n",
              "      <td>Westview Instruments, Inc.</td>\n",
              "      <td>Herbert Markman owns the patent to a system th...</td>\n",
              "      <td>0</td>\n",
              "      <td>['Herbert', 'Markman', 'owns', 'the', 'patent'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2478 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86b634b6-0d28-42e6-b606-3517ba28c7df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-86b634b6-0d28-42e6-b606-3517ba28c7df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-86b634b6-0d28-42e6-b606-3517ba28c7df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df = pd.read_csv('/content/preprocessed_1.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoeC0O12DER"
      },
      "source": [
        "## 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbM_6rnq2DER",
        "outputId": "97478668-c201-44d8-d024-f12324447449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"_name_or_path\": \"nlpaueb/legal-bert-base-uncased\",\n",
              "  \"architectures\": [\n",
              "    \"BertForPreTraining\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"eos_token_ids\": 0,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"output_past\": true,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.30.2\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# nlpaueb/legal-bert-small-uncased\n",
        "tokenizer_3 = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "model_3 = TFBertModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
        "config_3 = AutoConfig.from_pretrained(\"nlpaueb/legal-bert-base-uncased\") # 모델 구조 파악(레이어는 몰라도 속성 알기 좋아서 썼습니다)\n",
        "\n",
        "config_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey1j4xRo2DER",
        "outputId": "b838d162-4b03-4e2a-d45f-4836a4e053f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_3의 0번 문장:\n",
            "['[CLS]', 'on', 'june', '27', ',', '1962', ',', 'phil', 'st', '.', 'am', '##ant', ',', 'a', 'candidate', 'for', 'public', 'office', ',', 'made', 'a', 'television', 'speech', 'in', 'baton', 'roug', '##e', ',', 'louisiana', '.', 'during', 'this', 'speech', ',', 'st', '.', 'am', '##ant', 'accused', 'his', 'political', 'opponent', 'of', 'being', 'a', 'communist', 'and', 'of', 'being', 'involved', 'in', 'criminal', 'activities', 'with', 'the', 'head', 'of', 'the', 'local', 'teamster', '##s', 'union', '.', 'finally', ',', 'st', '.', 'am', '##ant', 'implicated', 'herman', 'thompson', ',', 'an', 'east', 'baton', 'roug', '##e', 'deputy', 'sheriff', ',', 'in', 'a', 'scheme', 'to', 'move', 'money', 'between', 'the', 'teamster', '##s', 'union', 'and', 'st', '.', 'am', '##ant', '[UNK]', 's', 'political', 'opponent', '.', 'thompson', 'successfully', 'sued', 'st', '.', 'am', '##ant', 'for', 'defam', '##ation', '.', 'louisiana', '[UNK]', 's', 'first', 'circuit', 'court', 'of', 'appeals', 'reversed', ',', 'holding', 'that', 'thompson', 'did', 'not', 'show', 'st', '.', 'am', '##ant', 'acted', 'with', '[UNK]', 'malic', '##e', '.', '[UNK]', 'thompson', 'the', '##n', 'appealed', 'to', 'the', 'supreme', 'court', 'of', 'louisiana', '.', 'that', 'court', 'held', 'that', ',', 'although', 'public', 'figures', 'forfeit', 'some', 'of', 'their', 'first', 'amendment', 'protection', 'from', 'defam', '##ation', ',', 'st', '.', 'am', '##ant', 'accused', 'thompson', 'of', 'a', 'crime', 'with', 'utter', 'disregard', 'of', 'whether', 'the', 'remarks', 'were', 'true', '.', 'finally', ',', 'that', 'court', 'held', 'that', 'the', 'first', 'amendment', 'protect', '##s', 'uni', '##n', '##hibited', ',', 'robust', 'debate', ',', 'rather', 'than', 'an', 'open', 'season', 'to', 'shoot', 'down', 'the', 'good', 'name', 'of', 'anyone', 'who', 'happens', 'to', 'be', 'a', 'public', 'servant', '.', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "# 토큰화\n",
        "token_3 = tokenizer_3(df['facts'].tolist(), truncation=True, padding=True, return_tensors = 'tf')\n",
        "\n",
        "# 토큰화 결과 체크([PAD] 토큰 제외)\n",
        "print(f'''token_3의 0번 문장:\\n{[token for token in token_3[0].tokens if not token == '[PAD]']}''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-vuSfr302DES"
      },
      "outputs": [],
      "source": [
        "# X, y 지정\n",
        "X_data = token_3\n",
        "y_data = df['first_party_winner'].astype(np.int32) # 그냥 astype() 좋아해서 쓴것입니다. int()써도 문제 없을거에요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mmEkPNXX2DES"
      },
      "outputs": [],
      "source": [
        "# Dataset 타입으로 변환\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(X_data),\n",
        "    y_data\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델링"
      ],
      "metadata": {
        "id": "cCuNbVVH4sgh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj00IYsf2DES",
        "outputId": "91c21c10-f336-4e40-d322-5f4d8f121f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        }
      ],
      "source": [
        "# 모델 수정\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, model_name):\n",
        "        super().__init__()\n",
        "        self.bert = TFBertModel.from_pretrained(model_name, from_pt = True) ## from_tf -> from_pt ##\n",
        "        self.classifier = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')\n",
        "\n",
        "    def call(self, input_ids = None, attention_mask=None, token_type_ids=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        cls_token = outputs[1]\n",
        "        prediction = self.classifier(cls_token)\n",
        "\n",
        "        return prediction\n",
        "\n",
        "\n",
        "# 모델 인스턴스\n",
        "model_name = \"nlpaueb/legal-bert-base-uncased\"\n",
        "model = MyModel(model_name)\n",
        "\n",
        "\n",
        "# 컴파일optimizer=Adam(learning_rate=1e4)\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e4)\n",
        "loss = tf.keras.losses.BinaryCrossentropy() ## SparseCategoricalCrossenctropy -> Binary ##\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy'])\n",
        "\n",
        "num_train = 1000\n",
        "\n",
        "# 학습\n",
        "model.fit(\n",
        "    train_dataset.shuffle(num_train).batch(16),\n",
        "    epochs=2, batch_size=16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yliABR7w2-fA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "tflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}