{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guscldns/TestProject/blob/main/0704/07_other_callback_APIs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Callbacks API 정리"
      ],
      "metadata": {
        "id": "5dE7yK_nbu8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 지금까지 다뤄본 callback class  \n",
        "1) learning 관련\n",
        "- Early Stopping  \n",
        "- CheckPoint\n",
        "- ReduceLROnPlateau  \n",
        "- LearningRateScheduler\n",
        "\n",
        "2) logging 관련\n",
        "- TensorBoard"
      ],
      "metadata": {
        "id": "8fdmP-RNb5J7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "gKMwFxKYdPKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ok5L_l3eQA0",
        "outputId": "aaaea081-1c2f-4edc-8282-5193eafc3fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단순한 모델 정의\n",
        "def create_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "            tf.keras.layers.Dense(512, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(10)\n",
        "            ])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "wq1pOETnd-0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "            loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiWQHqq5eH9V",
        "outputId": "3c3698e9-f609-4d66-cc9f-7c34dfaf77d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               401920    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(patience=2),\n",
        "        tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{loss:.2f}.h5'),\n",
        "        tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=1, min_lr=0.001)\n",
        "    ]"
      ],
      "metadata": {
        "id": "V841ohpHdSn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=x_train,\n",
        "          y=y_train,\n",
        "          epochs=3,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXmIGAZ1eLis",
        "outputId": "8de6ff02-33f1-433b-e097-c0566980b0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1875/1875 [==============================] - 15s 7ms/step - loss: 2.7193 - accuracy: 0.8749 - val_loss: 0.3394 - val_accuracy: 0.9301 - lr: 0.0010\n",
            "Epoch 2/3\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.4355 - accuracy: 0.9059 - val_loss: 0.3839 - val_accuracy: 0.9220 - lr: 0.0010\n",
            "Epoch 3/3\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.4036 - accuracy: 0.9118 - val_loss: 0.3263 - val_accuracy: 0.9359 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f86f74fef10>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other callbacks Summary  \n",
        "1) learning 관련\n",
        "- TerminateOnNaN\n",
        "\n",
        "2) logging 관련\n",
        "- BaseLogger\n",
        "- CSVLogger\n",
        "- ProgbarLogger\n",
        "- RemoteMonitor\n",
        "- History"
      ],
      "metadata": {
        "id": "n1AlKJy6cNnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  TerminateOnNaN : loss가 NaN이 뜨면 학습 중단"
      ],
      "metadata": {
        "id": "NdTdw2KFii6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BaseLogger : epoch마다 metric의 평균을 축적하는 콜백\n",
        "# CSVLogger : 결과를 CSV 파일로 스트리밍하는 콜백\n",
        "# ProgbarLogger : metric을 stdout으로 프린트하는 콜백\n",
        "# RemoteMonitor : 이벤트를 서버로 스트리밍하는 콜백\n",
        "# History : History object로 이벤트를 기록하는 콜백"
      ],
      "metadata": {
        "id": "fawOwAYj3OEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Own Callback\n"
      ],
      "metadata": {
        "id": "Sw8LzzNQz74c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BCCNoF5i3VLV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3acr98SC3yNf"
      },
      "source": [
        "다음 세가지 메서드에 콜백을 전달할 수 있다.\n",
        "\n",
        "- model.fit()\n",
        "- model.evaluate()\n",
        "- model.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yJQDf5l4Tl9"
      },
      "source": [
        "다음과 같이 정해진 시점마다 콜백을 수행하도록 구현할 수 있다.\n",
        "> fit / evaluate / predict가 시작하거나 끝날 때  \n",
        "      - on_train_begin / on_train_end  \n",
        "      - on_test_begin / on_test_end  \n",
        "      - on_predict_begin / on_predict_end  \n",
        "\n",
        "   > 각 에포크가 시작하거나 끝날 때  \n",
        "      - on_epoch_begin / on_epoch_end  \n",
        "\n",
        "   > 각 배치가 시작하거나 끝날 때  \n",
        "      - on_train_batch_begin / on_train_batch_end  \n",
        "      - on_test_batch_begin / on_test_batch_end  \n",
        "      - on_predict_batch_begin / on_predict_batch_end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Eze3prmP4zBu"
      },
      "outputs": [],
      "source": [
        "# 단순한 모델 정의\n",
        "def create_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "            tf.keras.layers.Dense(512, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "            tf.keras.layers.Dense(10)\n",
        "            ])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prlR5zPP4-Y9",
        "outputId": "d09e8f73-c52f-43b1-dc62-8e654d6cab1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               401920    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_model()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "            loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDWDUDqf5AfU",
        "outputId": "d30650ea-63aa-4cde-ef0f-ffb7e9e2f80c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlGIEVQo7bHv"
      },
      "source": [
        "## 연습하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMq9N_0U5IO0"
      },
      "outputs": [],
      "source": [
        "class CustomCallback(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"학습 시작; log 목록: {}\".format(keys))\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"학습 종료; log 목록: {}\".format(keys))\n",
        "\n",
        "    def on_test_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"테스트 시작; log 목록: {}\".format(keys))\n",
        "\n",
        "    def on_test_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"테스트 종료; log 목록: {}\".format(keys))\n",
        "\n",
        "    def on_predict_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"예측 시작; log 목록: {}\".format(keys))\n",
        "\n",
        "    def on_predict_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"예측 종료; log 목록: {}\".format(keys))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymo58-SD5tP_"
      },
      "outputs": [],
      "source": [
        "callback = CustomCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cIK2O2P5m9u",
        "outputId": "5fd6a025-fe2f-4f31-994c-77234ebbda6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습 시작; log 목록: []\n",
            "테스트 시작; log 목록: []\n",
            "테스트 종료; log 목록: ['loss', 'accuracy']\n",
            "학습 종료; log 목록: ['loss', 'accuracy', 'val_loss', 'val_accuracy']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f60b4ff8910>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=128,\n",
        "    epochs=1,\n",
        "    verbose=0,\n",
        "    validation_split=0.5,\n",
        "    callbacks=callback,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufveLwr95w2t",
        "outputId": "c225646b-f98b-42aa-ef0c-e775df4bd830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "테스트 시작; log 목록: []\n",
            "테스트 종료; log 목록: ['loss', 'accuracy']\n"
          ]
        }
      ],
      "source": [
        "result = model.evaluate(x_test, y_test, batch_size=128, verbose=0, callbacks=callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJvQ3BvB59XN",
        "outputId": "134dee5b-e625-4e6b-a496-4c172d79cc28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "예측 시작; log 목록: []\n",
            "예측 종료; log 목록: []\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(x_test, batch_size=128, callbacks=callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgLy5BRa6ere"
      },
      "source": [
        "#### loss 출력 콜백"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nOGO8dQ6l32"
      },
      "outputs": [],
      "source": [
        "class LossPrintingCallback(keras.callbacks.Callback):\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        print(\n",
        "            \"Up to batch {}, loss : {:7.2f}.\".format(batch, logs[\"loss\"])\n",
        "        )\n",
        "\n",
        "    def on_test_batch_end(self, batch, logs=None):\n",
        "        print(\n",
        "            \"Up to batch {}, loss : {:7.2f}.\".format(batch, logs[\"loss\"])\n",
        "        )\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(\"loss for epoch {} : {:7.2f} \".format(epoch, logs[\"loss\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6rGddA26elG",
        "outputId": "70de06c1-e7f0-4d69-e3b2-ddb538469f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Up to batch 0, loss :  136.11.\n",
            "Up to batch 1, loss :  105.75.\n",
            "Up to batch 2, loss :   99.95.\n",
            "Up to batch 3, loss :   92.79.\n",
            "Up to batch 4, loss :   84.07.\n",
            "Up to batch 5, loss :   76.66.\n",
            "Up to batch 6, loss :   70.01.\n",
            "Up to batch 7, loss :   65.33.\n",
            "Up to batch 8, loss :   60.31.\n",
            "Up to batch 9, loss :   56.67.\n",
            "Up to batch 10, loss :   53.88.\n",
            "Up to batch 11, loss :   50.86.\n",
            "Up to batch 12, loss :   48.51.\n",
            "Up to batch 13, loss :   46.59.\n",
            "Up to batch 14, loss :   44.42.\n",
            "Up to batch 15, loss :   42.42.\n",
            "Up to batch 16, loss :   40.88.\n",
            "Up to batch 17, loss :   39.46.\n",
            "Up to batch 18, loss :   38.19.\n",
            "Up to batch 19, loss :   37.10.\n",
            "Up to batch 20, loss :   35.93.\n",
            "Up to batch 21, loss :   35.18.\n",
            "Up to batch 22, loss :   34.06.\n",
            "Up to batch 23, loss :   33.33.\n",
            "Up to batch 24, loss :   32.62.\n",
            "Up to batch 25, loss :   31.61.\n",
            "Up to batch 26, loss :   30.71.\n",
            "Up to batch 27, loss :   30.01.\n",
            "Up to batch 28, loss :   29.31.\n",
            "Up to batch 29, loss :   28.95.\n",
            "Up to batch 30, loss :   28.46.\n",
            "Up to batch 31, loss :   27.82.\n",
            "Up to batch 32, loss :   27.30.\n",
            "Up to batch 33, loss :   26.86.\n",
            "Up to batch 34, loss :   26.45.\n",
            "Up to batch 35, loss :   25.94.\n",
            "Up to batch 36, loss :   25.46.\n",
            "Up to batch 37, loss :   25.00.\n",
            "Up to batch 38, loss :   24.51.\n",
            "Up to batch 39, loss :   24.09.\n",
            "Up to batch 40, loss :   23.76.\n",
            "Up to batch 41, loss :   23.33.\n",
            "Up to batch 42, loss :   23.12.\n",
            "Up to batch 43, loss :   22.80.\n",
            "Up to batch 44, loss :   22.45.\n",
            "Up to batch 45, loss :   22.19.\n",
            "Up to batch 46, loss :   21.96.\n",
            "Up to batch 47, loss :   21.75.\n",
            "Up to batch 48, loss :   21.52.\n",
            "Up to batch 49, loss :   21.23.\n",
            "Up to batch 50, loss :   21.04.\n",
            "Up to batch 51, loss :   20.73.\n",
            "Up to batch 52, loss :   20.46.\n",
            "Up to batch 53, loss :   20.18.\n",
            "Up to batch 54, loss :   19.94.\n",
            "Up to batch 55, loss :   19.66.\n",
            "Up to batch 56, loss :   19.43.\n",
            "Up to batch 57, loss :   19.28.\n",
            "Up to batch 58, loss :   19.04.\n",
            "Up to batch 59, loss :   18.82.\n",
            "Up to batch 60, loss :   18.60.\n",
            "Up to batch 61, loss :   18.40.\n",
            "Up to batch 62, loss :   18.22.\n",
            "Up to batch 63, loss :   18.09.\n",
            "Up to batch 64, loss :   17.88.\n",
            "Up to batch 65, loss :   17.72.\n",
            "Up to batch 66, loss :   17.51.\n",
            "Up to batch 67, loss :   17.31.\n",
            "Up to batch 68, loss :   17.13.\n",
            "Up to batch 69, loss :   16.96.\n",
            "Up to batch 70, loss :   16.76.\n",
            "Up to batch 71, loss :   16.58.\n",
            "Up to batch 72, loss :   16.40.\n",
            "Up to batch 73, loss :   16.25.\n",
            "Up to batch 74, loss :   16.10.\n",
            "Up to batch 75, loss :   16.01.\n",
            "Up to batch 76, loss :   15.86.\n",
            "Up to batch 77, loss :   15.70.\n",
            "Up to batch 78, loss :   15.62.\n",
            "Up to batch 79, loss :   15.47.\n",
            "Up to batch 80, loss :   15.35.\n",
            "Up to batch 81, loss :   15.23.\n",
            "Up to batch 82, loss :   15.12.\n",
            "Up to batch 83, loss :   15.02.\n",
            "Up to batch 84, loss :   14.87.\n",
            "Up to batch 85, loss :   14.77.\n",
            "Up to batch 86, loss :   14.65.\n",
            "Up to batch 87, loss :   14.52.\n",
            "Up to batch 88, loss :   14.39.\n",
            "Up to batch 89, loss :   14.27.\n",
            "Up to batch 90, loss :   14.15.\n",
            "Up to batch 91, loss :   14.05.\n",
            "Up to batch 92, loss :   13.96.\n",
            "Up to batch 93, loss :   13.87.\n",
            "Up to batch 94, loss :   13.78.\n",
            "Up to batch 95, loss :   13.67.\n",
            "Up to batch 96, loss :   13.58.\n",
            "Up to batch 97, loss :   13.48.\n",
            "Up to batch 98, loss :   13.43.\n",
            "Up to batch 99, loss :   13.31.\n",
            "Up to batch 100, loss :   13.23.\n",
            "Up to batch 101, loss :   13.12.\n",
            "Up to batch 102, loss :   13.04.\n",
            "Up to batch 103, loss :   12.96.\n",
            "Up to batch 104, loss :   12.87.\n",
            "Up to batch 105, loss :   12.77.\n",
            "Up to batch 106, loss :   12.68.\n",
            "Up to batch 107, loss :   12.60.\n",
            "Up to batch 108, loss :   12.54.\n",
            "Up to batch 109, loss :   12.48.\n",
            "Up to batch 110, loss :   12.41.\n",
            "Up to batch 111, loss :   12.32.\n",
            "Up to batch 112, loss :   12.24.\n",
            "Up to batch 113, loss :   12.17.\n",
            "Up to batch 114, loss :   12.10.\n",
            "Up to batch 115, loss :   12.02.\n",
            "Up to batch 116, loss :   11.94.\n",
            "Up to batch 117, loss :   11.85.\n",
            "Up to batch 118, loss :   11.78.\n",
            "Up to batch 119, loss :   11.72.\n",
            "Up to batch 120, loss :   11.65.\n",
            "Up to batch 121, loss :   11.59.\n",
            "Up to batch 122, loss :   11.52.\n",
            "Up to batch 123, loss :   11.44.\n",
            "Up to batch 124, loss :   11.38.\n",
            "Up to batch 125, loss :   11.30.\n",
            "Up to batch 126, loss :   11.24.\n",
            "Up to batch 127, loss :   11.18.\n",
            "Up to batch 128, loss :   11.13.\n",
            "Up to batch 129, loss :   11.06.\n",
            "Up to batch 130, loss :   10.99.\n",
            "Up to batch 131, loss :   10.94.\n",
            "Up to batch 132, loss :   10.90.\n",
            "Up to batch 133, loss :   10.83.\n",
            "Up to batch 134, loss :   10.79.\n",
            "Up to batch 135, loss :   10.73.\n",
            "Up to batch 136, loss :   10.67.\n",
            "Up to batch 137, loss :   10.63.\n",
            "Up to batch 138, loss :   10.58.\n",
            "Up to batch 139, loss :   10.52.\n",
            "Up to batch 140, loss :   10.46.\n",
            "Up to batch 141, loss :   10.40.\n",
            "Up to batch 142, loss :   10.33.\n",
            "Up to batch 143, loss :   10.28.\n",
            "Up to batch 144, loss :   10.24.\n",
            "Up to batch 145, loss :   10.17.\n",
            "Up to batch 146, loss :   10.13.\n",
            "Up to batch 147, loss :   10.07.\n",
            "Up to batch 148, loss :   10.02.\n",
            "Up to batch 149, loss :    9.97.\n",
            "Up to batch 150, loss :    9.92.\n",
            "Up to batch 151, loss :    9.87.\n",
            "Up to batch 152, loss :    9.82.\n",
            "Up to batch 153, loss :    9.76.\n",
            "Up to batch 154, loss :    9.72.\n",
            "Up to batch 155, loss :    9.68.\n",
            "Up to batch 156, loss :    9.64.\n",
            "Up to batch 157, loss :    9.60.\n",
            "Up to batch 158, loss :    9.56.\n",
            "Up to batch 159, loss :    9.52.\n",
            "Up to batch 160, loss :    9.47.\n",
            "Up to batch 161, loss :    9.42.\n",
            "Up to batch 162, loss :    9.38.\n",
            "Up to batch 163, loss :    9.33.\n",
            "Up to batch 164, loss :    9.29.\n",
            "Up to batch 165, loss :    9.26.\n",
            "Up to batch 166, loss :    9.22.\n",
            "Up to batch 167, loss :    9.18.\n",
            "Up to batch 168, loss :    9.14.\n",
            "Up to batch 169, loss :    9.10.\n",
            "Up to batch 170, loss :    9.06.\n",
            "Up to batch 171, loss :    9.02.\n",
            "Up to batch 172, loss :    8.99.\n",
            "Up to batch 173, loss :    8.94.\n",
            "Up to batch 174, loss :    8.91.\n",
            "Up to batch 175, loss :    8.88.\n",
            "Up to batch 176, loss :    8.84.\n",
            "Up to batch 177, loss :    8.81.\n",
            "Up to batch 178, loss :    8.77.\n",
            "Up to batch 179, loss :    8.74.\n",
            "Up to batch 180, loss :    8.70.\n",
            "Up to batch 181, loss :    8.67.\n",
            "Up to batch 182, loss :    8.63.\n",
            "Up to batch 183, loss :    8.59.\n",
            "Up to batch 184, loss :    8.56.\n",
            "Up to batch 185, loss :    8.52.\n",
            "Up to batch 186, loss :    8.49.\n",
            "Up to batch 187, loss :    8.45.\n",
            "Up to batch 188, loss :    8.42.\n",
            "Up to batch 189, loss :    8.40.\n",
            "Up to batch 190, loss :    8.36.\n",
            "Up to batch 191, loss :    8.33.\n",
            "Up to batch 192, loss :    8.30.\n",
            "Up to batch 193, loss :    8.27.\n",
            "Up to batch 194, loss :    8.25.\n",
            "Up to batch 195, loss :    8.22.\n",
            "Up to batch 196, loss :    8.19.\n",
            "Up to batch 197, loss :    8.15.\n",
            "Up to batch 198, loss :    8.12.\n",
            "Up to batch 199, loss :    8.09.\n",
            "Up to batch 200, loss :    8.06.\n",
            "Up to batch 201, loss :    8.03.\n",
            "Up to batch 202, loss :    8.00.\n",
            "Up to batch 203, loss :    7.96.\n",
            "Up to batch 204, loss :    7.93.\n",
            "Up to batch 205, loss :    7.90.\n",
            "Up to batch 206, loss :    7.87.\n",
            "Up to batch 207, loss :    7.84.\n",
            "Up to batch 208, loss :    7.82.\n",
            "Up to batch 209, loss :    7.80.\n",
            "Up to batch 210, loss :    7.77.\n",
            "Up to batch 211, loss :    7.73.\n",
            "Up to batch 212, loss :    7.70.\n",
            "Up to batch 213, loss :    7.68.\n",
            "Up to batch 214, loss :    7.65.\n",
            "Up to batch 215, loss :    7.63.\n",
            "Up to batch 216, loss :    7.60.\n",
            "Up to batch 217, loss :    7.57.\n",
            "Up to batch 218, loss :    7.54.\n",
            "Up to batch 219, loss :    7.52.\n",
            "Up to batch 220, loss :    7.49.\n",
            "Up to batch 221, loss :    7.47.\n",
            "Up to batch 222, loss :    7.44.\n",
            "Up to batch 223, loss :    7.41.\n",
            "Up to batch 224, loss :    7.39.\n",
            "Up to batch 225, loss :    7.37.\n",
            "Up to batch 226, loss :    7.34.\n",
            "Up to batch 227, loss :    7.32.\n",
            "Up to batch 228, loss :    7.29.\n",
            "Up to batch 229, loss :    7.27.\n",
            "Up to batch 230, loss :    7.24.\n",
            "Up to batch 231, loss :    7.22.\n",
            "Up to batch 232, loss :    7.19.\n",
            "Up to batch 233, loss :    7.17.\n",
            "Up to batch 234, loss :    7.15.\n",
            "Up to batch 235, loss :    7.12.\n",
            "Up to batch 236, loss :    7.10.\n",
            "Up to batch 237, loss :    7.08.\n",
            "Up to batch 238, loss :    7.06.\n",
            "Up to batch 239, loss :    7.03.\n",
            "Up to batch 240, loss :    7.01.\n",
            "Up to batch 241, loss :    6.99.\n",
            "Up to batch 242, loss :    6.97.\n",
            "Up to batch 243, loss :    6.94.\n",
            "Up to batch 244, loss :    6.91.\n",
            "Up to batch 245, loss :    6.89.\n",
            "Up to batch 246, loss :    6.87.\n",
            "Up to batch 247, loss :    6.85.\n",
            "Up to batch 248, loss :    6.83.\n",
            "Up to batch 249, loss :    6.81.\n",
            "Up to batch 250, loss :    6.79.\n",
            "Up to batch 251, loss :    6.76.\n",
            "Up to batch 252, loss :    6.74.\n",
            "Up to batch 253, loss :    6.72.\n",
            "Up to batch 254, loss :    6.70.\n",
            "Up to batch 255, loss :    6.67.\n",
            "Up to batch 256, loss :    6.66.\n",
            "Up to batch 257, loss :    6.63.\n",
            "Up to batch 258, loss :    6.61.\n",
            "Up to batch 259, loss :    6.59.\n",
            "Up to batch 260, loss :    6.58.\n",
            "Up to batch 261, loss :    6.56.\n",
            "Up to batch 262, loss :    6.54.\n",
            "Up to batch 263, loss :    6.52.\n",
            "Up to batch 264, loss :    6.50.\n",
            "Up to batch 265, loss :    6.48.\n",
            "Up to batch 266, loss :    6.46.\n",
            "Up to batch 267, loss :    6.45.\n",
            "Up to batch 268, loss :    6.43.\n",
            "Up to batch 269, loss :    6.41.\n",
            "Up to batch 270, loss :    6.39.\n",
            "Up to batch 271, loss :    6.37.\n",
            "Up to batch 272, loss :    6.35.\n",
            "Up to batch 273, loss :    6.33.\n",
            "Up to batch 274, loss :    6.31.\n",
            "Up to batch 275, loss :    6.30.\n",
            "Up to batch 276, loss :    6.28.\n",
            "Up to batch 277, loss :    6.26.\n",
            "Up to batch 278, loss :    6.24.\n",
            "Up to batch 279, loss :    6.22.\n",
            "Up to batch 280, loss :    6.20.\n",
            "Up to batch 281, loss :    6.18.\n",
            "Up to batch 282, loss :    6.17.\n",
            "Up to batch 283, loss :    6.15.\n",
            "Up to batch 284, loss :    6.14.\n",
            "Up to batch 285, loss :    6.12.\n",
            "Up to batch 286, loss :    6.11.\n",
            "Up to batch 287, loss :    6.09.\n",
            "Up to batch 288, loss :    6.07.\n",
            "Up to batch 289, loss :    6.05.\n",
            "Up to batch 290, loss :    6.04.\n",
            "Up to batch 291, loss :    6.02.\n",
            "Up to batch 292, loss :    6.01.\n",
            "Up to batch 293, loss :    5.99.\n",
            "Up to batch 294, loss :    5.97.\n",
            "Up to batch 295, loss :    5.96.\n",
            "Up to batch 296, loss :    5.94.\n",
            "Up to batch 297, loss :    5.93.\n",
            "Up to batch 298, loss :    5.92.\n",
            "Up to batch 299, loss :    5.90.\n",
            "Up to batch 300, loss :    5.88.\n",
            "Up to batch 301, loss :    5.87.\n",
            "Up to batch 302, loss :    5.85.\n",
            "Up to batch 303, loss :    5.84.\n",
            "Up to batch 304, loss :    5.82.\n",
            "Up to batch 305, loss :    5.81.\n",
            "Up to batch 306, loss :    5.79.\n",
            "Up to batch 307, loss :    5.78.\n",
            "Up to batch 308, loss :    5.76.\n",
            "Up to batch 309, loss :    5.75.\n",
            "Up to batch 310, loss :    5.74.\n",
            "Up to batch 311, loss :    5.72.\n",
            "Up to batch 312, loss :    5.71.\n",
            "Up to batch 313, loss :    5.69.\n",
            "Up to batch 314, loss :    5.68.\n",
            "Up to batch 315, loss :    5.66.\n",
            "Up to batch 316, loss :    5.65.\n",
            "Up to batch 317, loss :    5.64.\n",
            "Up to batch 318, loss :    5.62.\n",
            "Up to batch 319, loss :    5.61.\n",
            "Up to batch 320, loss :    5.59.\n",
            "Up to batch 321, loss :    5.58.\n",
            "Up to batch 322, loss :    5.57.\n",
            "Up to batch 323, loss :    5.56.\n",
            "Up to batch 324, loss :    5.55.\n",
            "Up to batch 325, loss :    5.53.\n",
            "Up to batch 326, loss :    5.52.\n",
            "Up to batch 327, loss :    5.51.\n",
            "Up to batch 328, loss :    5.50.\n",
            "Up to batch 329, loss :    5.48.\n",
            "Up to batch 330, loss :    5.47.\n",
            "Up to batch 331, loss :    5.46.\n",
            "Up to batch 332, loss :    5.44.\n",
            "Up to batch 333, loss :    5.43.\n",
            "Up to batch 334, loss :    5.41.\n",
            "Up to batch 335, loss :    5.40.\n",
            "Up to batch 336, loss :    5.39.\n",
            "Up to batch 337, loss :    5.37.\n",
            "Up to batch 338, loss :    5.36.\n",
            "Up to batch 339, loss :    5.35.\n",
            "Up to batch 340, loss :    5.33.\n",
            "Up to batch 341, loss :    5.32.\n",
            "Up to batch 342, loss :    5.31.\n",
            "Up to batch 343, loss :    5.29.\n",
            "Up to batch 344, loss :    5.28.\n",
            "Up to batch 345, loss :    5.27.\n",
            "Up to batch 346, loss :    5.26.\n",
            "Up to batch 347, loss :    5.25.\n",
            "Up to batch 348, loss :    5.23.\n",
            "Up to batch 349, loss :    5.22.\n",
            "Up to batch 350, loss :    5.21.\n",
            "Up to batch 351, loss :    5.20.\n",
            "Up to batch 352, loss :    5.18.\n",
            "Up to batch 353, loss :    5.18.\n",
            "Up to batch 354, loss :    5.16.\n",
            "Up to batch 355, loss :    5.16.\n",
            "Up to batch 356, loss :    5.15.\n",
            "Up to batch 357, loss :    5.14.\n",
            "Up to batch 358, loss :    5.12.\n",
            "Up to batch 359, loss :    5.11.\n",
            "Up to batch 360, loss :    5.10.\n",
            "Up to batch 361, loss :    5.09.\n",
            "Up to batch 362, loss :    5.08.\n",
            "Up to batch 363, loss :    5.07.\n",
            "Up to batch 364, loss :    5.05.\n",
            "Up to batch 365, loss :    5.04.\n",
            "Up to batch 366, loss :    5.03.\n",
            "Up to batch 367, loss :    5.02.\n",
            "Up to batch 368, loss :    5.01.\n",
            "Up to batch 369, loss :    5.00.\n",
            "Up to batch 370, loss :    4.99.\n",
            "Up to batch 371, loss :    4.98.\n",
            "Up to batch 372, loss :    4.97.\n",
            "Up to batch 373, loss :    4.95.\n",
            "Up to batch 374, loss :    4.94.\n",
            "Up to batch 375, loss :    4.93.\n",
            "Up to batch 376, loss :    4.92.\n",
            "Up to batch 377, loss :    4.91.\n",
            "Up to batch 378, loss :    4.90.\n",
            "Up to batch 379, loss :    4.89.\n",
            "Up to batch 380, loss :    4.88.\n",
            "Up to batch 381, loss :    4.87.\n",
            "Up to batch 382, loss :    4.86.\n",
            "Up to batch 383, loss :    4.85.\n",
            "Up to batch 384, loss :    4.84.\n",
            "Up to batch 385, loss :    4.83.\n",
            "Up to batch 386, loss :    4.82.\n",
            "Up to batch 387, loss :    4.80.\n",
            "Up to batch 388, loss :    4.79.\n",
            "Up to batch 389, loss :    4.78.\n",
            "Up to batch 390, loss :    4.78.\n",
            "Up to batch 391, loss :    4.77.\n",
            "Up to batch 392, loss :    4.76.\n",
            "Up to batch 393, loss :    4.75.\n",
            "Up to batch 394, loss :    4.74.\n",
            "Up to batch 395, loss :    4.73.\n",
            "Up to batch 396, loss :    4.72.\n",
            "Up to batch 397, loss :    4.71.\n",
            "Up to batch 398, loss :    4.70.\n",
            "Up to batch 399, loss :    4.69.\n",
            "Up to batch 400, loss :    4.68.\n",
            "Up to batch 401, loss :    4.67.\n",
            "Up to batch 402, loss :    4.66.\n",
            "Up to batch 403, loss :    4.65.\n",
            "Up to batch 404, loss :    4.64.\n",
            "Up to batch 405, loss :    4.63.\n",
            "Up to batch 406, loss :    4.62.\n",
            "Up to batch 407, loss :    4.61.\n",
            "Up to batch 408, loss :    4.60.\n",
            "Up to batch 409, loss :    4.59.\n",
            "Up to batch 410, loss :    4.59.\n",
            "Up to batch 411, loss :    4.58.\n",
            "Up to batch 412, loss :    4.57.\n",
            "Up to batch 413, loss :    4.56.\n",
            "Up to batch 414, loss :    4.55.\n",
            "Up to batch 415, loss :    4.54.\n",
            "Up to batch 416, loss :    4.54.\n",
            "Up to batch 417, loss :    4.53.\n",
            "Up to batch 418, loss :    4.52.\n",
            "Up to batch 419, loss :    4.51.\n",
            "Up to batch 420, loss :    4.50.\n",
            "Up to batch 421, loss :    4.50.\n",
            "Up to batch 422, loss :    4.49.\n",
            "Up to batch 423, loss :    4.48.\n",
            "Up to batch 424, loss :    4.47.\n",
            "Up to batch 425, loss :    4.47.\n",
            "Up to batch 426, loss :    4.46.\n",
            "Up to batch 427, loss :    4.45.\n",
            "Up to batch 428, loss :    4.44.\n",
            "Up to batch 429, loss :    4.43.\n",
            "Up to batch 430, loss :    4.43.\n",
            "Up to batch 431, loss :    4.42.\n",
            "Up to batch 432, loss :    4.41.\n",
            "Up to batch 433, loss :    4.40.\n",
            "Up to batch 434, loss :    4.40.\n",
            "Up to batch 435, loss :    4.39.\n",
            "Up to batch 436, loss :    4.38.\n",
            "Up to batch 437, loss :    4.37.\n",
            "Up to batch 438, loss :    4.37.\n",
            "Up to batch 439, loss :    4.36.\n",
            "Up to batch 440, loss :    4.35.\n",
            "Up to batch 441, loss :    4.34.\n",
            "Up to batch 442, loss :    4.33.\n",
            "Up to batch 443, loss :    4.32.\n",
            "Up to batch 444, loss :    4.31.\n",
            "Up to batch 445, loss :    4.31.\n",
            "Up to batch 446, loss :    4.30.\n",
            "Up to batch 447, loss :    4.29.\n",
            "Up to batch 448, loss :    4.28.\n",
            "Up to batch 449, loss :    4.27.\n",
            "Up to batch 450, loss :    4.27.\n",
            "Up to batch 451, loss :    4.26.\n",
            "Up to batch 452, loss :    4.25.\n",
            "Up to batch 453, loss :    4.24.\n",
            "Up to batch 454, loss :    4.24.\n",
            "Up to batch 455, loss :    4.23.\n",
            "Up to batch 456, loss :    4.22.\n",
            "Up to batch 457, loss :    4.21.\n",
            "Up to batch 458, loss :    4.20.\n",
            "Up to batch 459, loss :    4.20.\n",
            "Up to batch 460, loss :    4.19.\n",
            "Up to batch 461, loss :    4.18.\n",
            "Up to batch 462, loss :    4.17.\n",
            "Up to batch 463, loss :    4.17.\n",
            "Up to batch 464, loss :    4.16.\n",
            "Up to batch 465, loss :    4.15.\n",
            "Up to batch 466, loss :    4.14.\n",
            "Up to batch 467, loss :    4.14.\n",
            "Up to batch 468, loss :    4.13.\n",
            "loss for epoch 0 :    4.13 \n",
            "Up to batch 0, loss :    0.83.\n",
            "Up to batch 1, loss :    0.63.\n",
            "Up to batch 2, loss :    0.61.\n",
            "Up to batch 3, loss :    0.54.\n",
            "Up to batch 4, loss :    0.57.\n",
            "Up to batch 5, loss :    0.52.\n",
            "Up to batch 6, loss :    0.50.\n",
            "Up to batch 7, loss :    0.50.\n",
            "Up to batch 8, loss :    0.50.\n",
            "Up to batch 9, loss :    0.53.\n",
            "Up to batch 10, loss :    0.52.\n",
            "Up to batch 11, loss :    0.52.\n",
            "Up to batch 12, loss :    0.51.\n",
            "Up to batch 13, loss :    0.50.\n",
            "Up to batch 14, loss :    0.52.\n",
            "Up to batch 15, loss :    0.53.\n",
            "Up to batch 16, loss :    0.53.\n",
            "Up to batch 17, loss :    0.57.\n",
            "Up to batch 18, loss :    0.59.\n",
            "Up to batch 19, loss :    0.57.\n",
            "Up to batch 20, loss :    0.56.\n",
            "Up to batch 21, loss :    0.57.\n",
            "Up to batch 22, loss :    0.56.\n",
            "Up to batch 23, loss :    0.54.\n",
            "Up to batch 24, loss :    0.54.\n",
            "Up to batch 25, loss :    0.53.\n",
            "Up to batch 26, loss :    0.52.\n",
            "Up to batch 27, loss :    0.51.\n",
            "Up to batch 28, loss :    0.51.\n",
            "Up to batch 29, loss :    0.51.\n",
            "Up to batch 30, loss :    0.51.\n",
            "Up to batch 31, loss :    0.52.\n",
            "Up to batch 32, loss :    0.51.\n",
            "Up to batch 33, loss :    0.51.\n",
            "Up to batch 34, loss :    0.51.\n",
            "Up to batch 35, loss :    0.51.\n",
            "Up to batch 36, loss :    0.51.\n",
            "Up to batch 37, loss :    0.51.\n",
            "Up to batch 38, loss :    0.50.\n",
            "Up to batch 39, loss :    0.51.\n",
            "Up to batch 40, loss :    0.51.\n",
            "Up to batch 41, loss :    0.50.\n",
            "Up to batch 42, loss :    0.51.\n",
            "Up to batch 43, loss :    0.53.\n",
            "Up to batch 44, loss :    0.53.\n",
            "Up to batch 45, loss :    0.53.\n",
            "Up to batch 46, loss :    0.52.\n",
            "Up to batch 47, loss :    0.53.\n",
            "Up to batch 48, loss :    0.52.\n",
            "Up to batch 49, loss :    0.53.\n",
            "Up to batch 50, loss :    0.52.\n",
            "Up to batch 51, loss :    0.52.\n",
            "Up to batch 52, loss :    0.51.\n",
            "Up to batch 53, loss :    0.52.\n",
            "Up to batch 54, loss :    0.52.\n",
            "Up to batch 55, loss :    0.52.\n",
            "Up to batch 56, loss :    0.53.\n",
            "Up to batch 57, loss :    0.52.\n",
            "Up to batch 58, loss :    0.52.\n",
            "Up to batch 59, loss :    0.52.\n",
            "Up to batch 60, loss :    0.52.\n",
            "Up to batch 61, loss :    0.51.\n",
            "Up to batch 62, loss :    0.51.\n",
            "Up to batch 63, loss :    0.51.\n",
            "Up to batch 64, loss :    0.51.\n",
            "Up to batch 65, loss :    0.51.\n",
            "Up to batch 66, loss :    0.50.\n",
            "Up to batch 67, loss :    0.50.\n",
            "Up to batch 68, loss :    0.50.\n",
            "Up to batch 69, loss :    0.50.\n",
            "Up to batch 70, loss :    0.50.\n",
            "Up to batch 71, loss :    0.50.\n",
            "Up to batch 72, loss :    0.50.\n",
            "Up to batch 73, loss :    0.50.\n",
            "Up to batch 74, loss :    0.50.\n",
            "Up to batch 75, loss :    0.50.\n",
            "Up to batch 76, loss :    0.49.\n",
            "Up to batch 77, loss :    0.50.\n",
            "Up to batch 78, loss :    0.49.\n",
            "Up to batch 79, loss :    0.50.\n",
            "Up to batch 80, loss :    0.50.\n",
            "Up to batch 81, loss :    0.50.\n",
            "Up to batch 82, loss :    0.50.\n",
            "Up to batch 83, loss :    0.51.\n",
            "Up to batch 84, loss :    0.50.\n",
            "Up to batch 85, loss :    0.51.\n",
            "Up to batch 86, loss :    0.51.\n",
            "Up to batch 87, loss :    0.51.\n",
            "Up to batch 88, loss :    0.51.\n",
            "Up to batch 89, loss :    0.51.\n",
            "Up to batch 90, loss :    0.52.\n",
            "Up to batch 91, loss :    0.52.\n",
            "Up to batch 92, loss :    0.53.\n",
            "Up to batch 93, loss :    0.53.\n",
            "Up to batch 94, loss :    0.53.\n",
            "Up to batch 95, loss :    0.53.\n",
            "Up to batch 96, loss :    0.52.\n",
            "Up to batch 97, loss :    0.52.\n",
            "Up to batch 98, loss :    0.52.\n",
            "Up to batch 99, loss :    0.52.\n",
            "Up to batch 100, loss :    0.52.\n",
            "Up to batch 101, loss :    0.52.\n",
            "Up to batch 102, loss :    0.52.\n",
            "Up to batch 103, loss :    0.52.\n",
            "Up to batch 104, loss :    0.51.\n",
            "Up to batch 105, loss :    0.51.\n",
            "Up to batch 106, loss :    0.52.\n",
            "Up to batch 107, loss :    0.52.\n",
            "Up to batch 108, loss :    0.52.\n",
            "Up to batch 109, loss :    0.53.\n",
            "Up to batch 110, loss :    0.52.\n",
            "Up to batch 111, loss :    0.52.\n",
            "Up to batch 112, loss :    0.52.\n",
            "Up to batch 113, loss :    0.53.\n",
            "Up to batch 114, loss :    0.52.\n",
            "Up to batch 115, loss :    0.52.\n",
            "Up to batch 116, loss :    0.52.\n",
            "Up to batch 117, loss :    0.52.\n",
            "Up to batch 118, loss :    0.52.\n",
            "Up to batch 119, loss :    0.52.\n",
            "Up to batch 120, loss :    0.52.\n",
            "Up to batch 121, loss :    0.52.\n",
            "Up to batch 122, loss :    0.52.\n",
            "Up to batch 123, loss :    0.52.\n",
            "Up to batch 124, loss :    0.52.\n",
            "Up to batch 125, loss :    0.52.\n",
            "Up to batch 126, loss :    0.52.\n",
            "Up to batch 127, loss :    0.52.\n",
            "Up to batch 128, loss :    0.52.\n",
            "Up to batch 129, loss :    0.51.\n",
            "Up to batch 130, loss :    0.51.\n",
            "Up to batch 131, loss :    0.51.\n",
            "Up to batch 132, loss :    0.51.\n",
            "Up to batch 133, loss :    0.51.\n",
            "Up to batch 134, loss :    0.51.\n",
            "Up to batch 135, loss :    0.51.\n",
            "Up to batch 136, loss :    0.51.\n",
            "Up to batch 137, loss :    0.51.\n",
            "Up to batch 138, loss :    0.51.\n",
            "Up to batch 139, loss :    0.51.\n",
            "Up to batch 140, loss :    0.52.\n",
            "Up to batch 141, loss :    0.52.\n",
            "Up to batch 142, loss :    0.52.\n",
            "Up to batch 143, loss :    0.52.\n",
            "Up to batch 144, loss :    0.52.\n",
            "Up to batch 145, loss :    0.52.\n",
            "Up to batch 146, loss :    0.52.\n",
            "Up to batch 147, loss :    0.52.\n",
            "Up to batch 148, loss :    0.52.\n",
            "Up to batch 149, loss :    0.52.\n",
            "Up to batch 150, loss :    0.51.\n",
            "Up to batch 151, loss :    0.51.\n",
            "Up to batch 152, loss :    0.52.\n",
            "Up to batch 153, loss :    0.52.\n",
            "Up to batch 154, loss :    0.51.\n",
            "Up to batch 155, loss :    0.52.\n",
            "Up to batch 156, loss :    0.51.\n",
            "Up to batch 157, loss :    0.52.\n",
            "Up to batch 158, loss :    0.52.\n",
            "Up to batch 159, loss :    0.52.\n",
            "Up to batch 160, loss :    0.52.\n",
            "Up to batch 161, loss :    0.52.\n",
            "Up to batch 162, loss :    0.52.\n",
            "Up to batch 163, loss :    0.52.\n",
            "Up to batch 164, loss :    0.52.\n",
            "Up to batch 165, loss :    0.52.\n",
            "Up to batch 166, loss :    0.52.\n",
            "Up to batch 167, loss :    0.52.\n",
            "Up to batch 168, loss :    0.52.\n",
            "Up to batch 169, loss :    0.52.\n",
            "Up to batch 170, loss :    0.52.\n",
            "Up to batch 171, loss :    0.52.\n",
            "Up to batch 172, loss :    0.52.\n",
            "Up to batch 173, loss :    0.52.\n",
            "Up to batch 174, loss :    0.52.\n",
            "Up to batch 175, loss :    0.52.\n",
            "Up to batch 176, loss :    0.52.\n",
            "Up to batch 177, loss :    0.51.\n",
            "Up to batch 178, loss :    0.51.\n",
            "Up to batch 179, loss :    0.52.\n",
            "Up to batch 180, loss :    0.52.\n",
            "Up to batch 181, loss :    0.51.\n",
            "Up to batch 182, loss :    0.51.\n",
            "Up to batch 183, loss :    0.51.\n",
            "Up to batch 184, loss :    0.51.\n",
            "Up to batch 185, loss :    0.51.\n",
            "Up to batch 186, loss :    0.51.\n",
            "Up to batch 187, loss :    0.51.\n",
            "Up to batch 188, loss :    0.51.\n",
            "Up to batch 189, loss :    0.51.\n",
            "Up to batch 190, loss :    0.51.\n",
            "Up to batch 191, loss :    0.51.\n",
            "Up to batch 192, loss :    0.51.\n",
            "Up to batch 193, loss :    0.51.\n",
            "Up to batch 194, loss :    0.51.\n",
            "Up to batch 195, loss :    0.51.\n",
            "Up to batch 196, loss :    0.51.\n",
            "Up to batch 197, loss :    0.51.\n",
            "Up to batch 198, loss :    0.51.\n",
            "Up to batch 199, loss :    0.51.\n",
            "Up to batch 200, loss :    0.51.\n",
            "Up to batch 201, loss :    0.51.\n",
            "Up to batch 202, loss :    0.51.\n",
            "Up to batch 203, loss :    0.51.\n",
            "Up to batch 204, loss :    0.50.\n",
            "Up to batch 205, loss :    0.50.\n",
            "Up to batch 206, loss :    0.50.\n",
            "Up to batch 207, loss :    0.50.\n",
            "Up to batch 208, loss :    0.50.\n",
            "Up to batch 209, loss :    0.50.\n",
            "Up to batch 210, loss :    0.50.\n",
            "Up to batch 211, loss :    0.50.\n",
            "Up to batch 212, loss :    0.50.\n",
            "Up to batch 213, loss :    0.50.\n",
            "Up to batch 214, loss :    0.50.\n",
            "Up to batch 215, loss :    0.50.\n",
            "Up to batch 216, loss :    0.50.\n",
            "Up to batch 217, loss :    0.50.\n",
            "Up to batch 218, loss :    0.50.\n",
            "Up to batch 219, loss :    0.50.\n",
            "Up to batch 220, loss :    0.50.\n",
            "Up to batch 221, loss :    0.50.\n",
            "Up to batch 222, loss :    0.50.\n",
            "Up to batch 223, loss :    0.50.\n",
            "Up to batch 224, loss :    0.51.\n",
            "Up to batch 225, loss :    0.51.\n",
            "Up to batch 226, loss :    0.50.\n",
            "Up to batch 227, loss :    0.50.\n",
            "Up to batch 228, loss :    0.50.\n",
            "Up to batch 229, loss :    0.50.\n",
            "Up to batch 230, loss :    0.50.\n",
            "Up to batch 231, loss :    0.50.\n",
            "Up to batch 232, loss :    0.50.\n",
            "Up to batch 233, loss :    0.50.\n",
            "Up to batch 234, loss :    0.50.\n",
            "Up to batch 235, loss :    0.50.\n",
            "Up to batch 236, loss :    0.50.\n",
            "Up to batch 237, loss :    0.50.\n",
            "Up to batch 238, loss :    0.50.\n",
            "Up to batch 239, loss :    0.50.\n",
            "Up to batch 240, loss :    0.50.\n",
            "Up to batch 241, loss :    0.50.\n",
            "Up to batch 242, loss :    0.50.\n",
            "Up to batch 243, loss :    0.50.\n",
            "Up to batch 244, loss :    0.50.\n",
            "Up to batch 245, loss :    0.50.\n",
            "Up to batch 246, loss :    0.50.\n",
            "Up to batch 247, loss :    0.50.\n",
            "Up to batch 248, loss :    0.50.\n",
            "Up to batch 249, loss :    0.50.\n",
            "Up to batch 250, loss :    0.50.\n",
            "Up to batch 251, loss :    0.50.\n",
            "Up to batch 252, loss :    0.50.\n",
            "Up to batch 253, loss :    0.50.\n",
            "Up to batch 254, loss :    0.50.\n",
            "Up to batch 255, loss :    0.50.\n",
            "Up to batch 256, loss :    0.50.\n",
            "Up to batch 257, loss :    0.50.\n",
            "Up to batch 258, loss :    0.50.\n",
            "Up to batch 259, loss :    0.50.\n",
            "Up to batch 260, loss :    0.50.\n",
            "Up to batch 261, loss :    0.50.\n",
            "Up to batch 262, loss :    0.50.\n",
            "Up to batch 263, loss :    0.50.\n",
            "Up to batch 264, loss :    0.50.\n",
            "Up to batch 265, loss :    0.50.\n",
            "Up to batch 266, loss :    0.49.\n",
            "Up to batch 267, loss :    0.49.\n",
            "Up to batch 268, loss :    0.49.\n",
            "Up to batch 269, loss :    0.49.\n",
            "Up to batch 270, loss :    0.49.\n",
            "Up to batch 271, loss :    0.49.\n",
            "Up to batch 272, loss :    0.49.\n",
            "Up to batch 273, loss :    0.49.\n",
            "Up to batch 274, loss :    0.49.\n",
            "Up to batch 275, loss :    0.49.\n",
            "Up to batch 276, loss :    0.49.\n",
            "Up to batch 277, loss :    0.49.\n",
            "Up to batch 278, loss :    0.49.\n",
            "Up to batch 279, loss :    0.49.\n",
            "Up to batch 280, loss :    0.49.\n",
            "Up to batch 281, loss :    0.49.\n",
            "Up to batch 282, loss :    0.49.\n",
            "Up to batch 283, loss :    0.49.\n",
            "Up to batch 284, loss :    0.49.\n",
            "Up to batch 285, loss :    0.49.\n",
            "Up to batch 286, loss :    0.49.\n",
            "Up to batch 287, loss :    0.49.\n",
            "Up to batch 288, loss :    0.49.\n",
            "Up to batch 289, loss :    0.49.\n",
            "Up to batch 290, loss :    0.49.\n",
            "Up to batch 291, loss :    0.49.\n",
            "Up to batch 292, loss :    0.50.\n",
            "Up to batch 293, loss :    0.49.\n",
            "Up to batch 294, loss :    0.50.\n",
            "Up to batch 295, loss :    0.49.\n",
            "Up to batch 296, loss :    0.49.\n",
            "Up to batch 297, loss :    0.49.\n",
            "Up to batch 298, loss :    0.49.\n",
            "Up to batch 299, loss :    0.49.\n",
            "Up to batch 300, loss :    0.49.\n",
            "Up to batch 301, loss :    0.49.\n",
            "Up to batch 302, loss :    0.49.\n",
            "Up to batch 303, loss :    0.50.\n",
            "Up to batch 304, loss :    0.49.\n",
            "Up to batch 305, loss :    0.50.\n",
            "Up to batch 306, loss :    0.50.\n",
            "Up to batch 307, loss :    0.50.\n",
            "Up to batch 308, loss :    0.50.\n",
            "Up to batch 309, loss :    0.50.\n",
            "Up to batch 310, loss :    0.50.\n",
            "Up to batch 311, loss :    0.50.\n",
            "Up to batch 312, loss :    0.49.\n",
            "Up to batch 313, loss :    0.50.\n",
            "Up to batch 314, loss :    0.50.\n",
            "Up to batch 315, loss :    0.50.\n",
            "Up to batch 316, loss :    0.49.\n",
            "Up to batch 317, loss :    0.49.\n",
            "Up to batch 318, loss :    0.49.\n",
            "Up to batch 319, loss :    0.49.\n",
            "Up to batch 320, loss :    0.49.\n",
            "Up to batch 321, loss :    0.49.\n",
            "Up to batch 322, loss :    0.49.\n",
            "Up to batch 323, loss :    0.49.\n",
            "Up to batch 324, loss :    0.49.\n",
            "Up to batch 325, loss :    0.49.\n",
            "Up to batch 326, loss :    0.49.\n",
            "Up to batch 327, loss :    0.49.\n",
            "Up to batch 328, loss :    0.49.\n",
            "Up to batch 329, loss :    0.49.\n",
            "Up to batch 330, loss :    0.49.\n",
            "Up to batch 331, loss :    0.49.\n",
            "Up to batch 332, loss :    0.49.\n",
            "Up to batch 333, loss :    0.49.\n",
            "Up to batch 334, loss :    0.49.\n",
            "Up to batch 335, loss :    0.49.\n",
            "Up to batch 336, loss :    0.49.\n",
            "Up to batch 337, loss :    0.49.\n",
            "Up to batch 338, loss :    0.49.\n",
            "Up to batch 339, loss :    0.49.\n",
            "Up to batch 340, loss :    0.49.\n",
            "Up to batch 341, loss :    0.49.\n",
            "Up to batch 342, loss :    0.49.\n",
            "Up to batch 343, loss :    0.49.\n",
            "Up to batch 344, loss :    0.49.\n",
            "Up to batch 345, loss :    0.48.\n",
            "Up to batch 346, loss :    0.49.\n",
            "Up to batch 347, loss :    0.49.\n",
            "Up to batch 348, loss :    0.49.\n",
            "Up to batch 349, loss :    0.49.\n",
            "Up to batch 350, loss :    0.49.\n",
            "Up to batch 351, loss :    0.49.\n",
            "Up to batch 352, loss :    0.49.\n",
            "Up to batch 353, loss :    0.49.\n",
            "Up to batch 354, loss :    0.49.\n",
            "Up to batch 355, loss :    0.48.\n",
            "Up to batch 356, loss :    0.48.\n",
            "Up to batch 357, loss :    0.48.\n",
            "Up to batch 358, loss :    0.48.\n",
            "Up to batch 359, loss :    0.48.\n",
            "Up to batch 360, loss :    0.48.\n",
            "Up to batch 361, loss :    0.48.\n",
            "Up to batch 362, loss :    0.48.\n",
            "Up to batch 363, loss :    0.48.\n",
            "Up to batch 364, loss :    0.48.\n",
            "Up to batch 365, loss :    0.48.\n",
            "Up to batch 366, loss :    0.48.\n",
            "Up to batch 367, loss :    0.48.\n",
            "Up to batch 368, loss :    0.48.\n",
            "Up to batch 369, loss :    0.49.\n",
            "Up to batch 370, loss :    0.49.\n",
            "Up to batch 371, loss :    0.49.\n",
            "Up to batch 372, loss :    0.48.\n",
            "Up to batch 373, loss :    0.48.\n",
            "Up to batch 374, loss :    0.48.\n",
            "Up to batch 375, loss :    0.48.\n",
            "Up to batch 376, loss :    0.48.\n",
            "Up to batch 377, loss :    0.48.\n",
            "Up to batch 378, loss :    0.48.\n",
            "Up to batch 379, loss :    0.48.\n",
            "Up to batch 380, loss :    0.48.\n",
            "Up to batch 381, loss :    0.48.\n",
            "Up to batch 382, loss :    0.48.\n",
            "Up to batch 383, loss :    0.48.\n",
            "Up to batch 384, loss :    0.48.\n",
            "Up to batch 385, loss :    0.48.\n",
            "Up to batch 386, loss :    0.48.\n",
            "Up to batch 387, loss :    0.48.\n",
            "Up to batch 388, loss :    0.48.\n",
            "Up to batch 389, loss :    0.48.\n",
            "Up to batch 390, loss :    0.48.\n",
            "Up to batch 391, loss :    0.48.\n",
            "Up to batch 392, loss :    0.48.\n",
            "Up to batch 393, loss :    0.48.\n",
            "Up to batch 394, loss :    0.48.\n",
            "Up to batch 395, loss :    0.48.\n",
            "Up to batch 396, loss :    0.48.\n",
            "Up to batch 397, loss :    0.48.\n",
            "Up to batch 398, loss :    0.48.\n",
            "Up to batch 399, loss :    0.48.\n",
            "Up to batch 400, loss :    0.48.\n",
            "Up to batch 401, loss :    0.48.\n",
            "Up to batch 402, loss :    0.48.\n",
            "Up to batch 403, loss :    0.48.\n",
            "Up to batch 404, loss :    0.48.\n",
            "Up to batch 405, loss :    0.49.\n",
            "Up to batch 406, loss :    0.49.\n",
            "Up to batch 407, loss :    0.49.\n",
            "Up to batch 408, loss :    0.49.\n",
            "Up to batch 409, loss :    0.48.\n",
            "Up to batch 410, loss :    0.48.\n",
            "Up to batch 411, loss :    0.48.\n",
            "Up to batch 412, loss :    0.48.\n",
            "Up to batch 413, loss :    0.48.\n",
            "Up to batch 414, loss :    0.48.\n",
            "Up to batch 415, loss :    0.48.\n",
            "Up to batch 416, loss :    0.48.\n",
            "Up to batch 417, loss :    0.48.\n",
            "Up to batch 418, loss :    0.48.\n",
            "Up to batch 419, loss :    0.48.\n",
            "Up to batch 420, loss :    0.48.\n",
            "Up to batch 421, loss :    0.48.\n",
            "Up to batch 422, loss :    0.48.\n",
            "Up to batch 423, loss :    0.48.\n",
            "Up to batch 424, loss :    0.48.\n",
            "Up to batch 425, loss :    0.48.\n",
            "Up to batch 426, loss :    0.48.\n",
            "Up to batch 427, loss :    0.48.\n",
            "Up to batch 428, loss :    0.48.\n",
            "Up to batch 429, loss :    0.48.\n",
            "Up to batch 430, loss :    0.48.\n",
            "Up to batch 431, loss :    0.48.\n",
            "Up to batch 432, loss :    0.48.\n",
            "Up to batch 433, loss :    0.48.\n",
            "Up to batch 434, loss :    0.48.\n",
            "Up to batch 435, loss :    0.48.\n",
            "Up to batch 436, loss :    0.48.\n",
            "Up to batch 437, loss :    0.48.\n",
            "Up to batch 438, loss :    0.48.\n",
            "Up to batch 439, loss :    0.48.\n",
            "Up to batch 440, loss :    0.48.\n",
            "Up to batch 441, loss :    0.48.\n",
            "Up to batch 442, loss :    0.48.\n",
            "Up to batch 443, loss :    0.48.\n",
            "Up to batch 444, loss :    0.48.\n",
            "Up to batch 445, loss :    0.48.\n",
            "Up to batch 446, loss :    0.48.\n",
            "Up to batch 447, loss :    0.48.\n",
            "Up to batch 448, loss :    0.48.\n",
            "Up to batch 449, loss :    0.48.\n",
            "Up to batch 450, loss :    0.47.\n",
            "Up to batch 451, loss :    0.47.\n",
            "Up to batch 452, loss :    0.47.\n",
            "Up to batch 453, loss :    0.47.\n",
            "Up to batch 454, loss :    0.47.\n",
            "Up to batch 455, loss :    0.47.\n",
            "Up to batch 456, loss :    0.47.\n",
            "Up to batch 457, loss :    0.47.\n",
            "Up to batch 458, loss :    0.47.\n",
            "Up to batch 459, loss :    0.47.\n",
            "Up to batch 460, loss :    0.47.\n",
            "Up to batch 461, loss :    0.47.\n",
            "Up to batch 462, loss :    0.47.\n",
            "Up to batch 463, loss :    0.47.\n",
            "Up to batch 464, loss :    0.47.\n",
            "Up to batch 465, loss :    0.47.\n",
            "Up to batch 466, loss :    0.47.\n",
            "Up to batch 467, loss :    0.47.\n",
            "Up to batch 468, loss :    0.47.\n",
            "loss for epoch 1 :    0.47 \n",
            "Up to batch 0, loss :    0.12.\n",
            "Up to batch 1, loss :    0.23.\n",
            "Up to batch 2, loss :    0.28.\n",
            "Up to batch 3, loss :    0.26.\n",
            "Up to batch 4, loss :    0.26.\n",
            "Up to batch 5, loss :    0.27.\n",
            "Up to batch 6, loss :    0.26.\n",
            "Up to batch 7, loss :    0.31.\n",
            "Up to batch 8, loss :    0.32.\n",
            "Up to batch 9, loss :    0.35.\n",
            "Up to batch 10, loss :    0.37.\n",
            "Up to batch 11, loss :    0.38.\n",
            "Up to batch 12, loss :    0.37.\n",
            "Up to batch 13, loss :    0.38.\n",
            "Up to batch 14, loss :    0.38.\n",
            "Up to batch 15, loss :    0.40.\n",
            "Up to batch 16, loss :    0.41.\n",
            "Up to batch 17, loss :    0.42.\n",
            "Up to batch 18, loss :    0.43.\n",
            "Up to batch 19, loss :    0.42.\n",
            "Up to batch 20, loss :    0.44.\n",
            "Up to batch 21, loss :    0.43.\n",
            "Up to batch 22, loss :    0.42.\n",
            "Up to batch 23, loss :    0.41.\n",
            "Up to batch 24, loss :    0.40.\n",
            "Up to batch 25, loss :    0.40.\n",
            "Up to batch 26, loss :    0.39.\n",
            "Up to batch 27, loss :    0.41.\n",
            "Up to batch 28, loss :    0.41.\n",
            "Up to batch 29, loss :    0.42.\n",
            "Up to batch 30, loss :    0.42.\n",
            "Up to batch 31, loss :    0.42.\n",
            "Up to batch 32, loss :    0.42.\n",
            "Up to batch 33, loss :    0.43.\n",
            "Up to batch 34, loss :    0.43.\n",
            "Up to batch 35, loss :    0.43.\n",
            "Up to batch 36, loss :    0.42.\n",
            "Up to batch 37, loss :    0.42.\n",
            "Up to batch 38, loss :    0.41.\n",
            "Up to batch 39, loss :    0.40.\n",
            "Up to batch 40, loss :    0.40.\n",
            "Up to batch 41, loss :    0.39.\n",
            "Up to batch 42, loss :    0.39.\n",
            "Up to batch 43, loss :    0.38.\n",
            "Up to batch 44, loss :    0.38.\n",
            "Up to batch 45, loss :    0.38.\n",
            "Up to batch 46, loss :    0.39.\n",
            "Up to batch 47, loss :    0.39.\n",
            "Up to batch 48, loss :    0.39.\n",
            "Up to batch 49, loss :    0.38.\n",
            "Up to batch 50, loss :    0.38.\n",
            "Up to batch 51, loss :    0.39.\n",
            "Up to batch 52, loss :    0.39.\n",
            "Up to batch 53, loss :    0.39.\n",
            "Up to batch 54, loss :    0.38.\n",
            "Up to batch 55, loss :    0.38.\n",
            "Up to batch 56, loss :    0.37.\n",
            "Up to batch 57, loss :    0.37.\n",
            "Up to batch 58, loss :    0.36.\n",
            "Up to batch 59, loss :    0.36.\n",
            "Up to batch 60, loss :    0.35.\n",
            "Up to batch 61, loss :    0.35.\n",
            "Up to batch 62, loss :    0.35.\n",
            "Up to batch 63, loss :    0.35.\n",
            "Up to batch 64, loss :    0.34.\n",
            "Up to batch 65, loss :    0.34.\n",
            "Up to batch 66, loss :    0.34.\n",
            "Up to batch 67, loss :    0.33.\n",
            "Up to batch 68, loss :    0.33.\n",
            "Up to batch 69, loss :    0.32.\n",
            "Up to batch 70, loss :    0.32.\n",
            "Up to batch 71, loss :    0.32.\n",
            "Up to batch 72, loss :    0.32.\n",
            "Up to batch 73, loss :    0.31.\n",
            "Up to batch 74, loss :    0.31.\n",
            "Up to batch 75, loss :    0.32.\n",
            "Up to batch 76, loss :    0.32.\n",
            "Up to batch 77, loss :    0.33.\n",
            "Up to batch 78, loss :    0.33.\n"
          ]
        }
      ],
      "source": [
        "model = create_model()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "            loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=128,\n",
        "    epochs=2,\n",
        "    verbose=0,\n",
        "    callbacks=[LossPrintingCallback()],\n",
        ")\n",
        "\n",
        "res = model.evaluate(\n",
        "    x_test,\n",
        "    y_test,\n",
        "    batch_size=128,\n",
        "    verbose=0,\n",
        "    callbacks=[LossPrintingCallback()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_gIPEcK6ef1"
      },
      "source": [
        "#### tf.keras.callbacks.EarlyStopping보다 나은 조기종료 콜백"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class 클래스명():\n",
        ""
      ],
      "metadata": {
        "id": "-TMBV6x-NvAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "마린 = 클래스명() 호출"
      ],
      "metadata": {
        "id": "bU5tuHxiNpsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU9wCzI56eaX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
        "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
        "\n",
        "  Arguments:\n",
        "      patience: Number of epochs to wait after min has been hit. After this\n",
        "      number of no improvement, training stops.\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, patience=0):\n",
        "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
        "        self.patience = patience\n",
        "        # best_weights to store the weights at which the minimum loss occurs.\n",
        "        self.best_weights = None\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # The number of epoch it has waited when loss is no longer minimum.\n",
        "        self.wait = 0\n",
        "        # The epoch the training stops at.\n",
        "        self.stopped_epoch = 0\n",
        "        # Initialize the best as infinity.\n",
        "        self.best = np.Inf\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current = logs.get(\"loss\")\n",
        "        if np.less(current, self.best):\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            # Record the best weights if current results is better (less).\n",
        "            self.best_weights = self.model.get_weights()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "                print(\"Restoring model weights from the end of the best epoch.\")\n",
        "                self.model.set_weights(self.best_weights)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n",
        "\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "            loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    steps_per_epoch=5,\n",
        "    epochs=30,\n",
        "    verbose=0,\n",
        "    callbacks=[LossPrintingCallback(), EarlyStoppingAtMinLoss()],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LambdaCallback 이용하기"
      ],
      "metadata": {
        "id": "ZBXzoPMQnWg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.callbacks.LambdaCallback(\n",
        "    on_epoch_begin=None,\n",
        "    on_epoch_end=None,\n",
        "    on_batch_begin=None,\n",
        "    on_batch_end=None,\n",
        "    on_train_begin=None,\n",
        "    on_train_end=None\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPIaF4ge-MK1",
        "outputId": "341fe65e-650e-4988-cabe-e8b35b78aed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.LambdaCallback at 0x7f60b7978090>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### arguments 정리\n",
        "on_epoch_begin and on_epoch_end : epoch, logs  \n",
        "on_batch_begin and on_batch_end : batch, logs  \n",
        "on_train_begin and on_train_end : logs"
      ],
      "metadata": {
        "id": "udPXFzXS-Q-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "GaoveipA_MeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 시작마다 배치 출력\n",
        "batch_print_callback = tf.keras.callbacks.LambdaCallback(\n",
        "    on_batch_begin=lambda batch,logs: print(batch))\n",
        "# lambda 입력값, 출력값\n",
        "\n",
        "# loss json log 파일 생성\n",
        "json_log = open('loss_log.json', mode='wt', buffering=1)\n",
        "json_logging_callback = tf.keras.callbacks.LambdaCallback(\n",
        "    on_epoch_end=lambda epoch, logs: json_log.write(\n",
        "        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),\n",
        "    on_train_end=lambda logs: json_log.close()\n",
        ")\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "            loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    steps_per_epoch=5,\n",
        "    epochs=30,\n",
        "    verbose=0,\n",
        "    callbacks=[batch_print_callback,\n",
        "                json_logging_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2mylMP0-mnl",
        "outputId": "672dd8e0-1e21-4e01-8bf1-60031efce1c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f60b77c56d0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}