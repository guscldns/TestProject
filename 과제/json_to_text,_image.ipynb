{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mgtuG9BVp7MT",
        "DZmDVGcfrVu8"
      ],
      "authorship_tag": "ABX9TyN/ZKNzk6N9xxkPgQTiqsWJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "69777b5aa6ff43c4a30a94ebdf23b6ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c07957bef7c4231be6ef67beffc12d8",
              "IPY_MODEL_78eb37cff0da433f98448f2f6e9e03bb",
              "IPY_MODEL_05e4c9e30034484ca0c215747b4915e1"
            ],
            "layout": "IPY_MODEL_90d9358e83e64eba877372a0fe3525ae"
          }
        },
        "2c07957bef7c4231be6ef67beffc12d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7253ece8a3fa4bada2a9c222bf2d06dc",
            "placeholder": "​",
            "style": "IPY_MODEL_aafd20936fca424897c0c1eb0044943b",
            "value": "Downloading (…)official/config.json: 100%"
          }
        },
        "78eb37cff0da433f98448f2f6e9e03bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32deeff688b6487a9e749ae283cc2e99",
            "max": 404,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cd7892d5a91435199734612b6f536c4",
            "value": 404
          }
        },
        "05e4c9e30034484ca0c215747b4915e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d5668a02a5646cb926522c3d65b8d11",
            "placeholder": "​",
            "style": "IPY_MODEL_a67d1cd870f44a20a587f548450cf5b4",
            "value": " 404/404 [00:00&lt;00:00, 11.7kB/s]"
          }
        },
        "90d9358e83e64eba877372a0fe3525ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7253ece8a3fa4bada2a9c222bf2d06dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aafd20936fca424897c0c1eb0044943b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32deeff688b6487a9e749ae283cc2e99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cd7892d5a91435199734612b6f536c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d5668a02a5646cb926522c3d65b8d11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a67d1cd870f44a20a587f548450cf5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55b9711af76f4f4e954c5fe2fd95f302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c79a520391a40b0a70b1449673c4fea",
              "IPY_MODEL_2f8e07741394458a9dfc2fe65683f480",
              "IPY_MODEL_987f33366ed44edd907f8baef218a3b7"
            ],
            "layout": "IPY_MODEL_3016eb293a8047dbb78c9512651630a6"
          }
        },
        "2c79a520391a40b0a70b1449673c4fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54d14ecdf4974ae2980a4f2124571ba5",
            "placeholder": "​",
            "style": "IPY_MODEL_dd35fdbfabc74c4b916a20ab20d3d688",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "2f8e07741394458a9dfc2fe65683f480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23e959eea5a0456d8f20cf4dec9e7931",
            "max": 858583555,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed2274a0218145aaa7d4babe16c69069",
            "value": 858583555
          }
        },
        "987f33366ed44edd907f8baef218a3b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce896ef248944869a6c33cc4792b46a0",
            "placeholder": "​",
            "style": "IPY_MODEL_db58552a9d6e4ce987fbfafac4c4e6d7",
            "value": " 859M/859M [00:05&lt;00:00, 247MB/s]"
          }
        },
        "3016eb293a8047dbb78c9512651630a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54d14ecdf4974ae2980a4f2124571ba5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd35fdbfabc74c4b916a20ab20d3d688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23e959eea5a0456d8f20cf4dec9e7931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed2274a0218145aaa7d4babe16c69069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce896ef248944869a6c33cc4792b46a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db58552a9d6e4ce987fbfafac4c4e6d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guscldns/TestProject/blob/main/%EA%B3%BC%EC%A0%9C/json_to_text%2C_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zNSgmmwVafim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd334a0f-193b-495d-a213-1a3ed15e6c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rswWV67Idv8OXI1qQ7o5aSSWvP_uA5MW\n",
            "To: /content/sample.zip\n",
            "100% 2.95M/2.95M [00:00<00:00, 170MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1G4jAcJLy_1PC9YQR36t7jMapj9da4vPd\n",
            "To: /content/malgun.ttf\n",
            "100% 13.5M/13.5M [00:00<00:00, 58.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# 샘플파일\n",
        "!gdown 1rswWV67Idv8OXI1qQ7o5aSSWvP_uA5MW\n",
        "# 맑음고딕 폰트\n",
        "!gdown 1G4jAcJLy_1PC9YQR36t7jMapj9da4vPd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import json\n",
        "from PIL import Image, ImageDraw, ImageFont"
      ],
      "metadata": {
        "id": "oQxQ_E0DauX1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/sample\n",
        "%cd /content/sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3KtH-jqbRPA",
        "outputId": "54368b1e-2951-4a34-d126-4ec7fbf9ab85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/sample.zip"
      ],
      "metadata": {
        "id": "26U5g5OJbJLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de3e43a1-73ff-4741-88af-f58a8c0358aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/sample.zip\n",
            "   creating: 01.news_r/\n",
            "   creating: 01.news_r/20per/\n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00001-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00005-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00005-00003.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00009-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00012-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00016-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00017-00003.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00019-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00020-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00022-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00023-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00025-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00026-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00032-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00034-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00036-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00045-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00047-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00048-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00052-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00054-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00056-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00060-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00062-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00069-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00072-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00073-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00074-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00074-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00077-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00084-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00085-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00085-00003.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00093-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00094-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00096-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00098-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00100-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00103-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00104-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00110-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00114-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00117-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00118-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00120-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00124-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00125-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00127-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00132-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00133-00003.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00136-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00138-00003.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00140-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00151-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00155-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00159-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00163-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00167-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00168-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00172-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00173-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00176-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00179-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00181-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00183-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00184-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00188-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00190-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00193-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00193-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00198-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00205-00004.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00205-00005.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00207-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00212-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00212-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00214-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00216-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00217-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00228-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00232-00003.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00237-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00241-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00242-00004.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00243-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00246-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00247-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00248-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00248-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00251-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00259-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00261-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00262-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00262-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00263-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00265-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00266-00002.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00268-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00269-00001.json  \n",
            "  inflating: 01.news_r/20per/REPORT-news_r-00275-00001.json  \n",
            "   creating: 01.news_r/2~3sent/\n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00007-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00018-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00020-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00024-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00029-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00030-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00031-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00037-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00038-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00040-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00041-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00042-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00042-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00046-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00049-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00050-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00050-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00058-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00059-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00063-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00064-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00067-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00070-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00071-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00077-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00082-00003.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00085-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00087-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00088-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00089-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00089-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00090-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00091-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00092-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00095-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00099-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00102-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00115-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00119-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00127-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00129-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00131-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00138-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00141-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00143-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00144-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00146-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00149-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00150-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00152-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00156-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00157-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00162-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00165-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00166-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00166-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00175-00003.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00178-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00179-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00183-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00188-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00189-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00190-00003.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00196-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00197-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00198-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00201-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00202-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00204-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00205-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00205-00003.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00209-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00210-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00210-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00211-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00214-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00222-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00223-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00225-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00229-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00231-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00235-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00238-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00241-00003.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00244-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00249-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00250-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00250-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00254-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00254-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00255-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00256-00002.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00258-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00261-00003.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00263-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00265-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00267-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00270-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00272-00001.json  \n",
            "  inflating: 01.news_r/2~3sent/REPORT-news_r-00273-00002.json  \n",
            "   creating: 02.briefing/\n",
            "   creating: 02.briefing/20per/\n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20091-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20092-00002.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20101-00011.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20103-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20108-00003.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20148-00002.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20157-00003.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20161-00002.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20204-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20205-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20206-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20313-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20325-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20329-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20337-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20343-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20346-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20356-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20359-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20364-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20365-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20367-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20387-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20388-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20392-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20398-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20406-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20407-00002.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20419-00002.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20422-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20432-00002.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20445-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20458-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20461-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20467-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20468-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20470-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20490-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20507-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20510-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20523-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20531-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20533-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20534-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20540-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20547-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20557-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20564-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20567-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20568-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20575-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20584-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20585-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20586-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20587-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20609-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20618-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20621-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20630-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20631-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20636-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20638-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20641-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20656-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20671-00003.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20672-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20680-00004.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20689-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20692-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20700-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20702-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20710-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20719-00002.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20736-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20737-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20750-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20757-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20766-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20771-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20774-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20780-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20792-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20820-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20824-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20849-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20854-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20855-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20859-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20875-00002.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20877-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20881-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20903-00003.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20914-00002.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20924-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20925-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20953-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20954-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20955-00001.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20971-00002.json  \n",
            "  inflating: 02.briefing/20per/REPORT-briefing-20988-00001.json  \n",
            "   creating: 02.briefing/2~3sent/\n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20105-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20109-00004.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20111-00002.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20113-00002.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20124-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20127-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20138-00008.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20141-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20153-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20154-00002.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20163-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20172-00002.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20194-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20207-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20213-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20294-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20298-00002.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20301-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20305-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20307-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20310-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20311-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20322-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20332-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20335-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20339-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20340-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20341-00002.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20349-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20350-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20351-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20369-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20373-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20374-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20379-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20381-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20382-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20383-00002.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20384-00003.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20386-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20389-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20390-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20393-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20394-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20404-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20410-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20412-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20415-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20416-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20421-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20425-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20427-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20428-00002.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20429-00003.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20431-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20441-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20442-00002.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20451-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20452-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20457-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20460-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20472-00002.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20480-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20487-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20494-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20495-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20502-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20505-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20512-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20522-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20536-00002.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20543-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20544-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20548-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20556-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20561-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20569-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20570-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20571-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20574-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20576-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20578-00003.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20581-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20582-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20583-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20588-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20591-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20604-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20613-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20614-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20616-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20619-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20625-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20626-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20633-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20653-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20654-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20657-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20660-00001.json  \n",
            "  inflating: 02.briefing/2~3sent/REPORT-briefing-20661-00001.json  \n",
            "   creating: 03.his_cul/\n",
            "   creating: 03.his_cul/20per/\n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06000-06000.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06001-06001.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06002-06002.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06003-06003.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06004-06004.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06005-06005.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06006-06006.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06007-06007.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06008-06008.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06009-06009.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06010-06010.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06011-06011.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06012-06012.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06013-06013.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06014-06014.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06015-06015.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06016-06016.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06017-06017.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06018-06018.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06019-06019.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06020-06020.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06021-06021.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06022-06022.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06023-06023.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06024-06024.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06025-06025.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06026-06026.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06027-06027.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06028-06028.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06029-06029.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06030-06030.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06031-06031.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06032-06032.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06033-06033.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06034-06034.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06035-06035.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06036-06036.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06037-06037.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06038-06038.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06039-06039.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06040-06040.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06041-06041.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06042-06042.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06043-06043.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06044-06044.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06045-06045.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06046-06046.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06047-06047.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06048-06048.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06049-06049.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06050-06050.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06051-06051.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06052-06052.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06053-06053.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06054-06054.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06055-06055.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06056-06056.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06057-06057.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06058-06058.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06059-06059.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06060-06060.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06061-06061.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06062-06062.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06063-06063.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06064-06064.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06065-06065.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06066-06066.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06067-06067.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06068-06068.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06069-06069.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06070-06070.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06071-06071.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06072-06072.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06073-06073.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06074-06074.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06075-06075.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06076-06076.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06077-06077.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06078-06078.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06079-06079.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06080-06080.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06081-06081.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06082-06082.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06083-06083.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06084-06084.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06085-06085.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06086-06086.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06087-06087.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06088-06088.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06089-06089.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06090-06090.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06091-06091.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06092-06092.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06093-06093.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06094-06094.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06095-06095.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06096-06096.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06097-06097.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06098-06098.json  \n",
            "  inflating: 03.his_cul/20per/REPORT-his_cul-06099-06099.json  \n",
            "   creating: 03.his_cul/2~3sent/\n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01000-01000.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01001-01001.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01002-01002.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01003-01003.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01004-01004.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01005-01005.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01006-01006.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01007-01007.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01008-01008.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01009-01009.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01010-01010.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01011-01011.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01012-01012.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01013-01013.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01014-01014.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01015-01015.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01016-01016.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01017-01017.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01018-01018.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01019-01019.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01020-01020.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01021-01021.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01022-01022.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01023-01023.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01024-01024.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01025-01025.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01026-01026.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01027-01027.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01028-01028.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01029-01029.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01030-01030.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01031-01031.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01032-01032.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01033-01033.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01034-01034.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01035-01035.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01036-01036.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01037-01037.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01038-01038.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01039-01039.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01040-01040.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01041-01041.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01042-01042.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01043-01043.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01044-01044.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01045-01045.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01046-01046.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01047-01047.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01048-01048.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01049-01049.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01050-01050.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01051-01051.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01052-01052.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01053-01053.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01054-01054.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01055-01055.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01056-01056.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01057-01057.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01058-01058.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01059-01059.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01060-01060.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01061-01061.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01062-01062.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01063-01063.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01064-01064.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01065-01065.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01066-01066.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01067-01067.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01068-01068.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01069-01069.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01070-01070.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01071-01071.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01072-01072.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01073-01073.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01074-01074.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01075-01075.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01076-01076.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01077-01077.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01078-01078.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01079-01079.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01080-01080.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01081-01081.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01082-01082.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01083-01083.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01084-01084.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01085-01085.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01086-01086.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01087-01087.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01088-01088.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01089-01089.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01090-01090.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01091-01091.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01092-01092.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01093-01093.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01094-01094.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01095-01095.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01096-01096.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01097-01097.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01098-01098.json  \n",
            "  inflating: 03.his_cul/2~3sent/REPORT-his_cul-01099-01099.json  \n",
            "   creating: 04.paper/\n",
            "   creating: 04.paper/20per/\n",
            "  inflating: 04.paper/20per/REPORT-paper-00001-00017.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00001-00022.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00001-00043.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00001-00064.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00001-00067.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00001-00070.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00001-00078.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00001-00082.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00001-00085.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00001-00087.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00001-00090.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00001-00091.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00003-00050.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00003-00083.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00003-00134.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00003-00176.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00004-00054.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00004-00105.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00005-00014.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00005-00027.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00005-00084.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00007-00002.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00007-00077.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00007-00097.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00007-00156.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00007-00174.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00007-00180.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00011-00106.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00011-00150.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00012-00006.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00012-00008.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00012-00053.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00012-00058.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00012-00073.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00012-00101.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00012-00117.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00013-00005.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00013-00047.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00014-00237.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00014-00517.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00014-00575.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00015-00252.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00015-00428.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00015-00552.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00018-00051.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00018-00082.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00018-00113.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00018-00282.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00019-00091.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00019-00207.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00019-00262.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00019-00275.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00021-00016.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00021-00081.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00021-00113.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00021-00119.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00021-00128.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00021-00157.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00022-00014.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00022-00060.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00022-00139.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00022-00268.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00022-00283.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00009.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00020.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00048.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00066.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00088.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00149.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00162.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00165.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00201.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00213.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00240.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00284.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00327.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00369.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00422.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00425.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00428.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00444.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00480.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00481.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00490.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00491.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00517.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00574.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00023-00613.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00024-00031.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00024-00039.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00024-00103.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00024-00158.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00024-00196.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00024-00319.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00024-00352.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00024-00443.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00024-00459.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00024-00534.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00024-00544.json  \n",
            "  inflating: 04.paper/20per/REPORT-paper-00024-00547.json  \n",
            "   creating: 04.paper/2~3sent/\n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00001-00084.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00001-00098.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00002-00099.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00003-00142.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00004-00053.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00004-00088.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00007-00054.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00007-00061.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00007-00153.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00008-00011.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00009-00094.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00009-00138.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00011-00070.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00011-00107.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00011-00139.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00011-00200.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00012-00002.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00012-00114.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00012-00119.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00013-00002.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00013-00003.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00014-00058.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00014-00089.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00014-00215.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00014-00364.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00015-00025.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00015-00094.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00015-00159.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00015-00264.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00015-00316.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00015-00318.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00015-00529.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00015-00555.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00016-00017.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00018-00109.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00018-00133.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00019-00053.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00019-00061.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00019-00177.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00019-00394.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00021-00013.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00021-00098.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00021-00135.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00021-00177.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00021-00256.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00038.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00059.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00063.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00110.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00143.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00158.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00235.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00266.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00344.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00386.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00423.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00437.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00439.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00460.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00483.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00554.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00023-00560.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00067.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00125.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00133.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00140.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00182.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00255.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00280.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00297.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00322.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00368.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00388.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00402.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00407.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00442.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00461.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00493.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00515.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00641.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00669.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00675.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00691.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00723.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00737.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00772.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00811.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00812.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00815.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00834.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00862.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00876.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-00877.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00024-01041.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00025-00029.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00025-00072.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00025-00078.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00025-00095.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00026-00001.json  \n",
            "  inflating: 04.paper/2~3sent/REPORT-paper-00026-00065.json  \n",
            "   creating: 05.minute/\n",
            "   creating: 05.minute/20per/\n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00004.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00006.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00019.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00032.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00033.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00038.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00040.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00047.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00054.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00063.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00065.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00069.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00071.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00087.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00091.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00101.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00112.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00114.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00115.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00120.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00140.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00149.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00156.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00159.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00170.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00171.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00178.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00183.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00190.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00193.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00194.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00195.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00196.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00200.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00204.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00209.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00216.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00219.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00220.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00227.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00228.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00230.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00232.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00234.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00247.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00252.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00253.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00258.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00261.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00263.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00275.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00277.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00278.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00299.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00302.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00308.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00318.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00320.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00323.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00334.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00336.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00346.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00352.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00355.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00362.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00375.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00380.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00392.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00393.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00397.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00403.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00405.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00406.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00411.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00415.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00417.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00422.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00425.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00433.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00434.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00439.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00442.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00455.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00456.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00466.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00467.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00468.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00480.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00496.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00497.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00500.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00502.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00505.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00523.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00524.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00532.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00533.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00544.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00547.json  \n",
            "  inflating: 05.minute/20per/REPORT-minute-00001-00548.json  \n",
            "   creating: 05.minute/2~3sent/\n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00001.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00003.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00035.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00044.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00103.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00169.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00172.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00233.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00348.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00399.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00518.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00526.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00587.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00744.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00759.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00778.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00916.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00919.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00976.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-00984.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01055.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01092.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01101.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01116.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01155.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01215.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01289.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01316.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01322.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01332.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01337.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01364.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01435.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01438.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01441.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01472.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01506.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01526.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01605.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01676.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00001-01740.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00006.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00008.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00014.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00029.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00042.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00058.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00071.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00085.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00101.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00159.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00218.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00311.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00324.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00337.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00348.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00350.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00367.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00373.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00414.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00524.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00554.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00666.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00757.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00777.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00874.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00965.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-00989.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01118.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01129.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01164.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01183.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01227.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01281.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01380.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01393.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01397.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01450.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01465.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01475.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01482.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01513.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01545.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01563.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01706.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01749.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01822.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01845.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01854.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01855.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01862.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-01983.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-02026.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-02032.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-02039.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-02042.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-02113.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-02152.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-02167.json  \n",
            "  inflating: 05.minute/2~3sent/REPORT-minute-00002-02186.json  \n",
            "   creating: 06.edit/\n",
            "   creating: 06.edit/20per/\n",
            "  inflating: 06.edit/20per/REPORT-edit-30212-00005.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30212-00006.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30213-00001.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30213-00003.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30214-00001.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30214-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30214-00003.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30214-00005.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30214-00006.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30214-00007.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30215-00001.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30215-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30215-00003.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30215-00004.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30215-00005.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30215-00006.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30215-00007.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30215-00009.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30215-00010.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30216-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30217-00001.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30217-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30217-00003.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30217-00004.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30217-00005.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30217-00006.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30217-00007.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30218-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30218-00003.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30218-00004.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30218-00005.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30218-00007.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30219-00001.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30219-00003.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30219-00004.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30219-00005.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30219-00006.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30219-00007.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30219-00008.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30219-00009.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30219-00011.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30220-00001.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30220-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30220-00004.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30220-00006.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30220-00008.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30220-00009.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30220-00010.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30221-00001.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30221-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30221-00003.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30221-00004.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30221-00005.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30221-00006.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30221-00007.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30222-00001.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30222-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30222-00003.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30223-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30223-00003.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30223-00004.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30223-00005.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30223-00006.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30224-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30224-00003.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30224-00004.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30224-00005.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30224-00006.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30224-00007.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30224-00008.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30225-00001.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30225-00004.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30225-00005.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30225-00006.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30225-00008.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30226-00001.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30226-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30226-00003.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30226-00004.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30226-00005.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30226-00006.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30226-00007.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30226-00009.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30226-00011.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30226-00012.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30227-00001.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30227-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30227-00003.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30227-00004.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30227-00005.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30227-00006.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30228-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30229-00004.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30230-00001.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30230-00002.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30230-00003.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30230-00004.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30230-00005.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30230-00006.json  \n",
            "  inflating: 06.edit/20per/REPORT-edit-30231-00002.json  \n",
            "   creating: 06.edit/2~3sent/\n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00011-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00012-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00012-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00012-00003.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00013-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00013-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00014-00003.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00015-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00015-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00016-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00019-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00022-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00023-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00025-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00027-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00027-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00029-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00029-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00031-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00032-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00033-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00034-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00034-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00037-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00037-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00037-00003.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00038-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00038-00003.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00039-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00040-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00040-00003.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00041-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00041-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00041-00003.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00042-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00042-00003.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00043-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00044-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00044-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00045-00003.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00046-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00046-00003.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00046-00004.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00047-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00048-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00049-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00049-00003.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00050-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00052-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00053-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00055-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00055-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00056-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00056-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00057-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00057-00003.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00058-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00060-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00061-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00062-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00064-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00066-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00066-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00067-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00067-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00068-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00069-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00069-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00070-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00072-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00072-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00073-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00074-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00078-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00078-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00079-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00079-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00081-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00082-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00083-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00084-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00086-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00086-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00087-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00088-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00089-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00089-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00090-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00090-00003.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00091-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-00091-00003.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-10001-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-10002-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-10004-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-10004-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-10005-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-10005-00002.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-10006-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-10008-00001.json  \n",
            "  inflating: 06.edit/2~3sent/REPORT-edit-10009-00002.json  \n",
            "   creating: 07.public/\n",
            "   creating: 07.public/20per/\n",
            "  inflating: 07.public/20per/REPORT-public-00001-00030.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00001-00059.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00001-00082.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00001-00102.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00001-00103.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00001-00110.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00001-00124.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00001-00174.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00001-00213.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00001-00260.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00001-00295.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00001-00303.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00001-00339.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00001-00403.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00055.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00059.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00063.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00190.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00204.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00311.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00353.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00407.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00426.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00434.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00575.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00634.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00698.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00002-00772.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00003-00099.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00003-00153.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00003-00207.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00006-00009.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00011-00076.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00011-00098.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00011-00122.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00012-00101.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00012-00132.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00012-00170.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00013-00001.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00014-00011.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00014-00033.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00014-00039.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00014-00095.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00015-00069.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00024.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00028.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00032.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00033.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00034.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00067.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00158.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00256.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00311.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00340.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00393.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00396.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00400.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00016-00404.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00017-00087.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00017-00213.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00017-00219.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00018-00149.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00018-00166.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00018-00191.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00018-00253.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00018-00254.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00018-00255.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00019-00012.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00020-00148.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00021-00037.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00021-00049.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00021-00114.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00021-00135.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00023-00212.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00023-00242.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00024-00215.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00024-00272.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00025-00017.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00025-00020.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00026-00004.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00026-00235.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00027-00001.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00027-00004.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00027-00028.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00028-00017.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00028-00094.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00028-00105.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00028-00121.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00028-00124.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00028-00129.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00029-00016.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00029-00023.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00029-00056.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00040-00008.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00054-00016.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00061-00031.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00061-00056.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00061-00077.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00061-00080.json  \n",
            "  inflating: 07.public/20per/REPORT-public-00061-00085.json  \n",
            "   creating: 07.public/2~3sent/\n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00022.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00025.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00066.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00088.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00133.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00161.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00171.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00184.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00253.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00278.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00300.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00315.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00328.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00433.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00001-00471.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00004.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00005.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00025.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00029.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00036.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00075.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00107.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00110.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00126.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00153.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00194.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00232.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00233.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00267.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00279.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00294.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00305.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00308.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00315.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00325.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00328.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00340.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00344.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00346.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00352.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00365.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00373.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00384.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00411.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00431.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00437.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00447.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00467.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00474.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00488.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00542.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00553.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00557.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00566.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00592.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00611.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00632.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00646.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00664.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00667.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00711.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00720.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00761.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00002-00770.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00001.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00019.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00034.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00040.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00045.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00066.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00080.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00097.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00117.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00125.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00155.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00188.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00204.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00003-00234.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00005-00001.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00006-00011.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00007-00005.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00010-00046.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00011-00045.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00012-00005.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00012-00009.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00012-00028.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00012-00083.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00012-00090.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00012-00094.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00012-00125.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00012-00168.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00013-00031.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00013-00062.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00013-00121.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00013-00144.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00014-00009.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00014-00038.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00014-00064.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00014-00067.json  \n",
            "  inflating: 07.public/2~3sent/REPORT-public-00014-00068.json  \n",
            "   creating: 08.speech/\n",
            "   creating: 08.speech/20per/\n",
            "  inflating: 08.speech/20per/REPORT-speech-00001-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00001-00005.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00002-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00003-00010.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00005-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00011-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00011-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00011-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00012-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00012-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00012-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00012-00006.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00013-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00014-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00014-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00015-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00015-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00016-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00017-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00018-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00020-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00023-00005.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00023-00006.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00023-00009.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00023-00010.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00025-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00025-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00025-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00026-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00027-00006.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00027-00008.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00028-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00029-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00030-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00032-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00033-00006.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00033-00009.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00034-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00034-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00036-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00036-00006.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00039-00009.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00041-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00041-00008.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00041-00012.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00044-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00046-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00047-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00047-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00048-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00049-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00049-00006.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00052-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00053-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00053-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00054-00007.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00054-00010.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00055-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00056-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00056-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00056-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00057-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00057-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00057-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00057-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00057-00006.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00060-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00060-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00061-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00061-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00061-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00061-00005.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00063-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00063-00005.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00063-00007.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00064-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00065-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00065-00006.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00065-00008.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00068-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00069-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00070-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00070-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00070-00006.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00070-00007.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00070-00012.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00070-00013.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00073-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00074-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00075-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00075-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00076-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00076-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00077-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00081-00003.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00082-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00082-00004.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00084-00001.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00084-00002.json  \n",
            "  inflating: 08.speech/20per/REPORT-speech-00084-00003.json  \n",
            "   creating: 08.speech/2~3sent/\n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00001-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00001-00004.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00002-00004.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00002-00006.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00002-00009.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00003-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00003-00004.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00003-00011.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00003-00013.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00005-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00006-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00006-00004.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00009-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00011-00002.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00012-00005.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00013-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00013-00008.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00013-00009.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00013-00012.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00013-00013.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00013-00014.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00015-00002.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00015-00004.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00015-00005.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00015-00006.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00018-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00020-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00021-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00023-00004.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00023-00007.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00023-00011.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00023-00012.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00024-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00025-00002.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00026-00002.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00027-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00027-00005.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00028-00002.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00028-00005.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00028-00006.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00029-00002.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00029-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00029-00004.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00030-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00031-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00031-00005.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00032-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00033-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00033-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00033-00004.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00033-00005.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00033-00007.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00033-00008.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00034-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00034-00004.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00037-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00038-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00038-00002.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00038-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00039-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00039-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00039-00004.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00039-00005.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00039-00007.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00039-00008.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00040-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00040-00002.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00041-00002.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00041-00005.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00041-00006.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00041-00009.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00041-00011.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00041-00013.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00042-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00045-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00045-00002.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00045-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00047-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00047-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00049-00002.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00049-00005.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00053-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00053-00007.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00053-00008.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00053-00009.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00054-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00054-00012.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00055-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00056-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00056-00005.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00057-00005.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00057-00008.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00058-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00058-00002.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00058-00003.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00058-00004.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00058-00005.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00059-00001.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00059-00002.json  \n",
            "  inflating: 08.speech/2~3sent/REPORT-speech-00060-00002.json  \n",
            "   creating: 09.literature/\n",
            "   creating: 09.literature/20per/\n",
            "  inflating: 09.literature/20per/REPORT-literature-07200-07200.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07201-07201.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07202-07202.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07203-07203.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07204-07204.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07205-07205.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07206-07206.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07207-07207.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07208-07208.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07209-07209.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07210-07210.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07211-07211.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07212-07212.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07213-07213.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07214-07214.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07215-07215.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07216-07216.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07217-07217.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07218-07218.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07219-07219.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07220-07220.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07221-07221.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07222-07222.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07223-07223.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07224-07224.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07225-07225.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07226-07226.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07227-07227.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07228-07228.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07229-07229.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07230-07230.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07231-07231.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07232-07232.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07233-07233.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07234-07234.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07235-07235.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07236-07236.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07237-07237.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07238-07238.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07239-07239.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07240-07240.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07241-07241.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07242-07242.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07243-07243.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07244-07244.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07245-07245.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07246-07246.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07247-07247.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07248-07248.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07249-07249.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07250-07250.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07251-07251.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07252-07252.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07253-07253.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07254-07254.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07255-07255.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07256-07256.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07257-07257.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07258-07258.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07259-07259.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07260-07260.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07261-07261.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07262-07262.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07263-07263.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07264-07264.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07265-07265.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07266-07266.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07267-07267.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07268-07268.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07269-07269.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07270-07270.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07271-07271.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07272-07272.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07273-07273.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07274-07274.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07275-07275.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07276-07276.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07277-07277.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07278-07278.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07279-07279.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07280-07280.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07281-07281.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07282-07282.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07283-07283.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07284-07284.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07285-07285.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07286-07286.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07287-07287.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07288-07288.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07289-07289.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07290-07290.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07291-07291.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07292-07292.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07293-07293.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07294-07294.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07295-07295.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07296-07296.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07297-07297.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07298-07298.json  \n",
            "  inflating: 09.literature/20per/REPORT-literature-07299-07299.json  \n",
            "   creating: 09.literature/2~3sent/\n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01200-01200.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01201-01201.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01202-01202.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01203-01203.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01204-01204.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01205-01205.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01206-01206.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01207-01207.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01208-01208.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01209-01209.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01210-01210.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01211-01211.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01212-01212.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01213-01213.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01214-01214.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01215-01215.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01216-01216.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01217-01217.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01218-01218.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01219-01219.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01220-01220.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01221-01221.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01222-01222.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01223-01223.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01224-01224.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01225-01225.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01226-01226.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01227-01227.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01228-01228.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01229-01229.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01230-01230.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01231-01231.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01232-01232.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01233-01233.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01234-01234.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01235-01235.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01236-01236.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01237-01237.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01238-01238.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01239-01239.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01240-01240.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01241-01241.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01242-01242.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01243-01243.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01244-01244.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01245-01245.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01246-01246.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01247-01247.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01248-01248.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01249-01249.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01250-01250.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01251-01251.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01252-01252.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01253-01253.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01254-01254.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01255-01255.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01256-01256.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01257-01257.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01258-01258.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01259-01259.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01260-01260.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01261-01261.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01262-01262.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01263-01263.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01264-01264.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01265-01265.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01266-01266.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01267-01267.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01268-01268.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01269-01269.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01270-01270.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01271-01271.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01272-01272.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01273-01273.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01274-01274.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01275-01275.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01276-01276.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01277-01277.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01278-01278.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01279-01279.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01280-01280.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01281-01281.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01282-01282.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01283-01283.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01284-01284.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01285-01285.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01286-01286.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01287-01287.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01288-01288.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01289-01289.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01290-01290.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01291-01291.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01292-01292.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01293-01293.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01294-01294.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01295-01295.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01296-01296.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01297-01297.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01298-01298.json  \n",
            "  inflating: 09.literature/2~3sent/REPORT-literature-01299-01299.json  \n",
            "   creating: 10.narration/\n",
            "   creating: 10.narration/20per/\n",
            "  inflating: 10.narration/20per/REPORT-narration-00003-00009.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-00189-00014.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-00189-00036.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-00192-00017.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-00193-00018.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-00193-00020.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-07720-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-09516-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-14374-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-14624-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-19690-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-22039-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-26255-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-27770-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-28430-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-34089-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-34194-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-46474-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51001-00003.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51001-00005.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51001-00006.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51001-00013.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51001-00018.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51002-00003.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51002-00004.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51002-00007.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51002-00011.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51002-00014.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51002-00016.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51002-00017.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51002-00018.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51002-00021.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51003-00008.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51003-00010.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51003-00011.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51003-00014.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51003-00015.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51004-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51004-00002.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51004-00004.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51004-00005.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51004-00007.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51005-00003.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51005-00004.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51005-00006.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51005-00008.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51005-00010.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51005-00011.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51005-00014.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51006-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51006-00002.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51006-00003.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51006-00007.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51006-00008.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51006-00009.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51006-00010.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51006-00014.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51006-00018.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51006-00020.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51006-00021.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51007-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51007-00002.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51007-00005.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51007-00006.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51007-00008.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51007-00010.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51007-00011.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51007-00012.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51008-00002.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51008-00009.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51008-00011.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51008-00012.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51008-00014.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51008-00015.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51008-00020.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51009-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51009-00003.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51009-00004.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51009-00005.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51009-00006.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51009-00007.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51009-00008.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51009-00009.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51009-00011.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51009-00015.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51010-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51010-00003.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51010-00004.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51010-00006.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51010-00008.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51010-00012.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51010-00016.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51010-00017.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51011-00001.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51011-00002.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51011-00006.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51011-00015.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51011-00016.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51011-00017.json  \n",
            "  inflating: 10.narration/20per/REPORT-narration-51011-00018.json  \n",
            "   creating: 10.narration/2~3sent/\n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00001-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00001-00002.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00001-00003.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00001-00005.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00001-00006.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00001-00007.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00001-00008.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00002-00006.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00003-00004.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00003-00005.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00003-00008.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00003-00010.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00006-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00186-00023.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00188-00005.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00189-00008.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00189-00013.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00189-00035.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00192-00032.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00192-00046.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00193-00019.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00193-00030.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00194-00013.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00199-00012.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-00200-00035.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-04668-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-05287-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-10001-00005.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-13949-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-16270-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-17363-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-20194-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-20295-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-24266-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-33533-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-35354-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-35933-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51001-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51001-00004.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51001-00007.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51001-00008.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51001-00009.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51001-00011.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51001-00012.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51001-00015.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51001-00016.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51001-00017.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51001-00021.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51002-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51002-00002.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51002-00005.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51002-00006.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51002-00008.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51002-00009.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51002-00010.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51002-00012.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51002-00013.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51002-00015.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51002-00019.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51002-00020.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51002-00022.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51003-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51003-00002.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51003-00003.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51003-00004.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51003-00005.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51003-00006.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51003-00007.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51003-00012.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51003-00016.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51003-00017.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51004-00003.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51004-00006.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51004-00008.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51004-00009.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51004-00011.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51004-00012.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51004-00013.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51004-00014.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51005-00001.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51005-00002.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51005-00005.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51005-00007.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51005-00009.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51005-00012.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51005-00013.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51006-00004.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51006-00005.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51006-00006.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51006-00011.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51006-00012.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51006-00013.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51006-00015.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51006-00016.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51006-00017.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51006-00019.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51007-00003.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51007-00004.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51007-00007.json  \n",
            "  inflating: 10.narration/2~3sent/REPORT-narration-51007-00009.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/clovaai/donut.git\n",
        "!mkdir /content/text\n",
        "!mkdir /content/json\n",
        "!mkdir /content/jpg"
      ],
      "metadata": {
        "id": "tXMYLzVPpoIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa29c32-3970-4188-fd82-18b1ad6c649a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'donut'...\n",
            "remote: Enumerating objects: 289, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 289 (delta 5), reused 11 (delta 5), pack-reused 273\u001b[K\n",
            "Receiving objects: 100% (289/289), 62.78 MiB | 36.26 MiB/s, done.\n",
            "Resolving deltas: 100% (131/131), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.25.1\n",
        "!pip install pytorch-lightning==1.6.4\n",
        "!pip install timm==0.5.4\n",
        "!pip install gradio\n",
        "!pip install donut-python\n",
        "!pip install synthtiger"
      ],
      "metadata": {
        "id": "_D754Ti-pob1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eba5c8bf-d782-45c2-eb9d-9a6b7dfddda6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.25.1\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0 (from transformers==4.25.1)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.25.1)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.25.1) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.1) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.1) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.25.1) (2023.7.22)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 tokenizers-0.13.3 transformers-4.25.1\n",
            "Collecting pytorch-lightning==1.6.4\n",
            "  Downloading pytorch_lightning-1.6.4-py3-none-any.whl (585 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.5/585.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.4) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.4) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.4) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.4) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.4) (2023.6.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.4) (2.12.3)\n",
            "Collecting torchmetrics>=0.4.1 (from pytorch-lightning==1.6.4)\n",
            "  Downloading torchmetrics-1.0.3-py3-none-any.whl (731 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyDeprecate>=0.3.1 (from pytorch-lightning==1.6.4)\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.4) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.6.4) (4.7.1)\n",
            "Collecting protobuf<=3.20.1 (from pytorch-lightning==1.6.4)\n",
            "  Downloading protobuf-3.20.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (3.8.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (3.4.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.4) (0.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.4) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.4) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.4) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.4) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning==1.6.4) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.*->pytorch-lightning==1.6.4) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.*->pytorch-lightning==1.6.4) (16.0.6)\n",
            "Collecting lightning-utilities>=0.7.0 (from torchmetrics>=0.4.1->pytorch-lightning==1.6.4)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.4) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.*->pytorch-lightning==1.6.4) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.6.4) (3.2.2)\n",
            "Installing collected packages: pyDeprecate, protobuf, lightning-utilities, torchmetrics, pytorch-lightning\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery 3.10.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.22.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-functions 1.13.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "googleapis-common-protos 1.60.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lightning-utilities-0.9.0 protobuf-3.20.1 pyDeprecate-0.3.2 pytorch-lightning-1.6.4 torchmetrics-1.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm==0.5.4\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/431.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/431.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.5.4) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.5.4) (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.5.4) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.5.4) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.5.4) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.5.4) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.5.4) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.5.4) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.5.4) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.5.4) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.5.4) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->timm==0.5.4) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm==0.5.4) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.40.1-py3-none-any.whl (20.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.8.5)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.101.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.4.0 (from gradio)\n",
            "  Downloading gradio_client-0.4.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.16.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.0.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.7.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.4.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio)\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio) (2.6.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.9.2)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.3)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=c4aa8327b58f4d381f102cf540492593c16787b372e8c7cca2488f9cbab564dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, semantic-version, python-multipart, orjson, markdown-it-py, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.0\n",
            "    Uninstalling mdit-py-plugins-0.4.0:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.0\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.101.1 ffmpy-0.3.1 gradio-3.40.1 gradio-client-0.4.0 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 orjson-3.9.5 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 uvicorn-0.23.2 websockets-11.0.3\n",
            "Collecting donut-python\n",
            "  Downloading donut_python-1.0.9-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: transformers>=4.11.3 in /usr/local/lib/python3.10/dist-packages (from donut-python) (4.25.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from donut-python) (0.5.4)\n",
            "Collecting datasets[vision] (from donut-python)\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytorch-lightning>=1.6.4 in /usr/local/lib/python3.10/dist-packages (from donut-python) (1.6.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from donut-python) (3.8.1)\n",
            "Collecting sentencepiece (from donut-python)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zss (from donut-python)\n",
            "  Downloading zss-1.2.0.tar.gz (9.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sconf>=0.2.3 (from donut-python)\n",
            "  Downloading sconf-0.2.5-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python) (2023.6.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python) (2.12.3)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python) (1.0.3)\n",
            "Requirement already satisfied: pyDeprecate>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python) (0.3.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python) (4.7.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python) (3.20.1)\n",
            "Collecting ruamel.yaml (from sconf>=0.2.3->donut-python)\n",
            "  Downloading ruamel.yaml-0.17.32-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting munch (from sconf>=0.2.3->donut-python)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.11.3->donut-python) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.11.3->donut-python) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.11.3->donut-python) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.11.3->donut-python) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.11.3->donut-python) (0.13.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->donut-python) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets[vision]->donut-python)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->donut-python) (1.5.3)\n",
            "Collecting xxhash (from datasets[vision]->donut-python)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets[vision]->donut-python)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->donut-python) (3.8.5)\n",
            "Requirement already satisfied: Pillow>=6.2.1 in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->donut-python) (9.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->donut-python) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->donut-python) (1.3.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->donut-python) (0.15.2+cu118)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from zss->donut-python) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.11.3->donut-python) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.11.3->donut-python) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.11.3->donut-python) (2023.7.22)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (3.4.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (0.41.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python) (16.0.6)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.4.1->pytorch-lightning>=1.6.4->donut-python) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets[vision]->donut-python) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets[vision]->donut-python) (2023.3)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->sconf>=0.2.3->donut-python)\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python) (3.2.2)\n",
            "Building wheels for collected packages: zss\n",
            "  Building wheel for zss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zss: filename=zss-1.2.0-py3-none-any.whl size=6723 sha256=48f52b28ad57f939657875ae24132487960775c4babf127a4631f16683683733\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/61/2a/cf33ab7301cc318a13418d9a805c1832be561b46e7d9337625\n",
            "Successfully built zss\n",
            "Installing collected packages: sentencepiece, zss, xxhash, ruamel.yaml.clib, munch, dill, ruamel.yaml, multiprocess, sconf, datasets, donut-python\n",
            "Successfully installed datasets-2.14.4 dill-0.3.7 donut-python-1.0.9 multiprocess-0.70.15 munch-4.0.0 ruamel.yaml-0.17.32 ruamel.yaml.clib-0.2.7 sconf-0.2.5 sentencepiece-0.1.99 xxhash-3.3.0 zss-1.2.0\n",
            "Collecting synthtiger\n",
            "  Downloading synthtiger-1.2.1-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.0/127.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting arabic-reshaper (from synthtiger)\n",
            "  Downloading arabic_reshaper-3.0.0-py3-none-any.whl (20 kB)\n",
            "Collecting blend-modes (from synthtiger)\n",
            "  Downloading blend_modes-2.1.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fonttools in /usr/local/lib/python3.10/dist-packages (from synthtiger) (4.42.0)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from synthtiger) (0.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from synthtiger) (1.23.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from synthtiger) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from synthtiger) (9.4.0)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from synthtiger) (2.5.1)\n",
            "Collecting python-bidi (from synthtiger)\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Collecting pytweening (from synthtiger)\n",
            "  Downloading pytweening-1.0.7.tar.gz (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from synthtiger) (6.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from synthtiger) (2023.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from synthtiger) (1.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug->synthtiger) (1.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug->synthtiger) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug->synthtiger) (0.19.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug->synthtiger) (2.31.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from imgaug->synthtiger) (2.0.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->synthtiger) (3.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->synthtiger) (2023.8.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->synthtiger) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->synthtiger) (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->synthtiger) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->synthtiger) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->synthtiger) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->synthtiger) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->synthtiger) (2.8.2)\n",
            "Building wheels for collected packages: blend-modes, pytweening\n",
            "  Building wheel for blend-modes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blend-modes: filename=blend_modes-2.1.0-py3-none-any.whl size=9539 sha256=697cec8c14bcca94e44658717cc191d6375a999bb16443020c19bc6eb23ff93c\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/59/0f/7f3f9e6e1fcbbaf41376ce408a4d1f14df7546bad2ced90fd4\n",
            "  Building wheel for pytweening (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytweening: filename=pytweening-1.0.7-py3-none-any.whl size=6198 sha256=45e66500bc190370e640134f86ef3bbddfce942e49ae29147964e189e83f1608\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/a3/d7/36c45539416215425b3247e37b691a98a03b1db7b13b7f9632\n",
            "Successfully built blend-modes pytweening\n",
            "Installing collected packages: pytweening, arabic-reshaper, python-bidi, blend-modes, synthtiger\n",
            "Successfully installed arabic-reshaper-3.0.0 blend-modes-2.1.0 python-bidi-0.4.2 pytweening-1.0.7 synthtiger-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# foilderlist = os.listdir('/content/sample')\n",
        "\n",
        "# for i in foilderlist:\n",
        "#     per_path = f'/content/sample/{i}/20per'\n",
        "#     sent_path = f'/content/sample/{i}/2~3sent'\n",
        "#     per_list = os.listdir(per_path)\n",
        "#     sent_list = os.listdir(sent_path)\n",
        "\n",
        "#     for j in per_list:\n",
        "#         with open(f'{per_path}/{j}', 'r', encoding='utf-8') as file:\n",
        "#             text = json.load(file)\n",
        "\n",
        "#             image_path0 = f'{j[:-5]}.jpg'\n",
        "#             ground_truth_parse = f\" {text['Meta(Refine)']['passage']}\"\n",
        "\n",
        "#             # Create a dictionary with the desired format\n",
        "#             sample = {\n",
        "#                 \"file_name\": image_path0,\n",
        "#                 \"ground_truth\": {\n",
        "#                     \"gt_parses\": [\n",
        "#                         {\n",
        "#                             \"text_sequence\": ground_truth_parse\n",
        "#                         }\n",
        "#                     ]\n",
        "#                 }\n",
        "#             }\n",
        "\n",
        "#             # Save the dictionary as a JSON file\n",
        "#             with open(f'/content/json/{j[:-5]}.json', 'w', encoding='utf-8') as fi:\n",
        "#                 json.dump(sample, fi, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "Od-VDS04kHXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# foilderlist = os.listdir('/content/sample')\n",
        "\n",
        "# for i in foilderlist:\n",
        "#     per_path = f'/content/sample/{i}/20per'\n",
        "#     sent_path = f'/content/sample/{i}/2~3sent'\n",
        "#     per_list = os.listdir(per_path)\n",
        "#     sent_list = os.listdir(sent_path)\n",
        "\n",
        "#     for j in per_list:\n",
        "#         with open(f'{per_path}/{j}', 'r', encoding='utf-8') as file:\n",
        "#             text = json.load(file)\n",
        "\n",
        "#             image_path0 = f'{j[:-5]}.jpg'\n",
        "#             ground_truth_parse = f\" {text['Meta(Refine)']['passage']}\"\n",
        "\n",
        "#             # Create a dictionary with the desired format\n",
        "#             sample = {\n",
        "#                 \"file_name\": image_path0,\n",
        "#                 \"ground_truth\": {\n",
        "#                     \"gt_parses\": [\n",
        "#                         {\n",
        "#                             \"text_sequence\": ground_truth_parse\n",
        "#                         }\n",
        "#                     ]\n",
        "#                 }\n",
        "#             }\n",
        "\n",
        "#             # Save the dictionary as a JSON file\n",
        "#             with open(f'/content/json/{j[:-5]}.json', 'w', encoding='utf-8') as fi:\n",
        "#                 json.dump(sample, fi, ensure_ascii=False, indent=4)\n",
        "\n",
        "#         # Save the extracted passage to the 'test' directory\n",
        "#             with open(f'/content/text/{j[:-5]}.txt', 'w', encoding='utf-8') as f:\n",
        "#                 f.write(f'{ground_truth_parse}')\n",
        "\n",
        "#         # Create an image with the text\n",
        "#         image_width = 800\n",
        "#         image_height = 1200\n",
        "#         font_size = 18 # font size\n",
        "#         left_margin = 50 # 왼쪽 여백\n",
        "#         right_margin = 50 # 오른쪽 여백\n",
        "#         vertical_space = 10 # 문단 여백\n",
        "#         top_margin = 50 # 맨 위 여백\n",
        "#         bottom_margin = 50 # 맨 아래 여백\n",
        "\n",
        "#         font_path = \"/content/malgun.ttf\"  # Replace with your font path\n",
        "\n",
        "#         # Load a Unicode font\n",
        "#         font = ImageFont.truetype(font_path, font_size)\n",
        "\n",
        "#         # Calculate image height and initialize variables\n",
        "#         lines = []\n",
        "#         line = \"\"\n",
        "\n",
        "#         for word in ground_truth_parse.split():\n",
        "#             # Check if adding the current word exceeds the available width\n",
        "#             if font.getsize(line + \" \" + word)[0] <= (image_width - left_margin - right_margin):\n",
        "#                 line += \" \" + word\n",
        "#             else:\n",
        "#                 lines.append(line)\n",
        "#                 line = word\n",
        "\n",
        "#         lines.append(line)  # Append the last line\n",
        "\n",
        "#         # Calculate image height based on the number of lines\n",
        "#         line_height = font.getsize('A')[1]  # Height of a line of text\n",
        "#         image_height = line_height * len(lines) + (len(lines) - 1) * vertical_space + top_margin + bottom_margin\n",
        "\n",
        "#         # Create a new image with margins\n",
        "#         image = Image.new('RGB', (image_width, image_height), color='white')\n",
        "#         draw = ImageDraw.Draw(image)\n",
        "\n",
        "#         # Draw the text on the image with margins\n",
        "#         y_position = top_margin  # Adjust the starting y position\n",
        "#         text_color = (0, 0, 0)  # Black color\n",
        "\n",
        "#         for line in lines:\n",
        "#             x_position = left_margin  # Adjust the starting x position\n",
        "#             draw.text((x_position, y_position), line, fill=text_color, font=font)\n",
        "#             y_position += line_height + vertical_space\n",
        "\n",
        "#         # Save the image as JPG\n",
        "#         image.save(f'/content/jpg/{j[:-5]}.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nPAFNSabL18",
        "outputId": "5336b42b-0e23-41f6-b763-ba89ed8dac3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-17ccff53c8a8>:54: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
            "  if font.getsize(line + \" \" + word)[0] <= (image_width - left_margin - right_margin):\n",
            "<ipython-input-7-17ccff53c8a8>:63: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
            "  line_height = font.getsize('A')[1]  # Height of a line of text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import shutil\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # 데이터가 들어있는 폴더 경로\n",
        "# json_folder = '/content/json'\n",
        "# jpg_folder = '/content/jpg'\n",
        "\n",
        "# # 데이터를 저장할 분할된 폴더 경로\n",
        "# train_folder = '/content/dataset/train'\n",
        "# test_folder = '/content/dataset/test'\n",
        "# valid_folder = '/content/dataset/valid'\n",
        "\n",
        "# # JSON 파일과 이미지 파일 목록을 가져옴\n",
        "# json_files = [f for f in os.listdir(json_folder) if f.endswith('.json')]\n",
        "# jpg_files = [f for f in os.listdir(jpg_folder) if f.endswith('.jpg')]\n",
        "\n",
        "# # 데이터를 train/test/valid로 분할\n",
        "# train_json, test_json, train_jpg, test_jpg = train_test_split(json_files, jpg_files, test_size=0.2, random_state=42)\n",
        "# train_json, valid_json, train_jpg, valid_jpg = train_test_split(train_json, train_jpg, test_size=0.2, random_state=42)\n",
        "\n",
        "# # 각 분할에 해당하는 폴더 생성\n",
        "# os.makedirs(train_folder, exist_ok=True)\n",
        "# os.makedirs(test_folder, exist_ok=True)\n",
        "# os.makedirs(valid_folder, exist_ok=True)\n",
        "\n",
        "# # 파일을 해당 폴더로 복사\n",
        "# for json_file, jpg_file in zip(train_json, train_jpg):\n",
        "#     shutil.copy(os.path.join(json_folder, json_file), os.path.join(train_folder, json_file))\n",
        "#     shutil.copy(os.path.join(jpg_folder, jpg_file), os.path.join(train_folder, jpg_file))\n",
        "\n",
        "# for json_file, jpg_file in zip(test_json, test_jpg):\n",
        "#     shutil.copy(os.path.join(json_folder, json_file), os.path.join(test_folder, json_file))\n",
        "#     shutil.copy(os.path.join(jpg_folder, jpg_file), os.path.join(test_folder, jpg_file))\n",
        "\n",
        "# for json_file, jpg_file in zip(valid_json, valid_jpg):\n",
        "#     shutil.copy(os.path.join(json_folder, json_file), os.path.join(valid_folder, json_file))\n",
        "#     shutil.copy(os.path.join(jpg_folder, jpg_file), os.path.join(valid_folder, jpg_file))\n"
      ],
      "metadata": {
        "id": "pP-YS7horYRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/donut/dataset/train\n",
        "!mkdir /content/donut/dataset/test\n",
        "!mkdir /content/donut/dataset/vaild"
      ],
      "metadata": {
        "id": "R6Kg_04HKqvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하나로 변경\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터가 들어있는 상위 폴더 경로\n",
        "folder = os.listdir('/content/sample')\n",
        "for i in folder:\n",
        "    per_path = f'/content/sample/{i}/20per'\n",
        "    sent_path = f'/content/sample/{i}/2~3sent'\n",
        "\n",
        "    # JSON 파일 목록\n",
        "    per_list = os.listdir(per_path)\n",
        "    sent_list = os.listdir(sent_path)\n",
        "\n",
        "    # 데이터를 train/test/valid로 분할\n",
        "    train_json, valid_json = train_test_split(sent_list, test_size=0.2, random_state=42)\n",
        "    test_json = per_list\n",
        "\n",
        "    train_path = '/content/donut/dataset/train'\n",
        "    valid_path = '/content/donut/dataset/test'\n",
        "    test_path = '/content/donut/dataset/vaild'\n",
        "\n",
        "    # train 데이터용 json 확인\n",
        "    for j in train_json:\n",
        "        with open(f'{sent_path}/{j}', 'r', encoding='utf-8') as file:\n",
        "            json_data = json.load(file)\n",
        "\n",
        "            image_path = f'{j[:-5]}.jpg'\n",
        "            ground_truth_parse = f\" {json_data['Meta(Refine)']['passage']}\"\n",
        "\n",
        "            metadata = {\n",
        "                \"file_name\": image_path,\n",
        "                \"ground_truth\": json.dumps({\"gt_parse\": {\"text_sequence\": ground_truth_parse}}, ensure_ascii=False)\n",
        "            }\n",
        "\n",
        "            # metadata.jsonl 저장\n",
        "            metadata_path = os.path.join(train_path, 'metadata.jsonl')\n",
        "            with open(metadata_path, 'a', encoding='utf-8') as f:\n",
        "                json.dump(metadata, f, ensure_ascii=False)\n",
        "                f.write('\\n')\n",
        "\n",
        "        # image text 넣어서 저장\n",
        "        image_width = 800\n",
        "        image_height = 1200\n",
        "        font_size = 18 # font size\n",
        "        left_margin = 50 # 왼쪽 여백\n",
        "        right_margin = 50 # 오른쪽 여백\n",
        "        vertical_space = 10 # 문단 여백\n",
        "        top_margin = 50 # 맨 위 여백\n",
        "        bottom_margin = 50 # 맨 아래 여백\n",
        "\n",
        "        font_path = \"/content/malgun.ttf\"  # font 경로\n",
        "\n",
        "        # 폰트 불러오기\n",
        "        font = ImageFont.truetype(font_path, font_size)\n",
        "\n",
        "        # 이미지 높이 계산 및 변수 초기화\n",
        "        lines = []\n",
        "        line = \"\"\n",
        "\n",
        "        for word in ground_truth_parse.split():\n",
        "            # Check if adding the current word exceeds the available width\n",
        "            if font.getsize(line + \" \" + word)[0] <= (image_width - left_margin - right_margin):\n",
        "                line += \" \" + word\n",
        "            else:\n",
        "                lines.append(line)\n",
        "                line = word\n",
        "\n",
        "        lines.append(line)  # 마지막 줄 추가\n",
        "\n",
        "        # 줄 수를 기준으로 이미지 높이 계산\n",
        "        line_height = font.getsize('A')[1]  # 텍스트 라인의 높이\n",
        "        image_height = line_height * len(lines) + (len(lines) - 1) * vertical_space + top_margin + bottom_margin\n",
        "\n",
        "        # 여백이 있는 새 이미지 만들기\n",
        "        image = Image.new('RGB', (image_width, image_height), color='white')\n",
        "\n",
        "        draw = ImageDraw.Draw(image)\n",
        "\n",
        "        # 여백을 사용하여 이미지에 텍스트 그리기\n",
        "        y_position = top_margin  # 시작 및 위치 조정\n",
        "        text_color = (0, 0, 0)  # 글자는 검은색\n",
        "\n",
        "        for line in lines:\n",
        "            x_position = left_margin  # 시작 x 위치 조정\n",
        "            draw.text((x_position, y_position), line, fill=text_color, font=font)\n",
        "            y_position += line_height + vertical_space\n",
        "\n",
        "        # 이미지를 JPG로 저장\n",
        "        image.save(f'{train_path}/{j[:-5]}.jpg')\n"
      ],
      "metadata": {
        "id": "f1-pNrROG6q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 변환"
      ],
      "metadata": {
        "id": "F7Cm3GetPk6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/outputs\n",
        "!mkdir /content/outputs/KoreanData"
      ],
      "metadata": {
        "id": "xG2iB5Bc1NQn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "foilderlist = os.listdir('/content/sample')\n",
        "\n",
        "# 'kowiki.text' 파일 내용 초기화\n",
        "with open(f'/content/donut/synthdog/resources/corpus/kowiki.txt', \"w\", encoding='utf-8') as fi:\n",
        "    fi.truncate(0)\n",
        "fi.close()\n",
        "\n",
        "for i in foilderlist:\n",
        "    per_path = f'/content/sample/{i}/20per'\n",
        "    sent_path = f'/content/sample/{i}/2~3sent'\n",
        "    per_list = os.listdir(per_path)\n",
        "    sent_list = os.listdir(sent_path)\n",
        "\n",
        "    # 원문만 불러오기\n",
        "    for a, j in enumerate(per_list):\n",
        "        with open(f'{per_path}/{j}', 'r', encoding='utf-8') as file:\n",
        "            text = json.load(file)\n",
        "            ground_truth_parse =  f\" {text['Meta(Refine)']['passage']}\"\n",
        "            paasage = ground_truth_parse.replace('\\n', ' ')\n",
        "\n",
        "            # 원문 'kowiki.text' 에 덮어쓰기\n",
        "            with open(f'/content/donut/synthdog/resources/corpus/kowiki.txt', 'a', encoding='utf-8') as f:\n",
        "                f.write(f'{paasage} \\n')"
      ],
      "metadata": {
        "id": "CIbPuAJV5Az9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/donut/synthdog\n",
        "# 환경 변수 설정\n",
        "!export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES\n",
        "\n",
        "# SynthDoG 실행\n",
        "!synthtiger -o ./outputs/KoreanData -c 10 -w 4 -v template.py SynthDoG config_ko.yaml"
      ],
      "metadata": {
        "id": "7HVo-otQPket",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb895a5d-3edd-41ea-aafa-259629beff66"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'config': '/content/donut/synthdog/config_ko.yaml',\n",
            " 'count': 10,\n",
            " 'name': 'SynthDoG',\n",
            " 'output': '/content/text',\n",
            " 'script': 'template.py',\n",
            " 'seed': None,\n",
            " 'verbose': True,\n",
            " 'worker': 4}\n",
            "{'aspect_ratio': [1, 2],\n",
            " 'background': {'effect': {'args': [{'args': {'sigma': [0, 10]}, 'prob': 1}]},\n",
            "                'image': {'paths': ['resources/background'], 'weights': [1]}},\n",
            " 'document': {'aspect_ratio': [1, 2],\n",
            "              'content': {'content_color': {'args': {'colorize': 1,\n",
            "                                                     'gray': [0, 64]},\n",
            "                                            'prob': 0.2},\n",
            "                          'font': {'bold': 0,\n",
            "                                   'paths': ['resources/font/ko'],\n",
            "                                   'weights': [1]},\n",
            "                          'layout': {'align': ['left', 'right', 'center'],\n",
            "                                     'fill': [0.5, 1],\n",
            "                                     'full': 0.1,\n",
            "                                     'max_col': 1,\n",
            "                                     'max_row': 5,\n",
            "                                     'stack_fill': [0.5, 1],\n",
            "                                     'stack_full': 0.1,\n",
            "                                     'stack_spacing': [0.0334, 0.0334],\n",
            "                                     'text_scale': [0.0334, 0.1]},\n",
            "                          'margin': [0, 0.1],\n",
            "                          'text': {'path': 'resources/corpus/kowiki.txt'},\n",
            "                          'textbox': {'fill': [0.5, 1]},\n",
            "                          'textbox_color': {'args': {'colorize': 1,\n",
            "                                                     'gray': [0, 64]},\n",
            "                                            'prob': 0.2}},\n",
            "              'effect': {'args': [{'args': {'alpha': [0, 1], 'sigma': [0, 0.5]},\n",
            "                                   'prob': 1},\n",
            "                                  {'args': {'per_channel': 0, 'scale': [0, 8]},\n",
            "                                   'prob': 1},\n",
            "                                  {'args': {'args': [{'percents': [[0.75, 1],\n",
            "                                                                   [0.75, 1],\n",
            "                                                                   [0.75, 1],\n",
            "                                                                   [0.75, 1]]},\n",
            "                                                     {'percents': [[0.75, 1],\n",
            "                                                                   [1, 1],\n",
            "                                                                   [0.75, 1],\n",
            "                                                                   [1, 1]]},\n",
            "                                                     {'percents': [[1, 1],\n",
            "                                                                   [0.75, 1],\n",
            "                                                                   [1, 1],\n",
            "                                                                   [0.75, 1]]},\n",
            "                                                     {'percents': [[0.75, 1],\n",
            "                                                                   [1, 1],\n",
            "                                                                   [1, 1],\n",
            "                                                                   [1, 1]]},\n",
            "                                                     {'percents': [[1, 1],\n",
            "                                                                   [0.75, 1],\n",
            "                                                                   [1, 1],\n",
            "                                                                   [1, 1]]},\n",
            "                                                     {'percents': [[1, 1],\n",
            "                                                                   [1, 1],\n",
            "                                                                   [0.75, 1],\n",
            "                                                                   [1, 1]]},\n",
            "                                                     {'percents': [[1, 1],\n",
            "                                                                   [1, 1],\n",
            "                                                                   [1, 1],\n",
            "                                                                   [0.75, 1]]},\n",
            "                                                     {'percents': [[1, 1],\n",
            "                                                                   [1, 1],\n",
            "                                                                   [1, 1],\n",
            "                                                                   [1, 1]]}],\n",
            "                                            'weights': [750,\n",
            "                                                        50,\n",
            "                                                        50,\n",
            "                                                        25,\n",
            "                                                        25,\n",
            "                                                        25,\n",
            "                                                        25,\n",
            "                                                        50]},\n",
            "                                   'prob': 1}]},\n",
            "              'fullscreen': 0.5,\n",
            "              'landscape': 0.5,\n",
            "              'paper': {'image': {'alpha': [0, 0.2],\n",
            "                                  'crop': 1,\n",
            "                                  'grayscale': 1,\n",
            "                                  'paths': ['resources/paper'],\n",
            "                                  'weights': [1]}},\n",
            "              'short_size': [480, 1024]},\n",
            " 'effect': {'args': [{'args': {'alpha': [0, 0.2],\n",
            "                               'rgb': [[0, 255], [0, 255], [0, 255]]},\n",
            "                      'prob': 0.2},\n",
            "                     {'args': {'amount': [0, 1],\n",
            "                               'bidirectional': 0,\n",
            "                               'intensity': [0, 160],\n",
            "                               'smoothing': [0.5, 1]},\n",
            "                      'prob': 1},\n",
            "                     {'args': {'alpha': [1, 1.5]}, 'prob': 1},\n",
            "                     {'args': {'beta': [-48, 0]}, 'prob': 1},\n",
            "                     {'args': {'angle': [0, 360], 'k': [3, 5]}, 'prob': 0.5},\n",
            "                     {'args': {'sigma': [0, 1.5]}, 'prob': 1}]},\n",
            " 'landscape': 0.5,\n",
            " 'quality': [50, 95],\n",
            " 'short_size': [720, 1024]}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/synthtiger\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/synthtiger/main.py\", line 114, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/synthtiger/main.py\", line 21, in run\n",
            "    template = synthtiger.read_template(args.script, args.name, config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/synthtiger/gen.py\", line 24, in read_template\n",
            "    template = getattr(__import__(module), name)(config)\n",
            "ModuleNotFoundError: No module named 'template'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/donut/synthdog/outputs/KoreanData/train/image_0.jpg"
      ],
      "metadata": {
        "id": "vSFSwwTeNYXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dount"
      ],
      "metadata": {
        "id": "mD1c0GD_pmMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 이미지 넣고 실험"
      ],
      "metadata": {
        "id": "mgtuG9BVp7MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print ( \"CUDA 사용 가능:\" , torch.cuda.is_available())\n",
        "!nvcc --version\n",
        "# 이제 관련 파일과 라이브러리를 다운로드할 수 있습니다.\n",
        "#pip install donut-python.\n",
        "\n",
        "!git clone https://github.com/clovaai/donut.git\n",
        "!cd donut && pip install .\n",
        "# Making an inference using the CORD fine-tuned model\n",
        "# First, we’ll demonstrate basic usage of the model."
      ],
      "metadata": {
        "id": "NYk2UoP0p_Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from donut import DonutModel\n",
        "from PIL import Image\n",
        "\n",
        "model = DonutModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
        "if torch.cuda.is_available():\n",
        "    model.half()\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "else:\n",
        "    model.encoder.to(torch.bfloat16)\n",
        "model.eval()\n",
        "image = Image.open(\"/content/jpg/REPORT-briefing-20091-00001.jpg\")\n",
        "image.convert(\"RGB\")\n",
        "output = model.inference(image=image, prompt=\"<s_cord-v2>\")\n",
        "output"
      ],
      "metadata": {
        "id": "JEH_4GmYokZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34557b4-4cb7-4448-ae5d-ac662c0777ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Some weights of the model checkpoint at naver-clova-ix/donut-base-finetuned-cord-v2 were not used when initializing DonutModel: ['encoder.model.norm.weight', 'encoder.model.norm.bias']\n",
            "- This IS expected if you are initializing DonutModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DonutModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'predictions': [{'cnt': '5'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 데모버전"
      ],
      "metadata": {
        "id": "DZmDVGcfrVu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import gradio as gr\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from donut import DonutModel"
      ],
      "metadata": {
        "id": "g2Mn2wW-UNZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_process_vqa(input_img, question):\n",
        "    global pretrained_model, task_prompt, task_name\n",
        "    input_img = Image.fromarray(input_img)\n",
        "    user_prompt = task_prompt.replace(\"{user_input}\", question)\n",
        "    output = pretrained_model.inference(input_img, prompt=user_prompt)[\"predictions\"][0]\n",
        "    return output\n",
        "\n",
        "\n",
        "def demo_process(input_img):\n",
        "    global pretrained_model, task_prompt, task_name\n",
        "    input_img = Image.fromarray(input_img)\n",
        "    output = pretrained_model.inference(image=input_img, prompt=task_prompt)[\"predictions\"][0]\n",
        "    return output"
      ],
      "metadata": {
        "id": "8_alWsH5UXWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--task\", type=str, default=\"cord-v2\")\n",
        "parser.add_argument(\"--pretrained_path\", type=str, default=\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
        "args, left_argv = parser.parse_known_args()\n",
        "\n",
        "task_name = args.task\n",
        "if \"docvqa\" == task_name:\n",
        "    task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n",
        "else:  # rvlcdip, cord, ...\n",
        "    task_prompt = f\"<s_{task_name}>\"\n",
        "\n",
        "pretrained_model = DonutModel.from_pretrained(args.pretrained_path)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    pretrained_model.half()\n",
        "    device = torch.device(\"cuda\")\n",
        "    pretrained_model.to(device)\n",
        "else:\n",
        "    pretrained_model.encoder.to(torch.bfloat16)\n",
        "\n",
        "pretrained_model.eval()\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=demo_process_vqa if task_name == \"docvqa\" else demo_process,\n",
        "    inputs=[\"image\", \"text\"] if task_name == \"docvqa\" else \"image\",\n",
        "    outputs=\"json\",\n",
        "    title=f\"Donut 🍩 demonstration for `{task_name}` task\",\n",
        ")\n",
        "\n",
        "\n",
        "demo.launch(debug=False, share=True`)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "f7RoSOEXUa6i",
        "outputId": "cb6d0917-0854-4c4f-8398-4f5feceea705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at naver-clova-ix/donut-base-finetuned-cord-v2 were not used when initializing DonutModel: ['encoder.model.norm.weight', 'encoder.model.norm.bias']\n",
            "- This IS expected if you are initializing DonutModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DonutModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://d8fbcfc1376251dc7c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d8fbcfc1376251dc7c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 훈련 .. 하는 중"
      ],
      "metadata": {
        "id": "jAOGei7aynOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# config_data ={\n",
        "#   \"_name_or_path\": \"naver-clova-ix/donut-base\",\n",
        "#   \"align_long_axis\": False,\n",
        "#   \"architectures\": [\n",
        "#     \"DonutModel\"\n",
        "#   ],\n",
        "#   \"decoder_layer\": 4,\n",
        "#   \"encoder_layer\": [\n",
        "#     2,\n",
        "#     2,\n",
        "#     14,\n",
        "#     2\n",
        "#   ],\n",
        "#   \"input_size\": [\n",
        "#     800,\n",
        "#     1200\n",
        "#   ],\n",
        "#   \"max_length\": 768,\n",
        "#   \"max_position_embeddings\": 768,\n",
        "#   \"model_type\": \"donut\",\n",
        "#   \"torch_dtype\": \"float32\",\n",
        "#   \"transformers_version\": \"4.11.3\",\n",
        "#   \"window_size\": 10\n",
        "# }\n",
        "\n",
        "# with open('/content/donut/config.json', 'w', encoding = 'utf-8') as file:\n",
        "#     json.dump(config_data, file, indent=4)"
      ],
      "metadata": {
        "id": "MzMQNLUpRxM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = DonutModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "69777b5aa6ff43c4a30a94ebdf23b6ad",
            "2c07957bef7c4231be6ef67beffc12d8",
            "78eb37cff0da433f98448f2f6e9e03bb",
            "05e4c9e30034484ca0c215747b4915e1",
            "90d9358e83e64eba877372a0fe3525ae",
            "7253ece8a3fa4bada2a9c222bf2d06dc",
            "aafd20936fca424897c0c1eb0044943b",
            "32deeff688b6487a9e749ae283cc2e99",
            "4cd7892d5a91435199734612b6f536c4",
            "2d5668a02a5646cb926522c3d65b8d11",
            "a67d1cd870f44a20a587f548450cf5b4",
            "55b9711af76f4f4e954c5fe2fd95f302",
            "2c79a520391a40b0a70b1449673c4fea",
            "2f8e07741394458a9dfc2fe65683f480",
            "987f33366ed44edd907f8baef218a3b7",
            "3016eb293a8047dbb78c9512651630a6",
            "54d14ecdf4974ae2980a4f2124571ba5",
            "dd35fdbfabc74c4b916a20ab20d3d688",
            "23e959eea5a0456d8f20cf4dec9e7931",
            "ed2274a0218145aaa7d4babe16c69069",
            "ce896ef248944869a6c33cc4792b46a0",
            "db58552a9d6e4ce987fbfafac4c4e6d7"
          ]
        },
        "id": "kze1SqROS90Y",
        "outputId": "581b0ec6-a463-4c52-b075-f2a44e176c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)official/config.json:   0%|          | 0.00/404 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69777b5aa6ff43c4a30a94ebdf23b6ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/859M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55b9711af76f4f4e954c5fe2fd95f302"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Some weights of the model checkpoint at naver-clova-ix/donut-base-finetuned-cord-v2 were not used when initializing DonutModel: ['encoder.model.norm.bias', 'encoder.model.norm.weight']\n",
            "- This IS expected if you are initializing DonutModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DonutModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cuda 사용하도록 변경\n",
        "import torch\n",
        "print ( \"CUDA 사용 가능:\" , torch.cuda.is_available())\n",
        "!nvcc --version\n",
        "\n",
        "# 관련 라이브러리, 파일 설치\n",
        "!git clone https://github.com/clovaai/donut.git\n",
        "!cd donut && pip install .\n",
        "\n",
        "## 모델 학습\n",
        "# 만든 데이터 경로 train_cord.yaml에서 변경\n",
        "# (예시) dataset_name_or_paths: [/content/donut/synthdog/outputs/KoreanData]\n",
        "!python /content/donut/train.py --config /content/donut/config/train_cord.yaml --pretrained_model_name_or_path  \"naver-clova-ix/donut-base-finetuned-cord-v2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjXeSKCjchoP",
        "outputId": "389344a9-83d5-4f73-c5ad-2f52a6ac161a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: 100% 40/40 [00:23<00:00,  1.67it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                                                          \n",
            "\u001b[APrediction: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탄으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞츄형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞츄 형 지원 과 과필 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 2: 100% 40/40 [00:25<00:00,  1.56it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A    Answer: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탕으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞춤형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞춤 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 2: 100% 40/40 [00:25<00:00,  1.56it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A Normed ED: 0.0855457227138643\n",
            "Epoch 2: 100% 40/40 [00:25<00:00,  1.56it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[APrediction: 뒤에 호 다리 의 역할을 하는 수신명의 여자 들이 숨으로 있고, 마이는 텃텃텃에서 젖을 하였 는 듯한 모습이다.\n",
            "Epoch 2: 100% 40/40 [00:26<00:00,  1.54it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A    Answer: 뒤에 놋 다리 의 역할을 하는 수십명의 여자 들이 모두 허리를 굽혀 앞사람의 허리를 두 손으로 잡고, 머리는 앞사람의 궁둥이 왼편 에 대는데 마치 생선을 꿰어 놓은 듯한 모습이다.\n",
            "Epoch 2: 100% 40/40 [00:26<00:00,  1.54it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A Normed ED: 0.52\n",
            "Epoch 2: 100% 40/40 [00:26<00:00,  1.54it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[APrediction: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져간다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 범조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 2: 100% 40/40 [00:26<00:00,  1.48it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A    Answer: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져갔다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 2: 100% 40/40 [00:26<00:00,  1.48it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A Normed ED: 0.010869565217391304\n",
            "Epoch 2: 100% 40/40 [00:26<00:00,  1.48it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[APrediction: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원주자 폭탄 완성에 가까워졌다는 내용이 나오면 하이(체베크를 사실하라 는 명령을 내렸다.1945년 1철 하이 전베베크는 나머지 직원 대부분과 함께 카이저 펼펼첼를 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무는 독일이 원자 폭탄 프로그래을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 과정에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 클린은 많은 독일 과 학 연\n",
            "Epoch 2: 100% 40/40 [00:28<00:00,  1.39it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A    Answer: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭탄 완성에 가까워졌다는 내용이 나오면 하이젠베르크를 사살하라 는 명령을 내렸다.1945년 1월 하이 젠베르크는 나머지 직원 대부분과 함께 카이저 빌헬름 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 작전에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 2: 100% 40/40 [00:28<00:00,  1.39it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A Normed ED: 0.06201550387596899\n",
            "Epoch 2: 100% 40/40 [00:28<00:00,  1.39it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation DataLoader 0:   0% 0/4 [00:04<?, ?it/s]\u001b[A\n",
            "Epoch 2: 100% 40/40 [00:28<00:00,  1.39it/s, loss=0.202, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Epoch 5: 100% 40/40 [00:23<00:00,  1.72it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                                                          \n",
            "\u001b[APrediction: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탄으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞츄형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 르 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞츄 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 5: 100% 40/40 [00:24<00:00,  1.61it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A    Answer: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탕으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞춤형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞춤 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 5: 100% 40/40 [00:24<00:00,  1.61it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A Normed ED: 0.011799410029498525\n",
            "Epoch 5: 100% 40/40 [00:24<00:00,  1.61it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[APrediction: 뒤에 돗 다리 의 역할을 하는 수신명의 여자 들이 몬으로 왔고, 마이도는 망자리왕이 묻혔다.마이치 성인왕이 묻혔다.마이치 성인왕이 묻은 도란 모습이다.\n",
            "Epoch 5: 100% 40/40 [00:25<00:00,  1.57it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A    Answer: 뒤에 놋 다리 의 역할을 하는 수십명의 여자 들이 모두 허리를 굽혀 앞사람의 허리를 두 손으로 잡고, 머리는 앞사람의 궁둥이 왼편 에 대는데 마치 생선을 꿰어 놓은 듯한 모습이다.\n",
            "Epoch 5: 100% 40/40 [00:25<00:00,  1.57it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A Normed ED: 0.57\n",
            "Epoch 5: 100% 40/40 [00:25<00:00,  1.57it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[APrediction: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져간다. 하지만 송 부시장은 청와대 원선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 5: 100% 40/40 [00:26<00:00,  1.52it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A    Answer: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져갔다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 5: 100% 40/40 [00:26<00:00,  1.52it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A Normed ED: 0.010869565217391304\n",
            "Epoch 5: 100% 40/40 [00:26<00:00,  1.52it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[APrediction: 권총을 갖고 강의에 참 석하다.독 했으며 강의에서 독일이 원자폭탄 앙상에 가까워졌다는 내용이 나오면 하이(천베르크를 사실하라 는 명령을 내렸다.1945년 1월 하이 전베르크는 나머지 직원 대부분과 함께 카이저 브월에 올라란 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 과정에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 5: 100% 40/40 [00:28<00:00,  1.42it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A    Answer: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭탄 완성에 가까워졌다는 내용이 나오면 하이젠베르크를 사살하라 는 명령을 내렸다.1945년 1월 하이 젠베르크는 나머지 직원 대부분과 함께 카이저 빌헬름 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 작전에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 5: 100% 40/40 [00:28<00:00,  1.42it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A Normed ED: 0.043701799485861184\n",
            "Epoch 5: 100% 40/40 [00:28<00:00,  1.42it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation DataLoader 0:   0% 0/4 [00:04<?, ?it/s]\u001b[A\n",
            "Epoch 5: 100% 40/40 [00:28<00:00,  1.42it/s, loss=0.139, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Epoch 8: 100% 40/40 [00:23<00:00,  1.72it/s, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]  \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                                                          \n",
            "\u001b[APrediction: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탄으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞츄형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 북추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞츄 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 8: 100% 40/40 [00:28<00:00,  1.38it/s, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A    Answer: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탕으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞춤형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞춤 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 8: 100% 40/40 [00:28<00:00,  1.38it/s, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A Normed ED: 0.011799410029498525\n",
            "Epoch 8: 100% 40/40 [00:28<00:00,  1.38it/s, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[APrediction: 뒤에 늦 다리 의 역할을 하는 수신명의 이자 들이 곤으로 찾고, 마이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다. 다이 다이 다이 다이 다이 다. 다이 다이 다이 다이 다. 다이 다이 다이 다. 다이 다이 다이 다. 다이 다이 다이 다. 다이 다이 다이 다. 다이 다이 다이 다. 다이 다이 다이 다이 다. 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다이 다 다이 다이 다 다이 다 다이 다 다이 다 다 다이 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다 다\n",
            "Epoch 8: 100% 40/40 [00:37<00:00,  1.08it/s, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A    Answer: 뒤에 놋 다리 의 역할을 하는 수십명의 여자 들이 모두 허리를 굽혀 앞사람의 허리를 두 손으로 잡고, 머리는 앞사람의 궁둥이 왼편 에 대는데 마치 생선을 꿰어 놓은 듯한 모습이다.\n",
            "Epoch 8: 100% 40/40 [00:37<00:00,  1.08it/s, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A Normed ED: 0.9659815733522324\n",
            "Epoch 8: 100% 40/40 [00:37<00:00,  1.08it/s, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[APrediction: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져있다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 8: 100% 40/40 [00:38<00:00,  1.04it/s, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A    Answer: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져갔다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 8: 100% 40/40 [00:38<00:00,  1.04it/s, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A Normed ED: 0.005434782608695652\n",
            "Epoch 8: 100% 40/40 [00:38<00:00,  1.04it/s, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[APrediction: 권총을 갖고 강의에 참 식하도록 했으며 강의에서 독일이 칠자폭탄 앙상에 가까워졌다는 내장이 나오면 하이(레베르크를 사실하라 는 명령을 내렸다.1945년 1월 하이 전베르크는 나머지 직원 대부분과 함께 카이져 릴릴을 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 일부.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 과정에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 8: 100% 40/40 [00:40<00:00,  1.01s/it, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A    Answer: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭탄 완성에 가까워졌다는 내용이 나오면 하이젠베르크를 사살하라 는 명령을 내렸다.1945년 1월 하이 젠베르크는 나머지 직원 대부분과 함께 카이저 빌헬름 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 작전에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 8: 100% 40/40 [00:40<00:00,  1.01s/it, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                          \n",
            "\u001b[A Normed ED: 0.04381443298969072\n",
            "Epoch 8: 100% 40/40 [00:40<00:00,  1.01s/it, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation DataLoader 0:   0% 0/4 [00:17<?, ?it/s]\u001b[A\n",
            "Epoch 8: 100% 40/40 [00:40<00:00,  1.01s/it, loss=2.570, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Epoch 11: 100% 40/40 [00:24<00:00,  1.63it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                                                             \n",
            "\u001b[APrediction: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탄으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞츄형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맛츄 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 11: 100% 40/40 [00:26<00:00,  1.48it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A    Answer: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탕으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞춤형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞춤 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 11: 100% 40/40 [00:26<00:00,  1.48it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A Normed ED: 0.011799410029498525\n",
            "Epoch 11: 100% 40/40 [00:26<00:00,  1.48it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[APrediction: 뒤에 갓 다리 의 역할을 하는 수십명의 이자 들이 주의로 짓고, 이는 망하를 갖도록 하였 다녔다 히 는 도란 모습이다.\n",
            "Epoch 11: 100% 40/40 [00:27<00:00,  1.45it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A    Answer: 뒤에 놋 다리 의 역할을 하는 수십명의 여자 들이 모두 허리를 굽혀 앞사람의 허리를 두 손으로 잡고, 머리는 앞사람의 궁둥이 왼편 에 대는데 마치 생선을 꿰어 놓은 듯한 모습이다.\n",
            "Epoch 11: 100% 40/40 [00:27<00:00,  1.45it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A Normed ED: 0.54\n",
            "Epoch 11: 100% 40/40 [00:27<00:00,  1.45it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[APrediction: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져간다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 11: 100% 40/40 [00:28<00:00,  1.39it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A    Answer: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져갔다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 11: 100% 40/40 [00:28<00:00,  1.39it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A Normed ED: 0.005434782608695652\n",
            "Epoch 11: 100% 40/40 [00:28<00:00,  1.39it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[APrediction: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭한 와성에 가까워졌다는 내장이 나오면 하이(燕山)를 사실하라 는 명령을 내렸다.1945년 1절 하이 전베르크는 나마지 직원 대부분과 함께 카이저 믿힐을 물려한 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무는 독일이 원자 폭탄 포로그룹을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 자하 인력을 이용하 려는 연합군의 노력이었다. 이 과정에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 11: 100% 40/40 [00:31<00:00,  1.26it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A    Answer: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭탄 완성에 가까워졌다는 내용이 나오면 하이젠베르크를 사살하라 는 명령을 내렸다.1945년 1월 하이 젠베르크는 나머지 직원 대부분과 함께 카이저 빌헬름 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 작전에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 11: 100% 40/40 [00:31<00:00,  1.26it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A Normed ED: 0.07493540051679587\n",
            "Epoch 11: 100% 40/40 [00:31<00:00,  1.26it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation DataLoader 0:   0% 0/4 [00:07<?, ?it/s]\u001b[A\n",
            "Epoch 11: 100% 40/40 [00:31<00:00,  1.26it/s, loss=0.00206, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Epoch 14: 100% 40/40 [00:25<00:00,  1.57it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탄으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞추형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞츄 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 14: 100% 40/40 [00:27<00:00,  1.48it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탕으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞춤형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞춤 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 14: 100% 40/40 [00:27<00:00,  1.48it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.08259587020648967\n",
            "Epoch 14: 100% 40/40 [00:27<00:00,  1.48it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 뒤에 츠 다리 의 역할을 하는 수십명의 여자 들이 곤으로 짓고, 마녀고 등지들의 곳을 두 번 에 하는데 여겨지 않은 마이 노은 도란 모습이다.\n",
            "Epoch 14: 100% 40/40 [00:27<00:00,  1.45it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 뒤에 놋 다리 의 역할을 하는 수십명의 여자 들이 모두 허리를 굽혀 앞사람의 허리를 두 손으로 잡고, 머리는 앞사람의 궁둥이 왼편 에 대는데 마치 생선을 꿰어 놓은 듯한 모습이다.\n",
            "Epoch 14: 100% 40/40 [00:27<00:00,  1.45it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.48\n",
            "Epoch 14: 100% 40/40 [00:27<00:00,  1.45it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져간다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 14: 100% 40/40 [00:28<00:00,  1.41it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져갔다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 14: 100% 40/40 [00:28<00:00,  1.41it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.005434782608695652\n",
            "Epoch 14: 100% 40/40 [00:28<00:00,  1.41it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 권총을 갖고 강의에 참 석하다로 했으며 강의에서 독일이 원자폭탄의 상쇄에 가까워졌다는 내용이 나오면 하이츠베르크를 사실하라 는 명령을 내렸다.1945년 1절 하이 츠베르크는 나머자 직원 대부분과 함께 카이저 링첼를 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 과정에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 14: 100% 40/40 [00:30<00:00,  1.32it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭탄 완성에 가까워졌다는 내용이 나오면 하이젠베르크를 사살하라 는 명령을 내렸다.1945년 1월 하이 젠베르크는 나머지 직원 대부분과 함께 카이저 빌헬름 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 작전에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 14: 100% 40/40 [00:30<00:00,  1.32it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.03865979381443299\n",
            "Epoch 14: 100% 40/40 [00:30<00:00,  1.32it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation DataLoader 0:   0% 0/4 [00:04<?, ?it/s]\u001b[A\n",
            "Epoch 14: 100% 40/40 [00:30<00:00,  1.32it/s, loss=0.000126, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Epoch 17: 100% 40/40 [00:22<00:00,  1.74it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탄으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞추형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맛-중 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 17: 100% 40/40 [00:26<00:00,  1.49it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탕으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞춤형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞춤 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 17: 100% 40/40 [00:26<00:00,  1.49it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.014705882352941176\n",
            "Epoch 17: 100% 40/40 [00:26<00:00,  1.49it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 뒤에 갓 다리 의 역할을 하는 수신명의 여자 들이 곤으로 짓고, 마이드 와서협력이 구축할 때에 대한 미국적 영향력을 하여 후손 도란 모습이다.\n",
            "Epoch 17: 100% 40/40 [00:28<00:00,  1.41it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 뒤에 놋 다리 의 역할을 하는 수십명의 여자 들이 모두 허리를 굽혀 앞사람의 허리를 두 손으로 잡고, 머리는 앞사람의 궁둥이 왼편 에 대는데 마치 생선을 꿰어 놓은 듯한 모습이다.\n",
            "Epoch 17: 100% 40/40 [00:28<00:00,  1.41it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.53\n",
            "Epoch 17: 100% 40/40 [00:28<00:00,  1.41it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져간다. 하지만 송 부시장은 청와대 원선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 범조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 17: 100% 40/40 [00:30<00:00,  1.31it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져갔다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 17: 100% 40/40 [00:30<00:00,  1.31it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.016304347826086956\n",
            "Epoch 17: 100% 40/40 [00:30<00:00,  1.31it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 권총을 갖고 강의에 참 석하다로 했으며 강의에서 독일이 원자폭한 일선에 가까워졌다는 내용이 나오면 하이레베르크를 사실하라 는 명령을 내렸다.1945년 1월 하이 레베르크는 나머지 직원 대부분과 함께 카이저 믿힐을 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 일부.알소스 일두는 독일이 원자 폭탄 파로 그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 자학 인격을 이용하 려는 연합군의 노력이었다. 이 과정에 두 일 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 17: 100% 40/40 [00:32<00:00,  1.23it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭탄 완성에 가까워졌다는 내용이 나오면 하이젠베르크를 사살하라 는 명령을 내렸다.1945년 1월 하이 젠베르크는 나머지 직원 대부분과 함께 카이저 빌헬름 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 작전에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 17: 100% 40/40 [00:32<00:00,  1.23it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.061696658097686374\n",
            "Epoch 17: 100% 40/40 [00:32<00:00,  1.23it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation DataLoader 0:   0% 0/4 [00:09<?, ?it/s]\u001b[A\n",
            "Epoch 17: 100% 40/40 [00:32<00:00,  1.23it/s, loss=0.000618, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Epoch 20: 100% 40/40 [00:22<00:00,  1.78it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탄으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞추형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞~ 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 20: 100% 40/40 [00:27<00:00,  1.45it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탕으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞춤형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞춤 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 20: 100% 40/40 [00:27<00:00,  1.45it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.008849557522123894\n",
            "Epoch 20: 100% 40/40 [00:27<00:00,  1.45it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 뒤에 츠 다리 의 역할을 하는 수신명의 여자 들이 온으로 컸고, 마이드 망처럼 아름답고 했다.중앙에 대한 해 미는 자신의 영감을 하며 늦는 듯한 모습이다.\n",
            "Epoch 20: 100% 40/40 [00:29<00:00,  1.37it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 뒤에 놋 다리 의 역할을 하는 수십명의 여자 들이 모두 허리를 굽혀 앞사람의 허리를 두 손으로 잡고, 머리는 앞사람의 궁둥이 왼편 에 대는데 마치 생선을 꿰어 놓은 듯한 모습이다.\n",
            "Epoch 20: 100% 40/40 [00:29<00:00,  1.37it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.52\n",
            "Epoch 20: 100% 40/40 [00:29<00:00,  1.37it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져간다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 범조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 20: 100% 40/40 [00:30<00:00,  1.31it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져갔다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 20: 100% 40/40 [00:30<00:00,  1.31it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.010869565217391304\n",
            "Epoch 20: 100% 40/40 [00:30<00:00,  1.31it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 권총을 갖고 강의에 참 석하다.돌 했으며 강의에서 독일이 원자폭한 완성에 가까워졌다는 내용이 나오면 하이 전베르크를 사살하라 는 명령을 내렸다.1945년 1칠 하이 전베르크는 나마자 직원 대부분과 함께 카이저 믿혔를 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 일부.알소스 일부는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 묻자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 과정에 두일 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 20: 100% 40/40 [00:33<00:00,  1.19it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭탄 완성에 가까워졌다는 내용이 나오면 하이젠베르크를 사살하라 는 명령을 내렸다.1945년 1월 하이 젠베르크는 나머지 직원 대부분과 함께 카이저 빌헬름 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 작전에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 20: 100% 40/40 [00:33<00:00,  1.19it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.056555269922879174\n",
            "Epoch 20: 100% 40/40 [00:33<00:00,  1.19it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation DataLoader 0:   0% 0/4 [00:10<?, ?it/s]\u001b[A\n",
            "Epoch 20: 100% 40/40 [00:33<00:00,  1.19it/s, loss=0.000701, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Epoch 23: 100% 40/40 [00:24<00:00,  1.63it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탄으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞츄형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맛츄 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 23: 100% 40/40 [00:28<00:00,  1.38it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탕으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞춤형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞춤 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 23: 100% 40/40 [00:28<00:00,  1.38it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.011799410029498525\n",
            "Epoch 23: 100% 40/40 [00:28<00:00,  1.38it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 뒤에 츠 다리 의 역할을 하는 수십명의 여자 들이 츠의 컸다.이닝 무너지는 단지방향의 움직임에도 아름답다.중국 웅전 하얀 마을 타고 여러 차지 방지를 마을린을 이어 늦는 듯한 모습이다.\n",
            "Epoch 23: 100% 40/40 [00:30<00:00,  1.33it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 뒤에 놋 다리 의 역할을 하는 수십명의 여자 들이 모두 허리를 굽혀 앞사람의 허리를 두 손으로 잡고, 머리는 앞사람의 궁둥이 왼편 에 대는데 마치 생선을 꿰어 놓은 듯한 모습이다.\n",
            "Epoch 23: 100% 40/40 [00:30<00:00,  1.33it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.5145631067961165\n",
            "Epoch 23: 100% 40/40 [00:30<00:00,  1.33it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져간다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 23: 100% 40/40 [00:31<00:00,  1.27it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져갔다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 23: 100% 40/40 [00:31<00:00,  1.27it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.005434782608695652\n",
            "Epoch 23: 100% 40/40 [00:31<00:00,  1.27it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[APrediction: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭한 양성에 가까워졌다는 내용이 나오면 하이 전베르크를 사실하라 는 명령을 내렸다.1945년 1월 하이 전베르크는 나머지 직원 대부분과 함께 카이저 링힐를 물리학 연구소에 서 슈바르발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 과정에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 23: 100% 40/40 [00:34<00:00,  1.16it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A    Answer: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭탄 완성에 가까워졌다는 내용이 나오면 하이젠베르크를 사살하라 는 명령을 내렸다.1945년 1월 하이 젠베르크는 나머지 직원 대부분과 함께 카이저 빌헬름 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 작전에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 23: 100% 40/40 [00:34<00:00,  1.16it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                              \n",
            "\u001b[A Normed ED: 0.031007751937984496\n",
            "Epoch 23: 100% 40/40 [00:34<00:00,  1.16it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation DataLoader 0:   0% 0/4 [00:09<?, ?it/s]\u001b[A\n",
            "Epoch 23: 100% 40/40 [00:34<00:00,  1.16it/s, loss=0.000136, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Epoch 26: 100% 40/40 [00:25<00:00,  1.59it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                                                             \n",
            "\u001b[APrediction: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탄으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞추형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맛 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 26: 100% 40/40 [00:26<00:00,  1.50it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A    Answer: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탕으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞춤형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞춤 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 26: 100% 40/40 [00:26<00:00,  1.50it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A Normed ED: 0.011799410029498525\n",
            "Epoch 26: 100% 40/40 [00:26<00:00,  1.50it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[APrediction: 뒤에 갓 다리 의 역할을 하는 수십명의 여자 들이 츠로 컸고, 마을 가지는 칼라비를 만들 어지 않았다. 이곳 역시 가장 먼저 까운 곳이다.\n",
            "Epoch 26: 100% 40/40 [00:27<00:00,  1.47it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A    Answer: 뒤에 놋 다리 의 역할을 하는 수십명의 여자 들이 모두 허리를 굽혀 앞사람의 허리를 두 손으로 잡고, 머리는 앞사람의 궁둥이 왼편 에 대는데 마치 생선을 꿰어 놓은 듯한 모습이다.\n",
            "Epoch 26: 100% 40/40 [00:27<00:00,  1.47it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A Normed ED: 0.56\n",
            "Epoch 26: 100% 40/40 [00:27<00:00,  1.47it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[APrediction: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져간다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 범조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 26: 100% 40/40 [00:28<00:00,  1.42it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A    Answer: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져갔다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 26: 100% 40/40 [00:28<00:00,  1.42it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A Normed ED: 0.010869565217391304\n",
            "Epoch 26: 100% 40/40 [00:28<00:00,  1.42it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[APrediction: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭한 완성에 가까워졌다는 내용이 나오면 하이전베르크를 사실하라 는 명령을 내렸다.1945년 1월 하이 전베르크는 나머지 직원 대부분과 함께 카이저 링컬를 물리라 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인격을 이용하 라는 연합군의 노력이었다. 이 과정에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 26: 100% 40/40 [00:29<00:00,  1.34it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A    Answer: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭탄 완성에 가까워졌다는 내용이 나오면 하이젠베르크를 사살하라 는 명령을 내렸다.1945년 1월 하이 젠베르크는 나머지 직원 대부분과 함께 카이저 빌헬름 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 작전에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 26: 100% 40/40 [00:29<00:00,  1.34it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A Normed ED: 0.04909560723514212\n",
            "Epoch 26: 100% 40/40 [00:29<00:00,  1.34it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation DataLoader 0:   0% 0/4 [00:04<?, ?it/s]\u001b[A\n",
            "Epoch 26: 100% 40/40 [00:29<00:00,  1.34it/s, loss=0.00409, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Epoch 29: 100% 40/40 [00:25<00:00,  1.59it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453] \n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "                                                                                                             \n",
            "\u001b[APrediction: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탄으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞추형 지원과 학교 교육여건을 개선해 나 가고자 합니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맛중 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 29: 100% 40/40 [00:27<00:00,  1.45it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A    Answer: 양에 대한 요 구가 크다는 것 을 알 수 있습니 다. 인천교육청 에서는 이런 결과를 바탕으 로 교육회복 과 더 나 은 인천미래교육 을 실현하기 위 해 학습결손 및 격 차 해소를 우선 과제 로 설정하고, 학 생 맞춤형 지원과 학교 교육여건을 개선해 나 가고자 합 니다. 인천 교육청에서는 인 천교육회 복추진 단을 구성 하였습니다. 인천교육회복추진단은 코로나19 로 인해 발생한 각종 결손을 회복하여 더 나 은 인천교육으로의 도약을 준 비하기 위한 기구입니 다. 인천교육 회복추진단에서는 방역, 교육결손 회복, 유아·직업계고· 취약계층 맞춤 형 지원 과 과밀 학급 해 소 등 교육 여건 개 선을 위한 종합방안을\n",
            "Epoch 29: 100% 40/40 [00:27<00:00,  1.45it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A Normed ED: 0.014749262536873156\n",
            "Epoch 29: 100% 40/40 [00:27<00:00,  1.45it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[APrediction: 뒤에 갓 다리 의 역할을 하는 수십명의 여자 들이 츠의 또 오직 또는 업적을 공유하 다. 이곳에서 유인 이라는 별이다.여객 철도인 보이 는 도란 모습이다.\n",
            "Epoch 29: 100% 40/40 [00:28<00:00,  1.41it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A    Answer: 뒤에 놋 다리 의 역할을 하는 수십명의 여자 들이 모두 허리를 굽혀 앞사람의 허리를 두 손으로 잡고, 머리는 앞사람의 궁둥이 왼편 에 대는데 마치 생선을 꿰어 놓은 듯한 모습이다.\n",
            "Epoch 29: 100% 40/40 [00:28<00:00,  1.41it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A Normed ED: 0.53\n",
            "Epoch 29: 100% 40/40 [00:28<00:00,  1.41it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[APrediction: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져간다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 29: 100% 40/40 [00:29<00:00,  1.37it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A    Answer: 찰에 전달해 김 전 시장에 대한 수사가 시작 됐고, 김 전 시장은 결국 선거 에서 낙 선했다. 그 자리는 고 노무 현 전 대 통령의 오랜 친 구로 알려 진 송철 호(더 불어민 주당) 현 울산 시장이 가져갔다. 하지만 송 부시장은 청와대 윗선으 로 수사가 올라가기 위한 ‘관문’에 가깝다는 게 법조계의 지적 이다. 검사 출신 한 변\n",
            "Epoch 29: 100% 40/40 [00:29<00:00,  1.37it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A Normed ED: 0.005434782608695652\n",
            "Epoch 29: 100% 40/40 [00:29<00:00,  1.37it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[APrediction: 권층을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭탄 은 상에 가까워졌다는 내용이 나오면 하이전베르크를 사실하라 는 명령을 내렸다.1945년 1월 하이 전베르크는 나머지 직원 대부분과 함께 카이저 링셀를 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 과정에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 29: 100% 40/40 [00:31<00:00,  1.29it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A    Answer: 권총을 갖고 강의에 참 석하도록 했으며 강의에서 독일이 원자폭탄 완성에 가까워졌다는 내용이 나오면 하이젠베르크를 사살하라 는 명령을 내렸다.1945년 1월 하이 젠베르크는 나머지 직원 대부분과 함께 카이저 빌헬름 물리학 연구소에 서 슈바르트발트 내의 시설로 이 사했다.2차 세계대전 이후.1945: 알소스 임무.알소스 임무는 독일이 원자 폭탄 프로그램을 가지고 있는지 확인하 고, 미국의 이익을 위해 독일의 원자 관련 시설, 연구, 물자, 과학 인력을 이용하 려는 연합군의 노력이었다. 이 작전에 투입 된 병력은 일반적으로 연합 군의 통제 하에 있던 지역 으 로 이동 했지 만, 때로는 여전 히 독일군 의 통제 하 에 있는 지 역에서 작전을 수 행하기도 했다. 베 를린은 많은 독일 과 학 연\n",
            "Epoch 29: 100% 40/40 [00:31<00:00,  1.29it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n",
            "                                                                                                             \n",
            "\u001b[A Normed ED: 0.030927835051546393\n",
            "Epoch 29: 100% 40/40 [00:31<00:00,  1.29it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Validation DataLoader 0:   0% 0/4 [00:05<?, ?it/s]\u001b[A\n",
            "Epoch 29: 100% 40/40 [00:31<00:00,  1.29it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n",
            "Epoch 29: 100% 40/40 [00:31<00:00,  1.29it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]`Trainer.fit` stopped: `max_epochs=30` reached.\n",
            "Epoch 29: 100% 40/40 [00:31<00:00,  1.29it/s, loss=0.00125, exp_name=train_cord, exp_version=20230821_123453]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cuda 사용하도록 변경\n",
        "import torch\n",
        "from donut import DonutModel\n",
        "from PIL import Image\n",
        "\n",
        "print ( \"CUDA 사용 가능:\" , torch.cuda.is_available())\n",
        "!nvcc --version\n",
        "\n",
        "# 관련 라이브러리, 파일 설치\n",
        "!git clone https://github.com/clovaai/donut.git\n",
        "!cd donut && pip install .\n",
        "\n",
        "## 모델 학습\n",
        "model = DonutModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
        "# 만든 데이터 경로 train_cord.yaml에서 변경\n",
        "# (예시) dataset_name_or_paths: [/content/donut/synthdog/outputs/KoreanData]\n",
        "!python /content/donut/train.py --config /content/donut/config/train_cord.yaml --pretrained_model_name_or_path  \"naver-clova-ix/donut-base-finetuned-cord-v2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO0C5PH5StHm",
        "outputId": "7e259dc2-9113-4d4d-bdd0-b76960f1720f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA 사용 가능: False\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "fatal: destination path 'donut' already exists and is not an empty directory.\n",
            "Processing /content/donut/synthdog/donut\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.11.3 in /usr/local/lib/python3.10/dist-packages (from donut-python==1.0.9) (4.25.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from donut-python==1.0.9) (0.5.4)\n",
            "Requirement already satisfied: datasets[vision] in /usr/local/lib/python3.10/dist-packages (from donut-python==1.0.9) (2.14.4)\n",
            "Requirement already satisfied: pytorch-lightning>=1.6.4 in /usr/local/lib/python3.10/dist-packages (from donut-python==1.0.9) (1.6.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from donut-python==1.0.9) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from donut-python==1.0.9) (0.1.99)\n",
            "Requirement already satisfied: zss in /usr/local/lib/python3.10/dist-packages (from donut-python==1.0.9) (1.2.0)\n",
            "Requirement already satisfied: sconf>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from donut-python==1.0.9) (0.2.5)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python==1.0.9) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python==1.0.9) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python==1.0.9) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python==1.0.9) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python==1.0.9) (2023.6.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python==1.0.9) (2.12.3)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python==1.0.9) (1.0.3)\n",
            "Requirement already satisfied: pyDeprecate>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python==1.0.9) (0.3.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python==1.0.9) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python==1.0.9) (4.7.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning>=1.6.4->donut-python==1.0.9) (3.20.1)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.10/dist-packages (from sconf>=0.2.3->donut-python==1.0.9) (0.17.32)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from sconf>=0.2.3->donut-python==1.0.9) (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.11.3->donut-python==1.0.9) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.11.3->donut-python==1.0.9) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.11.3->donut-python==1.0.9) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.11.3->donut-python==1.0.9) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.11.3->donut-python==1.0.9) (0.13.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->donut-python==1.0.9) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->donut-python==1.0.9) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->donut-python==1.0.9) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->donut-python==1.0.9) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->donut-python==1.0.9) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->donut-python==1.0.9) (3.8.5)\n",
            "Requirement already satisfied: Pillow>=6.2.1 in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->donut-python==1.0.9) (9.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->donut-python==1.0.9) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->donut-python==1.0.9) (1.3.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->donut-python==1.0.9) (0.15.2+cu118)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from zss->donut-python==1.0.9) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python==1.0.9) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python==1.0.9) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python==1.0.9) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python==1.0.9) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python==1.0.9) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python==1.0.9) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->donut-python==1.0.9) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.11.3->donut-python==1.0.9) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.11.3->donut-python==1.0.9) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.11.3->donut-python==1.0.9) (2023.7.22)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (3.4.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (0.41.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python==1.0.9) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python==1.0.9) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python==1.0.9) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python==1.0.9) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python==1.0.9) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python==1.0.9) (16.0.6)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.4.1->pytorch-lightning>=1.6.4->donut-python==1.0.9) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets[vision]->donut-python==1.0.9) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets[vision]->donut-python==1.0.9) (2023.3)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml->sconf>=0.2.3->donut-python==1.0.9) (0.2.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.*->pytorch-lightning>=1.6.4->donut-python==1.0.9) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning>=1.6.4->donut-python==1.0.9) (3.2.2)\n",
            "Building wheels for collected packages: donut-python\n",
            "  Building wheel for donut-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for donut-python: filename=donut_python-1.0.9-py3-none-any.whl size=21775 sha256=3f328b7e0fd30919dd6aa04096dd5d120e908760fba8c87d881a4ef7d4832d3d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p9gc_2zt/wheels/45/80/88/21300fa0c8cc959ee524cc43f3295b11c9fe84782bcfa858f1\n",
            "Successfully built donut-python\n",
            "Installing collected packages: donut-python\n",
            "  Attempting uninstall: donut-python\n",
            "    Found existing installation: donut-python 1.0.9\n",
            "    Uninstalling donut-python-1.0.9:\n",
            "      Successfully uninstalled donut-python-1.0.9\n",
            "Successfully installed donut-python-1.0.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at naver-clova-ix/donut-base-finetuned-cord-v2 were not used when initializing DonutModel: ['encoder.model.norm.weight', 'encoder.model.norm.bias']\n",
            "- This IS expected if you are initializing DonutModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DonutModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-21 13:48:47.681303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "resume_from_checkpoint_path: None\n",
            "result_path: ./result\n",
            "\u001b[36mpretrained_model_name_or_path: naver-clova-ix/donut-base-finetuned-cord-v2\n",
            "\u001b[0mdataset_name_or_paths: \n",
            "  - /content/donut/dataset\n",
            "sort_json_key: False\n",
            "train_batch_sizes: \n",
            "  - 1\n",
            "val_batch_sizes: \n",
            "  - 1\n",
            "input_size: \n",
            "  - 1280\n",
            "  - 960\n",
            "max_length: 768\n",
            "align_long_axis: False\n",
            "num_nodes: 1\n",
            "seed: 2022\n",
            "lr: 3e-05\n",
            "warmup_steps: 300\n",
            "num_training_samples_per_epoch: 800\n",
            "max_epochs: 30\n",
            "max_steps: -1\n",
            "num_workers: 8\n",
            "val_check_interval: 1.0\n",
            "check_val_every_n_epoch: 3\n",
            "gradient_clip_val: 1.0\n",
            "verbose: True\n",
            "exp_name: train_cord\n",
            "exp_version: 20230821_134850\n",
            "Config is saved at result/train_cord/20230821_134850/config.yaml\n",
            "Global seed set to 2022\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Some weights of the model checkpoint at naver-clova-ix/donut-base-finetuned-cord-v2 were not used when initializing DonutModel: ['encoder.model.norm.weight', 'encoder.model.norm.bias']\n",
            "- This IS expected if you are initializing DonutModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DonutModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Resolving data files: 100% 804/804 [00:00<00:00, 164554.75it/s]\n",
            "Downloading data files: 100% 800/800 [00:00<00:00, 40272.25it/s]\n",
            "Downloading data files: 0it [00:00, ?it/s]\n",
            "Extracting data files: 0it [00:00, ?it/s]\n",
            "Generating train split: 800 examples [00:00, 9601.74 examples/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/donut/train.py\", line 176, in <module>\n",
            "    train(config)\n",
            "  File \"/content/donut/train.py\", line 104, in train\n",
            "    DonutDataset(\n",
            "  File \"/content/donut/donut/util.py\", line 69, in __init__\n",
            "    ground_truth = json.loads(sample[\"ground_truth\"])\n",
            "KeyError: 'ground_truth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"/content/Dataset\", split=\"train\")\n",
        "for sample in dataset:\n",
        "    print(\"osh\",ground_truth)\n",
        "    print(\"gt_parses\" in ground_truth)\n",
        "    if \"gt_parses\" in ground_truth:  # when multiple ground truths are available, e.g., docvqa\n",
        "        assert isinstance(ground_truth[\"gt_parses\"], list)\n",
        "        gt_jsons = ground_truth[\"gt_parses\"]\n",
        "    else:\n",
        "        print(ground_truth[\"gt_parse\"])\n",
        "        print(type(ground_truth[\"gt_parse\"]))\n",
        "        assert \"gt_parse\" in ground_truth and isinstance(ground_truth[\"gt_parse\"], dict)\n",
        "        gt_jsons = [ground_truth[\"gt_parse\"]]\n"
      ],
      "metadata": {
        "id": "oq8hFNfzDIzE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}