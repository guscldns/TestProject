{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1d0Nweaaix1WReV1otMy6A9jG6lENNiR0",
      "authorship_tag": "ABX9TyN/WNO6pWcaKGb1mIcps8Bn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guscldns/TestProject/blob/main/%EC%9E%A5%EA%B8%B0/%20eenzeenee_T5_finetuning_for_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "IdsH4v2siRpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g5XLql5klvM",
        "outputId": "a347d4e5-5eba-44b9-ea1b-7f34b593fdfd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ualyvwgchgth"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/eenzeenee/T5_finetuning_for_summary.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0RsfKxVh3IT",
        "outputId": "460d9f66-fef3-43b7-dc6f-04d4dbc3042e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'T5_finetuning_for_summary'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 42 (delta 20), reused 31 (delta 12), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (42/42), 17.75 KiB | 17.75 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/sum3_train.csv')\n",
        "val = pd.read_csv('/content/drive/MyDrive/sum3_test.csv')"
      ],
      "metadata": {
        "id": "P8xo0sS0h7hr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 프레임을 불러옵니다 (이전에 생성한 데이터프레임을 불러올 수 있도록 데이터 프레임 생성 부분을 추가해야 합니다)\n",
        "# 예: df = pd.read_csv('your_data.csv')\n",
        "\n",
        "# 데이터를 나눌 비율을 설정합니다\n",
        "train_ratio = 0.8  # 80%를 train.json으로 사용\n",
        "test_ratio = 1 - train_ratio  # 나머지를 test.json으로 사용\n",
        "\n",
        "# 데이터를 나눌 인덱스 계산\n",
        "num_samples = len(train)\n",
        "num_train_samples = int(train_ratio * num_samples)\n",
        "\n",
        "# 데이터를 무작위로 섞는 것이 좋을 수 있습니다\n",
        "train = train.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# 데이터를 나눕니다\n",
        "train_data = train[:num_train_samples]\n",
        "test_data = train[num_train_samples:]"
      ],
      "metadata": {
        "id": "R7731enTllZZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 저장할 리스트\n",
        "data = []\n",
        "\n",
        "# 데이터 프레임의 각 행을 반복하여 데이터 추출\n",
        "for index, row in test_data.iterrows():\n",
        "    item = {}\n",
        "    item['source'] = str(row['passage'])\n",
        "    item['target'] = str(row['summary3'])\n",
        "    data.append(item)\n",
        "\n",
        "# JSON 파일로 데이터 저장\n",
        "with open('test.json', 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(data, json_file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "QEegRgJQhyxK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/val.json', 'r', encoding='utf-8') as json_file:\n",
        "    test = json.load(json_file)\n",
        "len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuRP3HVxl7zk",
        "outputId": "3f12e030-0449-44cd-d0aa-a96ebf637eb4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9150"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/T5_finetuning_for_summary/train.py --max_target_length 512 --base_path /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSz22rtfj8Xv",
        "outputId": "c2de669a-a76f-43f4-f5bb-cacb449e95e9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2023-10-12 11:14:54.693584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 24624.09it/s]\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1182.60it/s]\n",
            "Generating train split: 58744 examples [00:02, 28307.86 examples/s]\n",
            "Generating validation split: 9150 examples [00:00, 32546.33 examples/s]\n",
            "Generating test split: 14687 examples [00:00, 29845.81 examples/s]\n",
            "- Training set:\t58744 (71.14%)\n",
            "- Val set:\t9150 (11.08%)\n",
            "- Test set:\t14687 (17.78%)\n",
            "Downloading (…)lve/main/config.json: 100% 728/728 [00:00<00:00, 1.83MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.10G/1.10G [00:13<00:00, 80.1MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 209/209 [00:00<00:00, 1.08MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 2.90M/2.90M [00:00<00:00, 52.3MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 67.0/67.0 [00:00<00:00, 409kB/s]\n",
            "Filter: 100% 58744/58744 [00:00<00:00, 100966.59 examples/s]\n",
            "Filter: 100% 9150/9150 [00:00<00:00, 109769.33 examples/s]\n",
            "Filter: 100% 14687/14687 [00:00<00:00, 102962.84 examples/s]\n",
            "Map:   0% 0/58744 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3864: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map:  27% 16000/58744 [00:30<01:20, 532.79 examples/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/T5_finetuning_for_summary/train.py\", line 66, in <module>\n",
            "    tokenized_datasets = dataset_cleaned.map(lambda x: preprocess_data(x, tokenizer, args), batched=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py\", line 853, in map\n",
            "    {\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py\", line 854, in <dictcomp>\n",
            "    k: dataset.map(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 592, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 557, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3097, in map\n",
            "    for rank, done, content in Dataset._map_single(**dataset_kwargs):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 3493, in _map_single\n",
            "    writer.write_batch(batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py\", line 555, in write_batch\n",
            "    arrays.append(pa.array(typed_sequence))\n",
            "  File \"pyarrow/array.pxi\", line 231, in pyarrow.lib.array\n",
            "  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py\", line 189, in __arrow_array__\n",
            "    out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}