{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QuAuZ7U6NodZxITmAFiDOSNSMqSjq5O6",
      "authorship_tag": "ABX9TyPrxaqPzZFhXls6+pIujwWw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guscldns/TestProject/blob/main/%EC%9E%A5%EA%B8%B0/%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert\n",
        "!pip install kiwipiepy\n",
        "!pip install google-cloud-vision\n",
        "!pip install PyMuPDF\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "2mNBPA8-1jaU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab9672ce-eedc-419e-a5d6-c01b2e2a8a41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keybert\n",
            "  Downloading keybert-0.8.3.tar.gz (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentence-transformers>=0.3.8 (from keybert)\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.23.5)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.6.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.16.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.2.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.15.2+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (23.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (17.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.1.7)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
            "Building wheels for collected packages: keybert, sentence-transformers\n",
            "  Building wheel for keybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keybert: filename=keybert-0.8.3-py3-none-any.whl size=39124 sha256=6cce3c0259d31d9dd24d62e33964aa675d12d32fad73afc92a23b65fc4e798f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/88/07/1a3bc11fd1dd5f89924a02dcbca89a3015e25e8faa31f904dc\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=d17dc499acd9486809e302ff5c7569071c7fd0a20fa7feeebbbb509697d42f99\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built keybert sentence-transformers\n",
            "Installing collected packages: sentencepiece, safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers, keybert\n",
            "Successfully installed huggingface-hub-0.17.3 keybert-0.8.3 safetensors-0.4.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.34.0\n",
            "Collecting kiwipiepy\n",
            "  Downloading kiwipiepy-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwipiepy-model~=0.16 (from kiwipiepy)\n",
            "  Downloading kiwipiepy_model-0.16.0.tar.gz (30.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from kiwipiepy) (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kiwipiepy) (4.66.1)\n",
            "Building wheels for collected packages: kiwipiepy-model\n",
            "  Building wheel for kiwipiepy-model (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kiwipiepy-model: filename=kiwipiepy_model-0.16.0-py3-none-any.whl size=30813255 sha256=5cdd654d92c7598688159cfc01a7e316210cedfe8b4b112ad9a10a559062d45e\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/03/2b/ac2c97cc65ebd9df3516f4b900adc2f0a744df8d1375b2e2ef\n",
            "Successfully built kiwipiepy-model\n",
            "Installing collected packages: kiwipiepy-model, kiwipiepy\n",
            "Successfully installed kiwipiepy-0.16.0 kiwipiepy-model-0.16.0\n",
            "Collecting google-cloud-vision\n",
            "  Downloading google_cloud_vision-3.4.4-py2.py3-none-any.whl (444 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (1.59.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (2023.7.22)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-vision) (0.5.0)\n",
            "Installing collected packages: google-cloud-vision\n",
            "Successfully installed google-cloud-vision-3.4.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.23.4-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.3 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.23.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.23.4 PyMuPDFb-1.23.3\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dCA0Yb81Zjj",
        "outputId": "9a8179cc-6c7a-4ba0-dcdc-daf3da543251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import fitz\n",
        "import zipfile\n",
        "import cv2\n",
        "import io\n",
        "import os\n",
        "import torch\n",
        "from keybert import KeyBERT\n",
        "from kiwipiepy import Kiwi\n",
        "from transformers import BertModel\n",
        "from PIL import Image\n",
        "from google.cloud import vision\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers.models.bart import BartForConditionalGeneration\n",
        "\n",
        "import torch\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers.models.bart import BartForConditionalGeneration, BartForCausalLM\n",
        "# 모델과 토크나이저 로드\n",
        "# model = BartForConditionalGeneration.from_pretrained('/content/kobart_summary')\n",
        "summary_model = BartForConditionalGeneration.from_pretrained('/content/drive/MyDrive/KoBART-summarization/kobart_summary')\n",
        "summary_tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
        "\n",
        "\n",
        "### 키워드 명사로만 제공\n",
        "from kiwipiepy import Kiwi\n",
        "from transformers import BertModel\n",
        "from keybert import KeyBERT\n",
        "\n",
        "# 텍스트 전문 : full_text\n",
        "# Kiwi 형태소 분석기 초기화\n",
        "kiwi = Kiwi()\n",
        "\n",
        "# BERT 모델 로드\n",
        "keywords_model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "# KeyBERT 모델 초기화\n",
        "kw_model = KeyBERT(keywords_model)\n",
        "\n",
        "# 파일 저장할 폴더 생성\n",
        "# os.mkdir(\"/content/upload_file\")\n",
        "# os.mkdir(\"/content/image_file\")\n",
        "\n",
        "# PDF 라면 이미지로 변경\n",
        "def pdf_to_png(files):\n",
        "    path = f\"/content/upload_file/{files}\"\n",
        "    doc = fitz.open(path)\n",
        "    for i, page in enumerate(doc):\n",
        "        img = page.get_pixmap()\n",
        "        img.save(f\"/content/image_file/{i}.png\")\n",
        "\n",
        "# OCR 함수\n",
        "def detect_paragraphs(image_path):\n",
        "    from google.cloud import vision\n",
        "    # API키 가져오기\n",
        "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"/content/drive/MyDrive/api-project-397750607032-5ddc025931cd.json\"\n",
        "\n",
        "    # API 가져오기\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "\n",
        "    # 주석을 추가할 이미지 파일 이름\n",
        "    file_name = os.path.abspath(image_path)\n",
        "\n",
        "    # 이미지 로드\n",
        "    with io.open(file_name, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    # 이미지 OCR\n",
        "    response = client.document_text_detection(image=image)\n",
        "\n",
        "    # 이미지 OCR 텍스트 전문\n",
        "    full_text = response.full_text_annotation.text\n",
        "\n",
        "    # 이미지 OCR 후 결과 (bbox, word 등)\n",
        "    pages = response.full_text_annotation.pages\n",
        "\n",
        "    # 텍스트 주석\n",
        "    # 참고 : https://cloud.google.com/dotnet/docs/reference/Google.Cloud.Vision.V1/latest/Google.Cloud.Vision.V1.TextAnnotation.Types.DetectedBreak.Types.BreakType\n",
        "    # 참고 : https://googleapis.github.io/googleapis/java/grpc-google-cloud-vision-v1/0.1.5/apidocs/com/google/cloud/vision/v1/TextAnnotation.DetectedBreak.BreakType.html\n",
        "    breaks = vision.TextAnnotation.DetectedBreak.BreakType\n",
        "\n",
        "    paragraphs = []\n",
        "    lines = []\n",
        "\n",
        "    for page in pages:\n",
        "        for block in page.blocks:\n",
        "            for paragraph in block.paragraphs:\n",
        "                para = \"★문단시작★\"\n",
        "                line = \"\"\n",
        "                for word in paragraph.words:\n",
        "                    for symbol in word.symbols:\n",
        "                        line += symbol.text\n",
        "                        # breaks.SPACE : 공백\n",
        "                        if symbol.property.detected_break.type == breaks.SPACE:\n",
        "                            line += ' '\n",
        "                        # breaks.EOL_SURE_SPACE : 줄 바꿈\n",
        "                        if symbol.property.detected_break.type == breaks.EOL_SURE_SPACE:\n",
        "                            line += ' '\n",
        "                            lines.append(line)\n",
        "                            para += line\n",
        "                            line = ''\n",
        "                        # breaks.LINE_BREAK : 단락을 끝내는 줄바꿈\n",
        "                        if symbol.property.detected_break.type == breaks.LINE_BREAK:\n",
        "                            lines.append(line)\n",
        "                            para += line\n",
        "                            line = ''\n",
        "                paragraphs.append(para)\n",
        "\n",
        "    return full_text, paragraphs\n",
        "\n",
        "def summarize_paragraphs(paragraphs):\n",
        "\n",
        "    # 문단 리스트를 512 토큰으로 나누기\n",
        "    # 입력문장 리스트 : paragraphs\n",
        "\n",
        "    # 모델 입력 데이터 생성\n",
        "    input_data = []\n",
        "\n",
        "    # 요약 데이터 생성\n",
        "    summary =[]\n",
        "\n",
        "    # 현재까지의 토큰 수 초기화\n",
        "    current_token_count = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        # '★문단시작★'를 제거하고 문단을 토큰화\n",
        "        paragraph_tokens = summary_tokenizer(paragraph.replace('★문단시작★', ''), return_tensors='pt', add_special_tokens=True).input_ids\n",
        "\n",
        "        # 현재 문단을 추가해도 512 토큰을 넘지 않으면 추가\n",
        "        if current_token_count + len(paragraph_tokens[0]) <= 512:\n",
        "            if not input_data:\n",
        "                input_data.append(paragraph_tokens)\n",
        "                current_token_count += len(paragraph_tokens[0])\n",
        "            else:\n",
        "                # 이미 문단이 추가되어 있는 경우 이어서 추가\n",
        "                input_data[-1] = torch.cat((input_data[-1], paragraph_tokens), dim=-1)\n",
        "                current_token_count += len(paragraph_tokens[0])\n",
        "        else:\n",
        "            # 256 토큰을 넘어가면 새로운 입력으로 시작\n",
        "            input_data.append(paragraph_tokens)\n",
        "            current_token_count = len(paragraph_tokens[0])\n",
        "\n",
        "    for inp in input_data:\n",
        "        output = summary_model.generate(inp, eos_token_id=1, max_length=512, num_beams=4)\n",
        "        t_output = summary_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        summary.append(t_output)\n",
        "\n",
        "    return summary\n",
        "\n",
        "def fulltext_keywords(full_text):\n",
        "\n",
        "    kiwi.analyze(full_text)\n",
        "\n",
        "    # 키워드 추출\n",
        "    keywords = kw_model.extract_keywords(full_text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=10)\n",
        "\n",
        "    # 명사 키워드 추출 함수 정의\n",
        "    def extract_nouns(keyword_list):\n",
        "        noun_keywords = []\n",
        "        keywordscore = []\n",
        "        for keyword, score in keywords:\n",
        "            # 형태소 분석 수행\n",
        "            tokens = kiwi.analyze(keyword)\n",
        "            # print(tokens)\n",
        "            for tk in tokens[0][0]:\n",
        "                # print(tk)\n",
        "                if tk.tag == \"NNG\":\n",
        "                    if tk.form not in noun_keywords:\n",
        "                        noun_keywords.append(tk.form)\n",
        "                        keywordscore.append((tk.form,score))\n",
        "\n",
        "        return noun_keywords, keywordscore\n",
        "    noun_keywords, keywordscore = extract_nouns(keywords)\n",
        "    return noun_keywords, keywordscore\n",
        "\n",
        "def keyword_sum(noun_keyword, n):\n",
        "    from collections import Counter\n",
        "    # 키워드 리스트 하나로 합치기\n",
        "    word_list = sum(noun_keyword,[])\n",
        "    # 단어 빈도수 계산\n",
        "    word_counts = Counter(word_list)\n",
        "\n",
        "    # 가장 많이 중복된 단어 5개 선택\n",
        "    top_n_words = [word for word, count in word_counts.most_common(n)]\n",
        "    return top_n_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코렙이라서 사용, flask에서는 다른 방법 필요\n",
        "%cd /content/upload_file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_names = os.listdir('/content/upload_file')\n",
        "if '.ipynb_checkpoints' in file_names:\n",
        "    file_names.remove('.ipynb_checkpoints')\n",
        "\n",
        "for files in file_names:\n",
        "    if '.pdf' in files[-4:]:\n",
        "\t    pdf_to_png(files)\n",
        "\n",
        "summarys, keywordscores, noun_keyword = [], [], []\n",
        "full_text = \"\"\n",
        "image_pathes = os.listdir('/content/image_file')\n",
        " # PDF 10장/ 2분 7초\n",
        "for image_paths in image_pathes:\n",
        "    image_path = f'/content/image_file/{image_paths}'\n",
        "    text_full, paragraphs = detect_paragraphs(image_path)\n",
        "    full_text = text_full\n",
        "    summarys.append(summarize_paragraphs(paragraphs))\n",
        "    noun_keywords, keywordscore = fulltext_keywords(full_text)\n",
        "    keywordscores.append(keywordscore)\n",
        "    noun_keyword.append(noun_keywords)\n",
        "key_5 =  keyword_sum(noun_keyword, 5)\n",
        "\n",
        "print(summarys, full_text, key_5)"
      ],
      "metadata": {
        "id": "T_JCGjTQ-8II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MxMTwBGTvcrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import fitz\n",
        "import io\n",
        "import os\n",
        "from google.cloud import vision\n",
        "from PIL import Image\n",
        "from kiwipiepy import Kiwi\n",
        "from transformers import BertModel\n",
        "from keybert import KeyBERT\n",
        "import torch\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers.models.bart import BartForConditionalGeneration\n",
        "\n",
        "# Kiwi, BERT 모델 및 KeyBERT 모델 초기화\n",
        "kiwi = Kiwi()\n",
        "keywords_model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "kw_model = KeyBERT(keywords_model)\n",
        "\n",
        "# 요약 모델 및 토크나이저 초기화\n",
        "summary_model = BartForConditionalGeneration.from_pretrained('/content/drive/MyDrive/KoBART-summarization/kobart_summary')\n",
        "summary_tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
        "\n",
        "# OCR 함수\n",
        "def detect_paragraphs(image_path):\n",
        "    from google.cloud import vision\n",
        "\n",
        "    # API키 가져오기\n",
        "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"/content/drive/MyDrive/api-project-397750607032-5ddc025931cd.json\"\n",
        "\n",
        "    # API 가져오기\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "\n",
        "    # 주석을 추가할 이미지 파일 이름\n",
        "    file_name = os.path.abspath(image_path)\n",
        "\n",
        "    # 이미지 로드\n",
        "    with io.open(file_name, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    # 이미지 OCR\n",
        "    response = client.document_text_detection(image=image)\n",
        "\n",
        "    # 이미지 OCR 텍스트 전문\n",
        "    full_text = response.full_text_annotation.text\n",
        "\n",
        "    # 이미지 OCR 후 결과 (bbox, word 등)\n",
        "    pages = response.full_text_annotation.pages\n",
        "\n",
        "    # 텍스트 주석\n",
        "    # 참고 : https://cloud.google.com/dotnet/docs/reference/Google.Cloud.Vision.V1/latest/Google.Cloud.Vision.V1.TextAnnotation.Types.DetectedBreak.Types.BreakType\n",
        "    # 참고 : https://googleapis.github.io/googleapis/java/grpc-google-cloud-vision-v1/0.1.5/apidocs/com/google/cloud/vision/v1/TextAnnotation.DetectedBreak.BreakType.html\n",
        "    breaks = vision.TextAnnotation.DetectedBreak.BreakType\n",
        "\n",
        "    paragraphs = []\n",
        "    lines = []\n",
        "\n",
        "    for page in pages:\n",
        "        for block in page.blocks:\n",
        "            for paragraph in block.paragraphs:\n",
        "                para = \"★문단시작★\"\n",
        "                line = \"\"\n",
        "                for word in paragraph.words:\n",
        "                    for symbol in word.symbols:\n",
        "                        line += symbol.text\n",
        "                        # breaks.SPACE : 공백\n",
        "                        if symbol.property.detected_break.type == breaks.SPACE:\n",
        "                            line += ' '\n",
        "                        # breaks.EOL_SURE_SPACE : 줄 바꿈\n",
        "                        if symbol.property.detected_break.type == breaks.EOL_SURE_SPACE:\n",
        "                            line += ' '\n",
        "                            lines.append(line)\n",
        "                            para += line\n",
        "                            line = ''\n",
        "                        # breaks.LINE_BREAK : 단락을 끝내는 줄바꿈\n",
        "                        if symbol.property.detected_break.type == breaks.LINE_BREAK:\n",
        "                            lines.append(line)\n",
        "                            para += line\n",
        "                            line = ''\n",
        "                paragraphs.append(para)\n",
        "\n",
        "    return full_text, paragraphs\n",
        "\n",
        "def summarize_paragraphs(paragraphs):\n",
        "\n",
        "    # 문단 리스트를 512 토큰으로 나누기\n",
        "    # 입력문장 리스트 : paragraphs\n",
        "\n",
        "    # 모델 입력 데이터 생성\n",
        "    input_data = []\n",
        "\n",
        "    # 요약 데이터 생성\n",
        "    summary =[]\n",
        "\n",
        "    # 현재까지의 토큰 수 초기화\n",
        "    current_token_count = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        # '★문단시작★'를 제거하고 문단을 토큰화\n",
        "        paragraph_tokens = summary_tokenizer(paragraph.replace('★문단시작★', ''), return_tensors='pt', add_special_tokens=True).input_ids\n",
        "\n",
        "        # 현재 문단을 추가해도 512 토큰을 넘지 않으면 추가\n",
        "        if current_token_count + len(paragraph_tokens[0]) <= 512:\n",
        "            if not input_data:\n",
        "                input_data.append(paragraph_tokens)\n",
        "                current_token_count += len(paragraph_tokens[0])\n",
        "            else:\n",
        "                # 이미 문단이 추가되어 있는 경우 이어서 추가\n",
        "                input_data[-1] = torch.cat((input_data[-1], paragraph_tokens), dim=-1)\n",
        "                current_token_count += len(paragraph_tokens[0])\n",
        "        else:\n",
        "            # 256 토큰을 넘어가면 새로운 입력으로 시작\n",
        "            input_data.append(paragraph_tokens)\n",
        "            current_token_count = len(paragraph_tokens[0])\n",
        "\n",
        "    for inp in input_data:\n",
        "        output = summary_model.generate(inp, eos_token_id=1, max_length=512, num_beams=4)\n",
        "        t_output = summary_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        summary.append(t_output)\n",
        "\n",
        "    return summary\n",
        "\n",
        "def fulltext_keywords(full_text):\n",
        "    kiwi.analyze(full_text)\n",
        "    keywords = kw_model.extract_keywords(full_text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=10)\n",
        "    # 명사 키워드 추출 함수 정의\n",
        "    def extract_nouns(keyword_list):\n",
        "        noun_keywords = []\n",
        "        keywordscore = []\n",
        "        for keyword, score in keywords:\n",
        "            # 형태소 분석 수행\n",
        "            tokens = kiwi.analyze(keyword)\n",
        "            # print(tokens)\n",
        "            for tk in tokens[0][0]:\n",
        "                # print(tk)\n",
        "                if tk.tag == \"NNG\":\n",
        "                    if tk.form not in noun_keywords:\n",
        "                        noun_keywords.append(tk.form)\n",
        "                        keywordscore.append((tk.form,score))\n",
        "\n",
        "        return noun_keywords, keywordscore\n",
        "\n",
        "    noun_keywords, keywordscore = extract_nouns(keywords)\n",
        "    return noun_keywords, keywordscore\n",
        "\n",
        "\n",
        "def process_images_and_text(image_folder, n):\n",
        "    summarys, keywordscores, noun_keyword = [], [], []\n",
        "    full_text = \"\"\n",
        "    image_paths = os.listdir(image_folder)\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        image_path = os.path.join(image_folder, image_path)\n",
        "        text_full, paragraphs = detect_paragraphs(image_path)\n",
        "        full_text = text_full\n",
        "        summarys.append(summarize_paragraphs(paragraphs))\n",
        "        noun_keywords, keywordscore = fulltext_keywords(full_text)\n",
        "        keywordscores.append(keywordscore)\n",
        "        noun_keyword.append(noun_keywords)\n",
        "\n",
        "        def keyword_sum(noun_keyword, n):\n",
        "            from collections import Counter\n",
        "            # 키워드 리스트 하나로 합치기\n",
        "            word_list = sum(noun_keyword,[])\n",
        "            # 단어 빈도수 계산\n",
        "            word_counts = Counter(word_list)\n",
        "\n",
        "            # 가장 많이 중복된 단어 5개 선택\n",
        "            top_n_words = [word for word, count in word_counts.most_common(n)]\n",
        "            return top_n_words\n",
        "    top_n_words =  keyword_sum(noun_keyword, n)\n",
        "    return summarys, full_text, top_n_words\n",
        "\n",
        "def pdf_to_img(file_folder):\n",
        "    file_paths = os.listdir(file_folder)\n",
        "    for image_path in file_paths:\n",
        "        # PDF면 이미지 파일로 변환\n",
        "        if image_path[-4:] == '.pdf':\n",
        "            doc = fitz.open(image_path)\n",
        "            # PDF to img\n",
        "            for i, page in enumerate(doc):\n",
        "                img = page.get_pixmap()\n",
        "                img.save(f\"./{image_path[2:-4]}_{i+1:03d}.png\")\n",
        "            # PDF 파일 삭제\n",
        "            os.remove(image_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeKGmRz-ujMl",
        "outputId": "19599767-5610-4ae8-d78f-553c26e9f847"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 폴더를 사용하여 process_images_and_text 함수 호출\n",
        "image_folder = '/content/test'\n",
        "summarys, full_text, key_5 = process_images_and_text(image_folder, 5) # 10장에 4분\n",
        "\n",
        "# 추가 처리 또는 필요에 따라 결과를 출력합니다.\n",
        "print(summarys, full_text, key_5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv4viGxvvMfo",
        "outputId": "f7a13398-9281-4faa-fa0a-487ec559b1e7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['이날은 교원인 듯한 양복쟁이를 동광학교에 태워다 주기로 하는 등 오래간만에 김첨지에게 닥친 운수 좋은 날이었다.', '김첨지는 오래간만에 김첨지에게 의지하면 오라질 년이 천방지축으로 냄비는에 손을 대고 끓였고 오래간만에 김첨지는 김첨지를 쉽게 물리쳤다.']] 운수 좋은날\n",
            "현진건\n",
            "새침하게 흐린 품이 눈이 올 듯하더니 눈은 아니 오고 얼다가 만 비가 추\n",
            "적추적 내리는 날이었다.\n",
            "이날이야말로 동소문 안에서 인력거꾼 노릇을 하는 김첨지에게는 오래간만\n",
            "에도 닥친 운수 좋은 날이었다. 문안에 거기도 문밖은 아니지만 들어간답\n",
            "시는 앞집 마마님을 전찻길까지 모셔다 드린 것을 비롯으로 행여나 손님이\n",
            "있을까 하고 정류장에서 어정어정하며 내리는 사람 하나하나에게 거의 비는\n",
            "듯한 눈결을 보내고 있다가 마침내 교원인 듯한 양복쟁이를 동광학교(東光\n",
            "學校)까지 태워다 주기로 되었다.\n",
            "첫 번에 삼십전, 둘째 번에 오십전 - 아침 댓바람에 그리 흉치 않은 일이\n",
            "었다. 그야말로 재수가 옴불어서 근 열흘 동안 돈 구경도 못한 김첨지는 십\n",
            "전짜리 백동화 서 푼, 또는 다섯 푼이 찰깍 하고 손바닥에 떨어질 제 거의\n",
            "눈물을 흘릴 만큼 기뻤었다. 더구나 이날 이때에 이 팔십 전이라는 돈이 그\n",
            "에게 얼마나 유용한지 몰랐다. 컬컬한 목에 모주 한 잔도 적실 수 있거니와\n",
            "그보다도 앓는 아내에게 설렁탕 한 그릇도 사다 줄 수 있음이다.\n",
            "그의 아내가 기침으로 쿨룩거리기는 벌써 달포가 넘었다. 조밥도 굶기를\n",
            "먹다시피 하는 형편이니 물론 약 한 첩 써본 일이 없다. 구태여 쓰려면 못\n",
            "쓸 바도 아니로되 그는 병이란 놈에게 약을 주어 보내면 재미를 붙여서 자\n",
            "꾸 온다는 자기의 신조(信)에 어디까지 충실하였다. 따라서 의사에게 보\n",
            "인 적이 없으니 무슨 병인지는 알 수 없으되 반듯이 누워 가지고 일어나기\n",
            "는 새로 모로도 못 눕는 걸 보면 중증은 중증인 듯. 병이 이대도록 심해지\n",
            "기는 열흘전에 조밥을 먹고 체한 때문이다. 그때도 김첨지가 오래간만에 돈\n",
            "을 얻어서 좁쌀 한 뇌와 십 전짜리 나무 한 단을 사다 주었더니 김첨지의\n",
            "말에 의지하면 그 오라질 년이 천방지축으로 냄비에 대고 끓였다. 마음은\n",
            "급하고 불길은 달지 않아 채 익지도 않은 것을 그 오라질년이 숟가락은 고\n",
            "만두고 손으로 움켜서 두 뺨에 주먹덩이 같은 혹이 불거지도록 누가 빼앗을\n",
            "듯이 처박질하더니만 그날 저녁부터 가슴이 땡긴다. 배가 켕긴다고 눈을 흡\n",
            "뜨고 지랄병을 하였다. 그때 김첨지는 열화와 같이 성을 내며,\n",
            "“에이, 오라질년, 조랑복은 할 수가 없어, 못 먹어 병, 먹어서 병! 어쩌\n",
            "란 말이야! 왜 눈을 바루 뜨지 못해!\"\n",
            "하고 앓는 이의 뺨을 한 번 후려갈겼다. 흡뜬 눈은 조금 바루어졌건만 이슬 ['날', '운수', '아침', '그때', '구경']\n"
          ]
        }
      ]
    }
  ]
}