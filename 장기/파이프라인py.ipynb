{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QuAuZ7U6NodZxITmAFiDOSNSMqSjq5O6",
      "authorship_tag": "ABX9TyME1G/+idiSbNpAvKFvE8VF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guscldns/TestProject/blob/main/%EC%9E%A5%EA%B8%B0/%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert\n",
        "!pip install kiwipiepy\n",
        "!pip install google-cloud-vision\n",
        "!pip install PyMuPDF\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "2mNBPA8-1jaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dCA0Yb81Zjj",
        "outputId": "9a8179cc-6c7a-4ba0-dcdc-daf3da543251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import fitz\n",
        "import zipfile\n",
        "import cv2\n",
        "import io\n",
        "import os\n",
        "import torch\n",
        "from keybert import KeyBERT\n",
        "from kiwipiepy import Kiwi\n",
        "from transformers import BertModel\n",
        "from PIL import Image\n",
        "from google.cloud import vision\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers.models.bart import BartForConditionalGeneration\n",
        "\n",
        "import torch\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers.models.bart import BartForConditionalGeneration, BartForCausalLM\n",
        "# 모델과 토크나이저 로드\n",
        "# model = BartForConditionalGeneration.from_pretrained('/content/kobart_summary')\n",
        "summary_model = BartForConditionalGeneration.from_pretrained('/content/drive/MyDrive/KoBART-summarization/kobart_summary')\n",
        "summary_tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
        "\n",
        "\n",
        "### 키워드 명사로만 제공\n",
        "from kiwipiepy import Kiwi\n",
        "from transformers import BertModel\n",
        "from keybert import KeyBERT\n",
        "\n",
        "# 텍스트 전문 : full_text\n",
        "# Kiwi 형태소 분석기 초기화\n",
        "kiwi = Kiwi()\n",
        "\n",
        "# BERT 모델 로드\n",
        "keywords_model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "# KeyBERT 모델 초기화\n",
        "kw_model = KeyBERT(keywords_model)\n",
        "\n",
        "# 파일 저장할 폴더 생성\n",
        "# os.mkdir(\"/content/upload_file\")\n",
        "# os.mkdir(\"/content/image_file\")\n",
        "\n",
        "# PDF 라면 이미지로 변경\n",
        "def pdf_to_png(files):\n",
        "    path = f\"/content/upload_file/{files}\"\n",
        "    doc = fitz.open(path)\n",
        "    for i, page in enumerate(doc):\n",
        "        img = page.get_pixmap()\n",
        "        img.save(f\"/content/image_file/{i}.png\")\n",
        "\n",
        "# OCR 함수\n",
        "def detect_paragraphs(image_path):\n",
        "    from google.cloud import vision\n",
        "    # API키 가져오기\n",
        "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"/content/drive/MyDrive/api-project-397750607032-5ddc025931cd.json\"\n",
        "\n",
        "    # API 가져오기\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "\n",
        "    # 주석을 추가할 이미지 파일 이름\n",
        "    file_name = os.path.abspath(image_path)\n",
        "\n",
        "    # 이미지 로드\n",
        "    with io.open(file_name, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    # 이미지 OCR\n",
        "    response = client.document_text_detection(image=image)\n",
        "\n",
        "    # 이미지 OCR 텍스트 전문\n",
        "    full_text = response.full_text_annotation.text\n",
        "\n",
        "    # 이미지 OCR 후 결과 (bbox, word 등)\n",
        "    pages = response.full_text_annotation.pages\n",
        "\n",
        "    # 텍스트 주석\n",
        "    # 참고 : https://cloud.google.com/dotnet/docs/reference/Google.Cloud.Vision.V1/latest/Google.Cloud.Vision.V1.TextAnnotation.Types.DetectedBreak.Types.BreakType\n",
        "    # 참고 : https://googleapis.github.io/googleapis/java/grpc-google-cloud-vision-v1/0.1.5/apidocs/com/google/cloud/vision/v1/TextAnnotation.DetectedBreak.BreakType.html\n",
        "    breaks = vision.TextAnnotation.DetectedBreak.BreakType\n",
        "\n",
        "    paragraphs = []\n",
        "    lines = []\n",
        "\n",
        "    for page in pages:\n",
        "        for block in page.blocks:\n",
        "            for paragraph in block.paragraphs:\n",
        "                para = \"★문단시작★\"\n",
        "                line = \"\"\n",
        "                for word in paragraph.words:\n",
        "                    for symbol in word.symbols:\n",
        "                        line += symbol.text\n",
        "                        # breaks.SPACE : 공백\n",
        "                        if symbol.property.detected_break.type == breaks.SPACE:\n",
        "                            line += ' '\n",
        "                        # breaks.EOL_SURE_SPACE : 줄 바꿈\n",
        "                        if symbol.property.detected_break.type == breaks.EOL_SURE_SPACE:\n",
        "                            line += ' '\n",
        "                            lines.append(line)\n",
        "                            para += line\n",
        "                            line = ''\n",
        "                        # breaks.LINE_BREAK : 단락을 끝내는 줄바꿈\n",
        "                        if symbol.property.detected_break.type == breaks.LINE_BREAK:\n",
        "                            lines.append(line)\n",
        "                            para += line\n",
        "                            line = ''\n",
        "                paragraphs.append(para)\n",
        "\n",
        "    return full_text, paragraphs\n",
        "\n",
        "def summarize_paragraphs(paragraphs):\n",
        "\n",
        "    # 문단 리스트를 512 토큰으로 나누기\n",
        "    # 입력문장 리스트 : paragraphs\n",
        "\n",
        "    # 모델 입력 데이터 생성\n",
        "    input_data = []\n",
        "\n",
        "    # 요약 데이터 생성\n",
        "    summary =[]\n",
        "\n",
        "    # 현재까지의 토큰 수 초기화\n",
        "    current_token_count = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        # '★문단시작★'를 제거하고 문단을 토큰화\n",
        "        paragraph_tokens = summary_tokenizer(paragraph.replace('★문단시작★', ''), return_tensors='pt', add_special_tokens=True).input_ids\n",
        "\n",
        "        # 현재 문단을 추가해도 512 토큰을 넘지 않으면 추가\n",
        "        if current_token_count + len(paragraph_tokens[0]) <= 512:\n",
        "            if not input_data:\n",
        "                input_data.append(paragraph_tokens)\n",
        "                current_token_count += len(paragraph_tokens[0])\n",
        "            else:\n",
        "                # 이미 문단이 추가되어 있는 경우 이어서 추가\n",
        "                input_data[-1] = torch.cat((input_data[-1], paragraph_tokens), dim=-1)\n",
        "                current_token_count += len(paragraph_tokens[0])\n",
        "        else:\n",
        "            # 256 토큰을 넘어가면 새로운 입력으로 시작\n",
        "            input_data.append(paragraph_tokens)\n",
        "            current_token_count = len(paragraph_tokens[0])\n",
        "\n",
        "    for inp in input_data:\n",
        "        output = summary_model.generate(inp, eos_token_id=1, max_length=512, num_beams=4)\n",
        "        t_output = summary_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        summary.append(t_output)\n",
        "\n",
        "    return summary\n",
        "\n",
        "def fulltext_keywords(full_text):\n",
        "\n",
        "    kiwi.analyze(full_text)\n",
        "\n",
        "    # 키워드 추출\n",
        "    keywords = kw_model.extract_keywords(full_text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=10)\n",
        "\n",
        "    # 명사 키워드 추출 함수 정의\n",
        "    def extract_nouns(keyword_list):\n",
        "        noun_keywords = []\n",
        "        keywordscore = []\n",
        "        for keyword, score in keywords:\n",
        "            # 형태소 분석 수행\n",
        "            tokens = kiwi.analyze(keyword)\n",
        "            # print(tokens)\n",
        "            for tk in tokens[0][0]:\n",
        "                # print(tk)\n",
        "                if tk.tag == \"NNG\":\n",
        "                    if tk.form not in noun_keywords:\n",
        "                        noun_keywords.append(tk.form)\n",
        "                        keywordscore.append((tk.form,score))\n",
        "\n",
        "        return noun_keywords, keywordscore\n",
        "    noun_keywords, keywordscore = extract_nouns(keywords)\n",
        "    return noun_keywords, keywordscore\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코렙이라서 사용, flask에서는 다른 방법 필요\n",
        "%cd /content/upload_file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_names = os.listdir('/content/upload_file')\n",
        "if '.ipynb_checkpoints' in file_names:\n",
        "    file_names.remove('.ipynb_checkpoints')\n",
        "\n",
        "for files in file_names:\n",
        "    if '.pdf' in files[-4:]:\n",
        "\t    pdf_to_png(files)\n",
        "\n",
        "summarys, keywordscores, noun_keyword = [], [], []\n",
        "full_text = \"\"\n",
        "image_pathes = os.listdir('/content/image_file')\n",
        " # PDF 10장/ 2분 7초\n",
        "for image_paths in image_pathes:\n",
        "    image_path = f'/content/image_file/{image_paths}'\n",
        "    text_full, paragraphs = detect_paragraphs(image_path)\n",
        "    full_text = text_full\n",
        "    summarys.append(summarize_paragraphs(paragraphs))\n",
        "    noun_keywords, keywordscore = fulltext_keywords(full_text)\n",
        "    keywordscores.append(keywordscore)\n",
        "    noun_keyword.append(noun_keywords)\n",
        "\n",
        "from collections import Counter\n",
        "# 키워드 리스트 하나로 합치기\n",
        "word_list = sum(noun_keyword,[])\n",
        "# 단어 빈도수 계산\n",
        "word_counts = Counter(word_list)\n",
        "\n",
        "# 가장 많이 중복된 단어 5개 선택\n",
        "top_5_words = [word for word, count in word_counts.most_common(5)]\n",
        "\n",
        "print(top_5_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "wpfp6o2g47LR",
        "outputId": "54ce39b1-1449-4490-d4a2-1efeac32203f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/upload_file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6115582e-a322-45d5-9663-87a8aa6c6c63\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6115582e-a322-45d5-9663-87a8aa6c6c63\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 현진건-운수좋은날+B3356-개벽.pdf to 현진건-운수좋은날+B3356-개벽.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import fitz\n",
        "import zipfile\n",
        "import cv2\n",
        "import io\n",
        "import os\n",
        "import torch\n",
        "from keybert import KeyBERT\n",
        "from kiwipiepy import Kiwi\n",
        "from transformers import BertModel\n",
        "from PIL import Image\n",
        "from google.cloud import vision\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers.models.bart import BartForConditionalGeneration\n",
        "\n",
        "import torch\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers.models.bart import BartForConditionalGeneration, BartForCausalLM\n",
        "# 모델과 토크나이저 로드\n",
        "# model = BartForConditionalGeneration.from_pretrained('/content/kobart_summary')\n",
        "summary_model = BartForConditionalGeneration.from_pretrained('/content/drive/MyDrive/KoBART-summarization/kobart_summary')\n",
        "summary_tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
        "\n",
        "\n",
        "### 키워드 명사로만 제공\n",
        "from kiwipiepy import Kiwi\n",
        "from transformers import BertModel\n",
        "from keybert import KeyBERT\n",
        "\n",
        "# 텍스트 전문 : full_text\n",
        "# Kiwi 형태소 분석기 초기화\n",
        "kiwi = Kiwi()\n",
        "\n",
        "# BERT 모델 로드\n",
        "keywords_model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "# KeyBERT 모델 초기화\n",
        "kw_model = KeyBERT(keywords_model)\n",
        "\n",
        "# 파일 저장할 폴더 생성\n",
        "os.mkdir(\"/content/upload_file\")\n",
        "os.mkdir(\"/content/image_file\")\n",
        "\n",
        "# PDF 라면 이미지로 변경\n",
        "def pdf_to_png(files):\n",
        "    path = f\"/content/upload_file/{files}\"\n",
        "    doc = fitz.open(path)\n",
        "    for i, page in enumerate(doc):\n",
        "        img = page.get_pixmap()\n",
        "        img.save(f\"/content/image_file/{i}.png\")\n",
        "\n",
        "# OCR 함수\n",
        "def detect_paragraphs(image_path):\n",
        "    from google.cloud import vision\n",
        "    # API키 가져오기\n",
        "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"/content/drive/MyDrive/api-project-397750607032-5ddc025931cd.json\"\n",
        "\n",
        "    # API 가져오기\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "\n",
        "    # 주석을 추가할 이미지 파일 이름\n",
        "    file_name = os.path.abspath(image_path)\n",
        "\n",
        "    # 이미지 로드\n",
        "    with io.open(file_name, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    # 이미지 OCR\n",
        "    response = client.document_text_detection(image=image)\n",
        "\n",
        "    # 이미지 OCR 텍스트 전문\n",
        "    full_text = response.full_text_annotation.text\n",
        "\n",
        "    # 이미지 OCR 후 결과 (bbox, word 등)\n",
        "    pages = response.full_text_annotation.pages\n",
        "\n",
        "    # 텍스트 주석\n",
        "    # 참고 : https://cloud.google.com/dotnet/docs/reference/Google.Cloud.Vision.V1/latest/Google.Cloud.Vision.V1.TextAnnotation.Types.DetectedBreak.Types.BreakType\n",
        "    # 참고 : https://googleapis.github.io/googleapis/java/grpc-google-cloud-vision-v1/0.1.5/apidocs/com/google/cloud/vision/v1/TextAnnotation.DetectedBreak.BreakType.html\n",
        "    breaks = vision.TextAnnotation.DetectedBreak.BreakType\n",
        "\n",
        "    paragraphs = []\n",
        "    lines = []\n",
        "\n",
        "    for page in pages:\n",
        "        for block in page.blocks:\n",
        "            for paragraph in block.paragraphs:\n",
        "                para = \"★문단시작★\"\n",
        "                line = \"\"\n",
        "                for word in paragraph.words:\n",
        "                    for symbol in word.symbols:\n",
        "                        line += symbol.text\n",
        "                        # breaks.SPACE : 공백\n",
        "                        if symbol.property.detected_break.type == breaks.SPACE:\n",
        "                            line += ' '\n",
        "                        # breaks.EOL_SURE_SPACE : 줄 바꿈\n",
        "                        if symbol.property.detected_break.type == breaks.EOL_SURE_SPACE:\n",
        "                            line += ' '\n",
        "                            lines.append(line)\n",
        "                            para += line\n",
        "                            line = ''\n",
        "                        # breaks.LINE_BREAK : 단락을 끝내는 줄바꿈\n",
        "                        if symbol.property.detected_break.type == breaks.LINE_BREAK:\n",
        "                            lines.append(line)\n",
        "                            para += line\n",
        "                            line = ''\n",
        "                paragraphs.append(para)\n",
        "\n",
        "    return full_text, paragraphs\n",
        "\n",
        "def summarize_paragraphs(paragraphs):\n",
        "\n",
        "    # 문단 리스트를 512 토큰으로 나누기\n",
        "    # 입력문장 리스트 : paragraphs\n",
        "\n",
        "    # 모델 입력 데이터 생성\n",
        "    input_data = []\n",
        "\n",
        "    # 요약 데이터 생성\n",
        "    summary =[]\n",
        "\n",
        "    # 현재까지의 토큰 수 초기화\n",
        "    current_token_count = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        # '★문단시작★'를 제거하고 문단을 토큰화\n",
        "        paragraph_tokens = summary_tokenizer(paragraph.replace('★문단시작★', ''), return_tensors='pt', add_special_tokens=True).input_ids\n",
        "\n",
        "        # 현재 문단을 추가해도 512 토큰을 넘지 않으면 추가\n",
        "        if current_token_count + len(paragraph_tokens[0]) <= 512:\n",
        "            if not input_data:\n",
        "                input_data.append(paragraph_tokens)\n",
        "                current_token_count += len(paragraph_tokens[0])\n",
        "            else:\n",
        "                # 이미 문단이 추가되어 있는 경우 이어서 추가\n",
        "                input_data[-1] = torch.cat((input_data[-1], paragraph_tokens), dim=-1)\n",
        "                current_token_count += len(paragraph_tokens[0])\n",
        "        else:\n",
        "            # 256 토큰을 넘어가면 새로운 입력으로 시작\n",
        "            input_data.append(paragraph_tokens)\n",
        "            current_token_count = len(paragraph_tokens[0])\n",
        "\n",
        "    for inp in input_data:\n",
        "        output = summary_model.generate(inp, eos_token_id=1, max_length=512, num_beams=4)\n",
        "        t_output = summary_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        summary.append(t_output)\n",
        "\n",
        "    return summary\n",
        "\n",
        "def fulltext_keywords(full_text):\n",
        "\n",
        "    kiwi.analyze(full_text)\n",
        "\n",
        "    # 키워드 추출\n",
        "    keywords = kw_model.extract_keywords(full_text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=10)\n",
        "\n",
        "    # 명사 키워드 추출 함수 정의\n",
        "    def extract_nouns(keyword_list):\n",
        "        noun_keywords = []\n",
        "        keywordscore = []\n",
        "        for keyword, score in keywords:\n",
        "            # 형태소 분석 수행\n",
        "            tokens = kiwi.analyze(keyword)\n",
        "            # print(tokens)\n",
        "            for tk in tokens[0][0]:\n",
        "                # print(tk)\n",
        "                if tk.tag == \"NNG\":\n",
        "                    if tk.form not in noun_keywords:\n",
        "                        noun_keywords.append(tk.form)\n",
        "                        keywordscore.append((tk.form,score))\n",
        "\n",
        "        return noun_keywords, keywordscore\n",
        "    noun_keywords, keywordscore = extract_nouns(keywords)\n",
        "    return noun_keywords, keywordscore\n",
        "\n",
        "def keyword_sum(noun_keyword, n):\n",
        "    from collections import Counter\n",
        "    # 키워드 리스트 하나로 합치기\n",
        "    word_list = sum(noun_keyword,[])\n",
        "    # 단어 빈도수 계산\n",
        "    word_counts = Counter(word_list)\n",
        "\n",
        "    # 가장 많이 중복된 단어 5개 선택\n",
        "    top_n_words = [word for word, count in word_counts.most_common(n)]\n",
        "    return top_n_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qILVNcHi5W0P",
        "outputId": "94ed2fd8-94e1-44fb-ac25-a556fabc3ea7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['김첨지는 원망스러워 인력거를 타시라고 대어섰고제야 말로 인력거가 무거워지자 마음이 초조해 온다.', '백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백 백'], ['설렁탕 국물이 마시고 싶다고 취한 김첨지는 아무리 설렁탕을 사주었건만 마음이 시원하지 않았다.', '제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제 제'], ['설렁탕을 사가지고 집에 다다라 집 전체를 세든 김첨지에게 김첨지가 주기를 띠지 않았던 한 발을 대문에 들여놓았을 제 그곳을 지배하는 무시 무시한 정적이 다리가 떨렸다.', '늙은 흥부가 몸과 입에 젖을 맞아 운다는 표정이 아닌 늙은 몸을 찡그려 붙여서 운다는 표정이다.'], ['김첨지는 돈 많이 벌었을 테니 어서 빨리 하라는 김 첨지의 말에 귀에 들려왔다.', '방울로 부어 삼십원을 벌었다고 온들 중 취한 이는 치삼의 목을 잡아 치며 술을 열시는 부르짖었다.'], ['김첨지는 주정꾼이 이 눈치를 알아보고 좁다며 일 원짜리 한 장을 꺼내어 중대가리 앞에 펄쩍 집어던졌다.', '마마님이 볼 때 망토를 잡수시고 비를 맞고 서 있을 때 내 손을 탁 뿌리치고 돌아서더니 왜 귀찮게 굴어!\"'], ['김첨지는 제 얼굴에서 떨어진 닭의 똥 같은 풀이 죽은 이의 얼굴에 비비대며 왜 먹지 못하여 개성이 좋냐고 말한다.'], ['학생을 태우고 나선 김 첨지의 다리는 가뿐하여 거의 나는 듯하였고 바퀴는 뒤에서 미끄러지듯 하였다.'], ['인력거를 쥔 채 정거장까지 끌어 준 김첨지는 제 손에 쥐에 쥐나큰 길을 비를 맞아 가며 온 생각은 아니지만 기뻤다.', '또 임시 번의 요행을 바라던 김 첨지는 손님을 물색하던 중 높은 구두를 신고 망토까지 두른 여편네가 눈에 띄었다.'], ['이날은 교원인 듯한 양복쟁이를 동광학교에 태워다 주기로 하는 등 오래간만에 김첨지에게 닥친 운수 좋은 날이었다.', '김첨지는 오래간만에 김첨지에게 의지하면 오라질 년이 천방지축으로 냄비는에 손을 대고 끓였고 오래간만에 김첨지는 김첨지를 쉽게 물리쳤다.'], ['밥 소리 때문에 자신을 귀찮게 굴보는 김첨지에게 치삼은 웃음을 지으며 울기 시작했다.']] [('웃음', 0.649), ('미친놈', 0.5149), ('소리', 0.4609)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코렙이라서 사용, flask에서는 다른 방법 필요\n",
        "%cd /content/upload_file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_names = os.listdir('/content/upload_file')\n",
        "if '.ipynb_checkpoints' in file_names:\n",
        "    file_names.remove('.ipynb_checkpoints')\n",
        "\n",
        "for files in file_names:\n",
        "    if '.pdf' in files[-4:]:\n",
        "\t    pdf_to_png(files)\n",
        "\n",
        "summarys, keywordscores, noun_keyword = [], [], []\n",
        "full_text = \"\"\n",
        "image_pathes = os.listdir('/content/image_file')\n",
        " # PDF 10장/ 2분 7초\n",
        "for image_paths in image_pathes:\n",
        "    image_path = f'/content/image_file/{image_paths}'\n",
        "    text_full, paragraphs = detect_paragraphs(image_path)\n",
        "    full_text = text_full\n",
        "    summarys.append(summarize_paragraphs(paragraphs))\n",
        "    noun_keywords, keywordscore = fulltext_keywords(full_text)\n",
        "    keywordscores.append(keywordscore)\n",
        "    noun_keyword.append(noun_keywords)\n",
        "key_5 =  keyword_sum(noun_keyword, 5)\n",
        "\n",
        "print(summarys, full_text, key_5)"
      ],
      "metadata": {
        "id": "T_JCGjTQ-8II"
      },
      "execution_count": 59,
      "outputs": []
    }
  ]
}