{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QuAuZ7U6NodZxITmAFiDOSNSMqSjq5O6",
      "authorship_tag": "ABX9TyO/kP/lQjlpjvKH4rGMeVQR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guscldns/TestProject/blob/main/%EC%9E%A5%EA%B8%B0/%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert\n",
        "!pip install kiwipiepy\n",
        "!pip install google-cloud-vision\n",
        "!pip install PyMuPDF\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "2mNBPA8-1jaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dCA0Yb81Zjj",
        "outputId": "9a8179cc-6c7a-4ba0-dcdc-daf3da543251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import fitz\n",
        "import zipfile\n",
        "import cv2\n",
        "import io\n",
        "import os\n",
        "import torch\n",
        "from keybert import KeyBERT\n",
        "from kiwipiepy import Kiwi\n",
        "from transformers import BertModel\n",
        "from PIL import Image\n",
        "from google.cloud import vision\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers.models.bart import BartForConditionalGeneration\n",
        "\n",
        "import torch\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers.models.bart import BartForConditionalGeneration, BartForCausalLM\n",
        "# 모델과 토크나이저 로드\n",
        "# model = BartForConditionalGeneration.from_pretrained('/content/kobart_summary')\n",
        "summary_model = BartForConditionalGeneration.from_pretrained('/content/drive/MyDrive/KoBART-summarization/kobart_summary')\n",
        "summary_tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
        "\n",
        "\n",
        "### 키워드 명사로만 제공\n",
        "from kiwipiepy import Kiwi\n",
        "from transformers import BertModel\n",
        "from keybert import KeyBERT\n",
        "\n",
        "# 텍스트 전문 : full_text\n",
        "# Kiwi 형태소 분석기 초기화\n",
        "kiwi = Kiwi()\n",
        "\n",
        "# BERT 모델 로드\n",
        "keywords_model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "# KeyBERT 모델 초기화\n",
        "kw_model = KeyBERT(keywords_model)\n",
        "\n",
        "# 파일 저장할 폴더 생성\n",
        "# os.mkdir(\"/content/upload_file\")\n",
        "# os.mkdir(\"/content/image_file\")\n",
        "\n",
        "# PDF 라면 이미지로 변경\n",
        "def pdf_to_png(files):\n",
        "    path = f\"/content/upload_file/{files}\"\n",
        "    doc = fitz.open(path)\n",
        "    for i, page in enumerate(doc):\n",
        "        img = page.get_pixmap()\n",
        "        img.save(f\"/content/image_file/{i}.png\")\n",
        "\n",
        "# OCR 함수\n",
        "def detect_paragraphs(image_path):\n",
        "    from google.cloud import vision\n",
        "    # API키 가져오기\n",
        "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"/content/drive/MyDrive/api-project-397750607032-5ddc025931cd.json\"\n",
        "\n",
        "    # API 가져오기\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "\n",
        "    # 주석을 추가할 이미지 파일 이름\n",
        "    file_name = os.path.abspath(image_path)\n",
        "\n",
        "    # 이미지 로드\n",
        "    with io.open(file_name, 'rb') as image_file:\n",
        "        content = image_file.read()\n",
        "\n",
        "    image = vision.Image(content=content)\n",
        "\n",
        "    # 이미지 OCR\n",
        "    response = client.document_text_detection(image=image)\n",
        "\n",
        "    # 이미지 OCR 텍스트 전문\n",
        "    full_text = response.full_text_annotation.text\n",
        "\n",
        "    # 이미지 OCR 후 결과 (bbox, word 등)\n",
        "    pages = response.full_text_annotation.pages\n",
        "\n",
        "    # 텍스트 주석\n",
        "    # 참고 : https://cloud.google.com/dotnet/docs/reference/Google.Cloud.Vision.V1/latest/Google.Cloud.Vision.V1.TextAnnotation.Types.DetectedBreak.Types.BreakType\n",
        "    # 참고 : https://googleapis.github.io/googleapis/java/grpc-google-cloud-vision-v1/0.1.5/apidocs/com/google/cloud/vision/v1/TextAnnotation.DetectedBreak.BreakType.html\n",
        "    breaks = vision.TextAnnotation.DetectedBreak.BreakType\n",
        "\n",
        "    paragraphs = []\n",
        "    lines = []\n",
        "\n",
        "    for page in pages:\n",
        "        for block in page.blocks:\n",
        "            for paragraph in block.paragraphs:\n",
        "                para = \"★문단시작★\"\n",
        "                line = \"\"\n",
        "                for word in paragraph.words:\n",
        "                    for symbol in word.symbols:\n",
        "                        line += symbol.text\n",
        "                        # breaks.SPACE : 공백\n",
        "                        if symbol.property.detected_break.type == breaks.SPACE:\n",
        "                            line += ' '\n",
        "                        # breaks.EOL_SURE_SPACE : 줄 바꿈\n",
        "                        if symbol.property.detected_break.type == breaks.EOL_SURE_SPACE:\n",
        "                            line += ' '\n",
        "                            lines.append(line)\n",
        "                            para += line\n",
        "                            line = ''\n",
        "                        # breaks.LINE_BREAK : 단락을 끝내는 줄바꿈\n",
        "                        if symbol.property.detected_break.type == breaks.LINE_BREAK:\n",
        "                            lines.append(line)\n",
        "                            para += line\n",
        "                            line = ''\n",
        "                paragraphs.append(para)\n",
        "\n",
        "    return full_text, paragraphs\n",
        "\n",
        "def summarize_paragraphs(paragraphs):\n",
        "\n",
        "    # 문단 리스트를 512 토큰으로 나누기\n",
        "    # 입력문장 리스트 : paragraphs\n",
        "\n",
        "    # 모델 입력 데이터 생성\n",
        "    input_data = []\n",
        "\n",
        "    # 요약 데이터 생성\n",
        "    summary =[]\n",
        "\n",
        "    # 현재까지의 토큰 수 초기화\n",
        "    current_token_count = 0\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        # '★문단시작★'를 제거하고 문단을 토큰화\n",
        "        paragraph_tokens = summary_tokenizer(paragraph.replace('★문단시작★', ''), return_tensors='pt', add_special_tokens=True).input_ids\n",
        "\n",
        "        # 현재 문단을 추가해도 512 토큰을 넘지 않으면 추가\n",
        "        if current_token_count + len(paragraph_tokens[0]) <= 512:\n",
        "            if not input_data:\n",
        "                input_data.append(paragraph_tokens)\n",
        "                current_token_count += len(paragraph_tokens[0])\n",
        "            else:\n",
        "                # 이미 문단이 추가되어 있는 경우 이어서 추가\n",
        "                input_data[-1] = torch.cat((input_data[-1], paragraph_tokens), dim=-1)\n",
        "                current_token_count += len(paragraph_tokens[0])\n",
        "        else:\n",
        "            # 256 토큰을 넘어가면 새로운 입력으로 시작\n",
        "            input_data.append(paragraph_tokens)\n",
        "            current_token_count = len(paragraph_tokens[0])\n",
        "\n",
        "    for inp in input_data:\n",
        "        output = summary_model.generate(inp, eos_token_id=1, max_length=512, num_beams=4)\n",
        "        t_output = summary_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        summary.append(t_output)\n",
        "\n",
        "    return summary\n",
        "\n",
        "def fulltext_keywords(full_text):\n",
        "\n",
        "    kiwi.analyze(full_text)\n",
        "\n",
        "    # 키워드 추출\n",
        "    keywords = kw_model.extract_keywords(full_text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=10)\n",
        "\n",
        "    # 명사 키워드 추출 함수 정의\n",
        "    def extract_nouns(keyword_list):\n",
        "        noun_keywords = []\n",
        "        keywordscore = []\n",
        "        for keyword, score in keywords:\n",
        "            # 형태소 분석 수행\n",
        "            tokens = kiwi.analyze(keyword)\n",
        "            # print(tokens)\n",
        "            for tk in tokens[0][0]:\n",
        "                # print(tk)\n",
        "                if tk.tag == \"NNG\":\n",
        "                    if tk.form not in noun_keywords:\n",
        "                        noun_keywords.append(tk.form)\n",
        "                        keywordscore.append((tk.form,score))\n",
        "\n",
        "        return noun_keywords, keywordscore\n",
        "    noun_keywords, keywordscore = extract_nouns(keywords)\n",
        "    return noun_keywords, keywordscore\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코렙이라서 사용, flask에서는 다른 방법 필요\n",
        "%cd /content/upload_file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_names = os.listdir('/content/upload_file')\n",
        "if '.ipynb_checkpoints' in file_names:\n",
        "    file_names.remove('.ipynb_checkpoints')\n",
        "\n",
        "for files in file_names:\n",
        "    if '.pdf' in files[-4:]:\n",
        "\t    pdf_to_png(files)\n",
        "\n",
        "summarys, keywordscores, noun_keyword = [], [], []\n",
        "full_text = \"\"\n",
        "image_pathes = os.listdir('/content/image_file')\n",
        " # PDF 10장/ 2분 7초\n",
        "for image_paths in image_pathes:\n",
        "    image_path = f'/content/image_file/{image_paths}'\n",
        "    text_full, paragraphs = detect_paragraphs(image_path)\n",
        "    full_text = text_full\n",
        "    summarys.append(summarize_paragraphs(paragraphs))\n",
        "    noun_keywords, keywordscore = fulltext_keywords(full_text)\n",
        "    keywordscores.append(keywordscore)\n",
        "    noun_keyword.append(noun_keywords)\n",
        "key_5 =  keyword_sum(noun_keyword, 5)\n",
        "\n",
        "print(summarys, full_text, key_5)"
      ],
      "metadata": {
        "id": "T_JCGjTQ-8II"
      },
      "execution_count": 59,
      "outputs": []
    }
  ]
}