{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guscldns/TestProject/blob/main/%EC%9E%A5%EA%B8%B0/poly_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsPUdwzL6k5h"
      },
      "source": [
        "### 5기분들 코드"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG_7aEWnN1bI",
        "outputId": "8ca73a2c-0aeb-4a29-9fb5-6dffeb087530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmu__DU2Tp72"
      },
      "outputs": [],
      "source": [
        "# 5기 분들 git파일 다운로드\n",
        "# !git clone https://github.com/doh0106/Poly-Encoder.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 구글 공유폴더 이용해서 학습하기\n",
        "- 구글 드라이브에서 공유폴더 바로가기 생성 > 구글 드라이브 마운트 > MyDrive에서 공유 폴더 바로가기 선택 > 현재 경로 공유 폴더 경로로 변경"
      ],
      "metadata": {
        "id": "E_Pe8llmOBxo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cxfaBBYbqoD",
        "outputId": "64faf09e-ab93-4ebd-b384-ff6958670381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Poly-Encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4D_W1J6N_q-",
        "outputId": "ca08685e-1ab2-4bd1-a3b5-69ab1db4b5d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Poly-Encoder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5WtBhCJASvXI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "27b77aad-7dca-4ee2-c060-2d42e9583f94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     index                             q1                                 q2  \\\n",
              "0        0     코로나 자가격리시 일을 못하는데 어떻게 하나요?             자가격리시 일을 못하는데 어떻게 하나요?   \n",
              "1        1      모든 사람이 코로나 지원금을 받을 수 있나요?              전부 코로나 지원금을 받을 수 있나요?   \n",
              "2        2      코로나 자격격리 지원금 대상이 어떻게 되나요?           코로나 자격격리 지원금 대상은 누구인가요?    \n",
              "3        3       코로나 자가격리 지원금이 얼마나 지원되나요?             코로나 자가격리 지원금이 얼마나 되나요?   \n",
              "4        4    코로나 자가격리 지원금을 위한 필요서류가 있나요?  코로나 자가격리 지원금을 위해 준비해야할 필요서류가 있나요?   \n",
              "..     ...                            ...                                ...   \n",
              "879    879        희망두배통장을 가입했는데 실직을 했습니다.            희망두배 통장 저축을 잠깐 멈춰도 되나요?   \n",
              "880    880               희망두배통장 신청 가능한가요?         희망두배통장 과외로 돈버는데 신청할 수 있나요?   \n",
              "881    881            서울사랑상품권 환불 할 수 있나요?                    서울사랑상품권 환불 되나요?   \n",
              "882    882          사랑상품권은 어디에서 사용 가능한가요?                 사랑상품권 어디서 쓸 수 있어요?   \n",
              "883    883  재난 긴급생활비 지급 가능 대상인지 어떻게 확인하죠?               재난 긴급생활비 기준이 어떻게 되죠?   \n",
              "\n",
              "                                   q3                             q4  \\\n",
              "0            자가격리로 일을 못하는데 방법이 있나요 ?    코로나 자가격리때문에 일을 못하는데 어떻게 하나요    \n",
              "1        모든 국민이 코로나 지원금을 받을 수 있나요  ?   모든 사람이 코로나 지원금을 수령할 수 있나요  ?    \n",
              "2        코로나 자격격리 지원금 대상이 누구누구 있나요 ?      코로나 자격격리 지원금 대상을 알 수 있을까요?   \n",
              "3         코로나 자가격리 지원금은 얼마나 받을 수 있나요?        코로나 자가격리 지원금이 얼마나 나오나요?   \n",
              "4    코로나 자가격리 지원금 신청할때 준비할 필요서류가 있나요?     코로나 자가격리 지원금 신청할때 서류가 있나요?   \n",
              "..                                ...                            ...   \n",
              "879              희망두배 통장 일시정지가 가능한가요?       실직하였는데 희망두배 통장은 어떻게 되나요?   \n",
              "880                  희망두배통장 개설하고 싶어요.        희망두배통장 만들고 싶은데 어떻게 하나요?   \n",
              "881              서울사랑상품권 안썼는데 환불 되나요?            서울사랑상품권 얼마나 환불 되나요?   \n",
              "882             서울사랑상품권 아무데나 쓸 수 있나요?             사랑상품권 이용 지역이 어딘가요?   \n",
              "883                 재난 긴급생활비 어떻게 받나요?      재난 긴급생활비 대상자는 어디서 확인하나요?    \n",
              "\n",
              "                                              response  \n",
              "0                               정부에서 별도 지원금을 지급하고 있습니다  \n",
              "1                         일용직 노동자 등 취약노동자들이 받을 수 있습니다.  \n",
              "2    주 사십시간 미만 단시간 노동자, 일용직 노동자, 특수형태노동종사자, 요양보호사가 ...  \n",
              "3                                    일인당 이십삼만원이 지급됩니다.  \n",
              "4                        신청서와 신분증 사분, 자가격리이행 확약서 등입니다.  \n",
              "..                                                 ...  \n",
              "879  실직으로 중단하실경우 최대 육개월까지 서울시 복지재단에서 일시중지 신청하실 수 있습니다.  \n",
              "880  고용임금확인서 및 통장사본을 제출함으로 신청하실 수 있습니다. 자세한 조건은 희망두...  \n",
              "881                        미사용 상품권은 7일 이내 전액 환불 가능합니다.  \n",
              "882  발행 자치구 관내에서만 가능하며 제로페이 홈페이지나 제로페이 콜센터를 통해 이용 가...  \n",
              "883  주소지 관할 동주민센터에서 지급 가능 대상인지 확인가능하며 대리인 방문시 신분증, ...  \n",
              "\n",
              "[884 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3234929b-5582-4889-a9d4-3652f59cd555\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>q1</th>\n",
              "      <th>q2</th>\n",
              "      <th>q3</th>\n",
              "      <th>q4</th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>코로나 자가격리시 일을 못하는데 어떻게 하나요?</td>\n",
              "      <td>자가격리시 일을 못하는데 어떻게 하나요?</td>\n",
              "      <td>자가격리로 일을 못하는데 방법이 있나요 ?</td>\n",
              "      <td>코로나 자가격리때문에 일을 못하는데 어떻게 하나요</td>\n",
              "      <td>정부에서 별도 지원금을 지급하고 있습니다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>모든 사람이 코로나 지원금을 받을 수 있나요?</td>\n",
              "      <td>전부 코로나 지원금을 받을 수 있나요?</td>\n",
              "      <td>모든 국민이 코로나 지원금을 받을 수 있나요  ?</td>\n",
              "      <td>모든 사람이 코로나 지원금을 수령할 수 있나요  ?</td>\n",
              "      <td>일용직 노동자 등 취약노동자들이 받을 수 있습니다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>코로나 자격격리 지원금 대상이 어떻게 되나요?</td>\n",
              "      <td>코로나 자격격리 지원금 대상은 누구인가요?</td>\n",
              "      <td>코로나 자격격리 지원금 대상이 누구누구 있나요 ?</td>\n",
              "      <td>코로나 자격격리 지원금 대상을 알 수 있을까요?</td>\n",
              "      <td>주 사십시간 미만 단시간 노동자, 일용직 노동자, 특수형태노동종사자, 요양보호사가 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>코로나 자가격리 지원금이 얼마나 지원되나요?</td>\n",
              "      <td>코로나 자가격리 지원금이 얼마나 되나요?</td>\n",
              "      <td>코로나 자가격리 지원금은 얼마나 받을 수 있나요?</td>\n",
              "      <td>코로나 자가격리 지원금이 얼마나 나오나요?</td>\n",
              "      <td>일인당 이십삼만원이 지급됩니다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>코로나 자가격리 지원금을 위한 필요서류가 있나요?</td>\n",
              "      <td>코로나 자가격리 지원금을 위해 준비해야할 필요서류가 있나요?</td>\n",
              "      <td>코로나 자가격리 지원금 신청할때 준비할 필요서류가 있나요?</td>\n",
              "      <td>코로나 자가격리 지원금 신청할때 서류가 있나요?</td>\n",
              "      <td>신청서와 신분증 사분, 자가격리이행 확약서 등입니다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>879</th>\n",
              "      <td>879</td>\n",
              "      <td>희망두배통장을 가입했는데 실직을 했습니다.</td>\n",
              "      <td>희망두배 통장 저축을 잠깐 멈춰도 되나요?</td>\n",
              "      <td>희망두배 통장 일시정지가 가능한가요?</td>\n",
              "      <td>실직하였는데 희망두배 통장은 어떻게 되나요?</td>\n",
              "      <td>실직으로 중단하실경우 최대 육개월까지 서울시 복지재단에서 일시중지 신청하실 수 있습니다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>880</th>\n",
              "      <td>880</td>\n",
              "      <td>희망두배통장 신청 가능한가요?</td>\n",
              "      <td>희망두배통장 과외로 돈버는데 신청할 수 있나요?</td>\n",
              "      <td>희망두배통장 개설하고 싶어요.</td>\n",
              "      <td>희망두배통장 만들고 싶은데 어떻게 하나요?</td>\n",
              "      <td>고용임금확인서 및 통장사본을 제출함으로 신청하실 수 있습니다. 자세한 조건은 희망두...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>881</th>\n",
              "      <td>881</td>\n",
              "      <td>서울사랑상품권 환불 할 수 있나요?</td>\n",
              "      <td>서울사랑상품권 환불 되나요?</td>\n",
              "      <td>서울사랑상품권 안썼는데 환불 되나요?</td>\n",
              "      <td>서울사랑상품권 얼마나 환불 되나요?</td>\n",
              "      <td>미사용 상품권은 7일 이내 전액 환불 가능합니다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>882</th>\n",
              "      <td>882</td>\n",
              "      <td>사랑상품권은 어디에서 사용 가능한가요?</td>\n",
              "      <td>사랑상품권 어디서 쓸 수 있어요?</td>\n",
              "      <td>서울사랑상품권 아무데나 쓸 수 있나요?</td>\n",
              "      <td>사랑상품권 이용 지역이 어딘가요?</td>\n",
              "      <td>발행 자치구 관내에서만 가능하며 제로페이 홈페이지나 제로페이 콜센터를 통해 이용 가...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>883</td>\n",
              "      <td>재난 긴급생활비 지급 가능 대상인지 어떻게 확인하죠?</td>\n",
              "      <td>재난 긴급생활비 기준이 어떻게 되죠?</td>\n",
              "      <td>재난 긴급생활비 어떻게 받나요?</td>\n",
              "      <td>재난 긴급생활비 대상자는 어디서 확인하나요?</td>\n",
              "      <td>주소지 관할 동주민센터에서 지급 가능 대상인지 확인가능하며 대리인 방문시 신분증, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>884 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3234929b-5582-4889-a9d4-3652f59cd555')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3234929b-5582-4889-a9d4-3652f59cd555 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3234929b-5582-4889-a9d4-3652f59cd555');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f77dc1fb-b0b6-4ccc-b35e-8fdfc90e1155\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f77dc1fb-b0b6-4ccc-b35e-8fdfc90e1155')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f77dc1fb-b0b6-4ccc-b35e-8fdfc90e1155 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# with open('/content/drive/MyDrive/Poly-Encoder/datasets/train.pickle', 'rb') as f:\n",
        "with open('/content/drive/MyDrive/Poly-Encoder/datasets/dasan_train_data.pickle', 'rb') as f:\n",
        "    corona = pickle.load(f)\n",
        "corona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQT9pKy_YUtO",
        "outputId": "e9f38a87-d1cb-4b0b-854f-3347d0ad1c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-02 04:50:13.501396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(bert_model='models/bert/', eval=False, model_type='bert', output_dir='result/train1', train_dir='datasets/', train_file='train.pickle', valid_file='train.pickle', test_file='test.pickle', neg_size=15, use_pretrain=True, architecture='poly', max_contexts_length=256, max_response_length=64, train_batch_size=2, eval_batch_size=2, print_freq=100, poly_m=16, learning_rate=5e-05, weight_decay=0.01, warmup_steps=100, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, seed=12345, gradient_accumulation_steps=1, fp16=False, fp16_opt_level='O1', gpu=0)\n",
            "================================================================================\n",
            "Train dir: datasets/\n",
            "Output dir: result/train1\n",
            "================================================================================\n",
            "100% 4702/4702 [00:11<00:00, 398.67it/s]\n",
            "100% 4702/4702 [00:14<00:00, 324.83it/s]\n",
            "Loading parameters from models/bert/pytorch_model.bin\n",
            "Some weights of BertModel were not initialized from the model checkpoint at models/bert/ and are newly initialized: ['encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.11.attention.self.query.bias', 'pooler.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 42001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "모델 투 디바이스  cuda\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Print freq: 100 Eval freq: 1000\n",
            "  4% 100/2351 [00:18<06:54,  5.43it/s]100 4.620238346285187\n",
            "  9% 200/2351 [00:33<05:59,  5.98it/s]200 3.3967899790476075\n",
            " 13% 300/2351 [00:49<05:32,  6.16it/s]300 2.955061507949916\n",
            " 17% 400/2351 [01:05<05:12,  6.25it/s]400 2.7640878928487655\n",
            " 21% 500/2351 [01:20<04:54,  6.29it/s]500 2.6271977207986636\n",
            " 26% 600/2351 [01:36<04:37,  6.30it/s]600 2.596692080259478\n",
            " 30% 700/2351 [01:52<04:23,  6.26it/s]700 2.5839482028576146\n",
            " 34% 800/2351 [02:08<04:08,  6.25it/s]800 2.5173484516941245\n",
            " 38% 900/2351 [02:25<03:52,  6.24it/s]900 2.4647538730802223\n",
            " 43% 1000/2351 [02:41<03:36,  6.23it/s]1000 2.4459635414602237\n",
            "Global Step 1000 VAL res:\n",
            " {'train_loss': 2.4459635414602237, 'eval_loss': 2.7725865938266963, 'R1': 0.07422373458102935, 'R2': 0.13845172267120373, 'R5': 0.33836665248830283, 'R10': 0.6448319863887707, 'MRR': 0.2026627951659853, 'epoch': 1, 'global_step': 1000}\n",
            "[Saving at] result/train1/poly_16_pytorch_model.bin\n",
            " 47% 1100/2351 [08:52<25:59,  1.25s/it]1100 2.4045378548135474\n",
            " 51% 1200/2351 [09:08<17:35,  1.09it/s]1200 2.3870227152256605\n",
            " 55% 1300/2351 [09:25<12:04,  1.45it/s]1300 2.3551146130743796\n",
            " 60% 1400/2351 [09:41<08:24,  1.88it/s]1400 2.324932594338565\n",
            " 64% 1500/2351 [09:58<05:57,  2.38it/s]1500 2.3052254900739837\n",
            " 68% 1600/2351 [10:14<04:18,  2.91it/s]1600 2.267568603298278\n",
            " 72% 1700/2351 [10:31<03:08,  3.45it/s]1700 2.238574479731238\n",
            " 77% 1800/2351 [10:47<02:19,  3.96it/s]1800 2.215352350991323\n",
            " 81% 1900/2351 [11:04<01:42,  4.42it/s]1900 2.183675581280044\n",
            " 85% 2000/2351 [11:20<01:13,  4.81it/s]2000 2.151754005186725\n",
            "Global Step 2000 VAL res:\n",
            " {'train_loss': 2.151754005186725, 'eval_loss': 2.7725885578643914, 'R1': 0.13845172267120373, 'R2': 0.226286686516376, 'R5': 0.4410888983411314, 'R10': 0.7216078264568269, 'MRR': 0.1562358369807328, 'epoch': 1, 'global_step': 2000}\n",
            " 89% 2100/2351 [17:32<05:16,  1.26s/it]2100 2.1275001559446434\n",
            " 94% 2200/2351 [17:49<02:20,  1.07it/s]2200 2.101921889003451\n",
            " 98% 2300/2351 [18:05<00:35,  1.42it/s]2300 2.0847474738538425\n",
            "[Saving at] result/train1/poly_16_pytorch_model_train.bin\n",
            "Epoch 1, Global Step 2351 VAL res:\n",
            " {'train_loss': 2.072546678977123, 'eval_loss': 2.772589014622061, 'R1': 0.13568694172692472, 'R2': 0.21565291365376435, 'R5': 0.4287537218205019, 'R10': 0.7216078264568269, 'MRR': 0.16083228323498422, 'epoch': 1, 'global_step': 2351}\n",
            " 98% 2300/2351 [24:11<00:32,  1.58it/s]\n"
          ]
        }
      ],
      "source": [
        "# train poly_encoder\n",
        "!python utils/run.py \\\n",
        "--model_type bert \\\n",
        "--bert_model models/bert/ \\\n",
        "--output_dir result/train1 \\\n",
        "--train_dir datasets/ \\\n",
        "--train_file train.pickle \\\n",
        "--valid_file train.pickle \\\n",
        "--use_pretrain \\\n",
        "--architecture poly \\\n",
        "--poly_m 16 \\\n",
        "--train_batch_size 2 \\\n",
        "--eval_batch_size 2 \\\n",
        "--max_contexts_length 256 \\\n",
        "--max_response_length 64 \\\n",
        "--num_train_epochs 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Poly-Encoder\n",
        "# train poly_encoder _ 재학습\n",
        "!python utils/run.py \\\n",
        "--model_type bert \\\n",
        "--bert_model models/bert/ \\\n",
        "--output_dir models/poly_encoder \\\n",
        "--train_dir datasets/ \\\n",
        "--train_file train.pickle \\\n",
        "--valid_file train.pickle \\\n",
        "--use_pretrain \\\n",
        "--architecture poly \\\n",
        "--poly_m 16 \\\n",
        "--train_batch_size 2 \\\n",
        "--eval_batch_size 2 \\\n",
        "--max_contexts_length 256 \\\n",
        "--max_response_length 64 \\\n",
        "--num_train_epochs 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsiMGnIwDOQ0",
        "outputId": "56737795-e719-4764-efb0-3f1d76b73f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Poly-Encoder\n",
            "2023-09-04 03:10:17.564785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(bert_model='models/bert/', eval=False, model_type='bert', output_dir='models/poly_encoder', train_dir='datasets/', train_file='train.pickle', valid_file='train.pickle', test_file='test.pickle', neg_size=15, use_pretrain=True, architecture='poly', max_contexts_length=256, max_response_length=64, train_batch_size=2, eval_batch_size=2, print_freq=100, poly_m=16, learning_rate=5e-05, weight_decay=0.01, warmup_steps=100, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, seed=12345, gradient_accumulation_steps=1, fp16=False, fp16_opt_level='O1', gpu=0)\n",
            "================================================================================\n",
            "Train dir: datasets/\n",
            "Output dir: models/poly_encoder\n",
            "================================================================================\n",
            "100% 4702/4702 [00:11<00:00, 423.40it/s]\n",
            "100% 4702/4702 [00:12<00:00, 374.84it/s]\n",
            "Loading parameters from models/bert/pytorch_model.bin\n",
            "Some weights of BertModel were not initialized from the model checkpoint at models/bert/ and are newly initialized: ['encoder.layer.10.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.10.attention.self.value.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'pooler.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.attention.self.value.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.10.attention.self.value.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.attention.self.key.bias', 'embeddings.LayerNorm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 42001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "모델 투 디바이스  cuda\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Print freq: 100 Eval freq: 1000\n",
            "  4% 100/2351 [00:16<06:12,  6.05it/s]100 4.620238346285187\n",
            "  9% 200/2351 [00:32<05:44,  6.24it/s]200 3.3967899790476075\n",
            " 13% 300/2351 [00:48<05:27,  6.25it/s]300 2.955061507949916\n",
            " 17% 400/2351 [01:04<05:13,  6.23it/s]400 2.7640878928487655\n",
            " 21% 500/2351 [01:20<04:58,  6.20it/s]500 2.6271977207986636\n",
            " 26% 600/2351 [01:37<04:45,  6.14it/s]600 2.596692080259478\n",
            " 30% 700/2351 [01:53<04:30,  6.10it/s]700 2.5839482028576146\n",
            " 34% 800/2351 [02:10<04:15,  6.06it/s]800 2.5173484516941245\n",
            " 38% 900/2351 [02:27<04:00,  6.03it/s]900 2.4647538730802223\n",
            " 43% 1000/2351 [02:43<03:43,  6.05it/s]1000 2.4459635414602237\n",
            "Global Step 1000 VAL res:\n",
            " {'train_loss': 2.4459635414602237, 'eval_loss': 2.7725865938266963, 'R1': 0.07422373458102935, 'R2': 0.13845172267120373, 'R5': 0.33836665248830283, 'R10': 0.6448319863887707, 'MRR': 0.2026627951659853, 'epoch': 1, 'global_step': 1000}\n",
            "[Saving at] models/poly_encoder/poly_16_pytorch_model.bin\n",
            " 47% 1100/2351 [08:59<26:22,  1.26s/it]1100 2.4045378548135474\n",
            " 51% 1200/2351 [09:16<17:51,  1.07it/s]1200 2.3870227152256605\n",
            " 55% 1300/2351 [09:32<12:14,  1.43it/s]1300 2.3551146130743796\n",
            " 60% 1400/2351 [09:49<08:31,  1.86it/s]1400 2.324932594338565\n",
            " 64% 1500/2351 [10:05<06:02,  2.35it/s]1500 2.3052254900739837\n",
            " 68% 1600/2351 [10:22<04:20,  2.88it/s]1600 2.267568603298278\n",
            " 72% 1700/2351 [10:39<03:10,  3.41it/s]1700 2.238574479731238\n",
            " 77% 1800/2351 [10:55<02:20,  3.92it/s]1800 2.215352350991323\n",
            " 81% 1900/2351 [11:12<01:43,  4.37it/s]1900 2.183675581280044\n",
            " 85% 2000/2351 [11:29<01:13,  4.76it/s]2000 2.151754005186725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLQF_xyvXD9s"
      },
      "outputs": [],
      "source": [
        "# !python3 /content/Poly-Encoder/utils/train.ipynb -bert_model /your/pretrained/model/dir/content/Poly-Encoder/models/bert/pytorch_model.bin --output_dir /your/ckpt/dir --train_dir /your/data/dir --use_pretrain --architecture poly --poly_m 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9A2NNPEcKqD",
        "outputId": "1c768da2-9d61-4353-bd48-ab3b246d1c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-02 23:57:52.761661: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: run.py\n",
            "       [-h]\n",
            "       [--bert_model BERT_MODEL]\n",
            "       [--eval]\n",
            "       [--model_type MODEL_TYPE]\n",
            "       --output_dir\n",
            "       OUTPUT_DIR\n",
            "       [--train_dir TRAIN_DIR]\n",
            "       [--train_file TRAIN_FILE]\n",
            "       [--valid_file VALID_FILE]\n",
            "       [--test_file TEST_FILE]\n",
            "       [--neg_size NEG_SIZE]\n",
            "       [--use_pretrain]\n",
            "       --architecture\n",
            "       ARCHITECTURE\n",
            "       [--max_contexts_length MAX_CONTEXTS_LENGTH]\n",
            "       [--max_response_length MAX_RESPONSE_LENGTH]\n",
            "       [--train_batch_size TRAIN_BATCH_SIZE]\n",
            "       [--eval_batch_size EVAL_BATCH_SIZE]\n",
            "       [--print_freq PRINT_FREQ]\n",
            "       [--poly_m POLY_M]\n",
            "       [--learning_rate LEARNING_RATE]\n",
            "       [--weight_decay WEIGHT_DECAY]\n",
            "       [--warmup_steps WARMUP_STEPS]\n",
            "       [--adam_epsilon ADAM_EPSILON]\n",
            "       [--max_grad_norm MAX_GRAD_NORM]\n",
            "       [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "       [--seed SEED]\n",
            "       [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "       [--fp16]\n",
            "       [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "       [--gpu GPU]\n",
            "run.py: error: unrecognized arguments: /content/drive/MyDrive/Poly-Encoder/result/train2/cross_0_pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "# train cross_encoder\n",
        "!python utils/run.py \\\n",
        "--model_type bert \\\n",
        "--bert_model models/bert/ \\\n",
        "--output_dir result/train2 \\\n",
        "--train_dir datasets/ \\\n",
        "--train_file train.pickle \\\n",
        "--valid_file train.pickle \\\n",
        "--use_pretrain \\\n",
        "--architecture cross \\\n",
        "--train_batch_size 2 \\\n",
        "--eval_batch_size 2 \\\n",
        "--max_contexts_length 256 \\\n",
        "--max_response_length 64 \\\n",
        "--num_train_epochs 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "4cflSu3RcG2P",
        "outputId": "a1df3613-a205-4f1d-c294-b63650b65f98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Poly-Encoder/result/train2/pytorch_model.bin\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0b73b5839f61>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0membedding_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcross_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoad_Model_Tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cross'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mpoly_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoad_Model_Tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoly_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'poly'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mcross_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Poly-Encoder/utils/model_for_inference.py\u001b[0m in \u001b[0;36mLoad_Model_Tokenizer\u001b[0;34m(model_path, model_type, bert_type)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_model_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_model_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizerClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Poly-Encoder/result/train2/pytorch_model.bin'"
          ]
        }
      ],
      "source": [
        "from utils.model_for_inference import Load_Model_Tokenizer\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "\n",
        "CATEGORY = 'corona' # normal, water, corona\n",
        "\n",
        "poly_dir = '/content/Poly-Encoder/result/train1'\n",
        "cross_dir = '/content/Poly-Encoder/result/train2'\n",
        "emb_dir = '/content/Poly-Encoder/datasets'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# q1, q2, q3, q4, text, embedding(text에 대한) 저장 pickle / 하지만 현재는 inference에 text와 embedding만 사용\n",
        "# with open(os.path.join(emb_dir, f'{CATEGORY}_with_text.pickle'), 'rb') as f:\n",
        "with open(os.path.join(emb_dir, f'{CATEGORY}_with_text.pickle'), 'rb') as f:\n",
        "    embedding_df = pickle.load(f)\n",
        "\n",
        "cross_encoder, _ = Load_Model_Tokenizer(cross_dir, model_type='cross') # train폴더에서 pytorch_model.bin으로 이름 변경\n",
        "poly_encoder, tokenizer = Load_Model_Tokenizer(poly_dir, model_type='poly')# train폴더에서 pytorch_model.bin으로 이름 변경\n",
        "cross_encoder.to(device)\n",
        "poly_encoder.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6BbwqCaq5QC"
      },
      "outputs": [],
      "source": [
        "# # 코드 셀 <undefined>\n",
        "# # # %% [code]\n",
        "from utils.inference import Callcenter\n",
        "import numpy as np\n",
        "\n",
        "call_center = Callcenter(poly_encoder=poly_encoder, cross_encoder=cross_encoder,\n",
        "                        tokenizer=tokenizer, emb_df=embedding_df, device=device, topk=5)\n",
        "\n",
        "query = '집에 가고 싶다.'\n",
        "top_k_cross_scores, top_k_indices= call_center.inference(query)\n",
        "# poly score가 가장 높은 k개의 답변 값들의 cross score 값과 index들 반환\n",
        "\n",
        "top_cross_idx = top_k_indices[np.argmax(top_k_cross_scores)]\n",
        "# k개의 답변들 중 cross score가 가장 높은 idx이용하여 최종 답변 산출\n",
        "answer = embedding_df['text'].iloc[top_cross_idx]\n",
        "\n",
        "print(f'질문 : {query}')\n",
        "print(f'답변 : {answer}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ludNeKZGrwS8",
        "outputId": "a479342a-c52b-41f6-df87-c21008b8a6dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "질문 : 코로나19.\n",
            "답변 :   네, 코로나십구 장기화로 매출 타격을 받고 있는 소상공인과 생계에 어려움을 겪는 취약계층 가구를 대상으로 도시가스 요금 납부 부담을 완화해드립니다.\n"
          ]
        }
      ],
      "source": [
        "# # 코드 셀 <undefined>\n",
        "# # # %% [code]\n",
        "from utils.inference import Callcenter\n",
        "import numpy as np\n",
        "\n",
        "call_center = Callcenter(poly_encoder=poly_encoder, cross_encoder=cross_encoder,\n",
        "                        tokenizer=tokenizer, emb_df=embedding_df, device=device, topk=5)\n",
        "\n",
        "query = '코로나19.'\n",
        "top_k_cross_scores, top_k_indices= call_center.inference(query)\n",
        "# poly score가 가장 높은 k개의 답변 값들의 cross score 값과 index들 반환\n",
        "\n",
        "top_cross_idx = top_k_indices[np.argmax(top_k_cross_scores)]\n",
        "# k개의 답변들 중 cross score가 가장 높은 idx이용하여 최종 답변 산출\n",
        "answer = embedding_df['text'].iloc[top_cross_idx]\n",
        "\n",
        "print(f'질문 : {query}')\n",
        "print(f'답변 : {answer}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IRnirFkca7r",
        "outputId": "d157ed1a-9442-4ef3-c6cc-7ba1cf6b8e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(bert_model='models/bert', text_path='/path/to/카테고리별답변들.txt', max_response_length=128, output_dir='/path/to/카테고리별embedding.pickle', model_type='bert', gpu=1)\n",
            "Loading parameters from models/bert/pytorch_model.bin\n",
            "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 42001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Poly-Encoder/utils/text_2_emb.py\", line 84, in <module>\n",
            "    model.load_state_dict(model_state_dict)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2041, in load_state_dict\n",
            "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
            "RuntimeError: Error(s) in loading state_dict for PolyEncoder:\n",
            "\tMissing key(s) in state_dict: \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"poly_code_embeddings.weight\". \n",
            "\tUnexpected key(s) in state_dict: \"encoder.embeddings.position_ids\", \"encoder.embeddings.word_embeddings.weight\", \"encoder.embeddings.position_embeddings.weight\", \"encoder.embeddings.token_type_embeddings.weight\", \"encoder.embeddings.LayerNorm.weight\", \"encoder.embeddings.LayerNorm.bias\", \"encoder.encoder.layer.0.attention.self.query.weight\", \"encoder.encoder.layer.0.attention.self.query.bias\", \"encoder.encoder.layer.0.attention.self.key.weight\", \"encoder.encoder.layer.0.attention.self.key.bias\", \"encoder.encoder.layer.0.attention.self.value.weight\", \"encoder.encoder.layer.0.attention.self.value.bias\", \"encoder.encoder.layer.0.attention.output.dense.weight\", \"encoder.encoder.layer.0.attention.output.dense.bias\", \"encoder.encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.0.intermediate.dense.weight\", \"encoder.encoder.layer.0.intermediate.dense.bias\", \"encoder.encoder.layer.0.output.dense.weight\", \"encoder.encoder.layer.0.output.dense.bias\", \"encoder.encoder.layer.0.output.LayerNorm.weight\", \"encoder.encoder.layer.0.output.LayerNorm.bias\", \"encoder.encoder.layer.1.attention.self.query.weight\", \"encoder.encoder.layer.1.attention.self.query.bias\", \"encoder.encoder.layer.1.attention.self.key.weight\", \"encoder.encoder.layer.1.attention.self.key.bias\", \"encoder.encoder.layer.1.attention.self.value.weight\", \"encoder.encoder.layer.1.attention.self.value.bias\", \"encoder.encoder.layer.1.attention.output.dense.weight\", \"encoder.encoder.layer.1.attention.output.dense.bias\", \"encoder.encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.1.intermediate.dense.weight\", \"encoder.encoder.layer.1.intermediate.dense.bias\", \"encoder.encoder.layer.1.output.dense.weight\", \"encoder.encoder.layer.1.output.dense.bias\", \"encoder.encoder.layer.1.output.LayerNorm.weight\", \"encoder.encoder.layer.1.output.LayerNorm.bias\", \"encoder.encoder.layer.2.attention.self.query.weight\", \"encoder.encoder.layer.2.attention.self.query.bias\", \"encoder.encoder.layer.2.attention.self.key.weight\", \"encoder.encoder.layer.2.attention.self.key.bias\", \"encoder.encoder.layer.2.attention.self.value.weight\", \"encoder.encoder.layer.2.attention.self.value.bias\", \"encoder.encoder.layer.2.attention.output.dense.weight\", \"encoder.encoder.layer.2.attention.output.dense.bias\", \"encoder.encoder.layer.2.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.2.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.2.intermediate.dense.weight\", \"encoder.encoder.layer.2.intermediate.dense.bias\", \"encoder.encoder.layer.2.output.dense.weight\", \"encoder.encoder.layer.2.output.dense.bias\", \"encoder.encoder.layer.2.output.LayerNorm.weight\", \"encoder.encoder.layer.2.output.LayerNorm.bias\", \"encoder.encoder.layer.3.attention.self.query.weight\", \"encoder.encoder.layer.3.attention.self.query.bias\", \"encoder.encoder.layer.3.attention.self.key.weight\", \"encoder.encoder.layer.3.attention.self.key.bias\", \"encoder.encoder.layer.3.attention.self.value.weight\", \"encoder.encoder.layer.3.attention.self.value.bias\", \"encoder.encoder.layer.3.attention.output.dense.weight\", \"encoder.encoder.layer.3.attention.output.dense.bias\", \"encoder.encoder.layer.3.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.3.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.3.intermediate.dense.weight\", \"encoder.encoder.layer.3.intermediate.dense.bias\", \"encoder.encoder.layer.3.output.dense.weight\", \"encoder.encoder.layer.3.output.dense.bias\", \"encoder.encoder.layer.3.output.LayerNorm.weight\", \"encoder.encoder.layer.3.output.LayerNorm.bias\", \"encoder.encoder.layer.4.attention.self.query.weight\", \"encoder.encoder.layer.4.attention.self.query.bias\", \"encoder.encoder.layer.4.attention.self.key.weight\", \"encoder.encoder.layer.4.attention.self.key.bias\", \"encoder.encoder.layer.4.attention.self.value.weight\", \"encoder.encoder.layer.4.attention.self.value.bias\", \"encoder.encoder.layer.4.attention.output.dense.weight\", \"encoder.encoder.layer.4.attention.output.dense.bias\", \"encoder.encoder.layer.4.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.4.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.4.intermediate.dense.weight\", \"encoder.encoder.layer.4.intermediate.dense.bias\", \"encoder.encoder.layer.4.output.dense.weight\", \"encoder.encoder.layer.4.output.dense.bias\", \"encoder.encoder.layer.4.output.LayerNorm.weight\", \"encoder.encoder.layer.4.output.LayerNorm.bias\", \"encoder.encoder.layer.5.attention.self.query.weight\", \"encoder.encoder.layer.5.attention.self.query.bias\", \"encoder.encoder.layer.5.attention.self.key.weight\", \"encoder.encoder.layer.5.attention.self.key.bias\", \"encoder.encoder.layer.5.attention.self.value.weight\", \"encoder.encoder.layer.5.attention.self.value.bias\", \"encoder.encoder.layer.5.attention.output.dense.weight\", \"encoder.encoder.layer.5.attention.output.dense.bias\", \"encoder.encoder.layer.5.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.5.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.5.intermediate.dense.weight\", \"encoder.encoder.layer.5.intermediate.dense.bias\", \"encoder.encoder.layer.5.output.dense.weight\", \"encoder.encoder.layer.5.output.dense.bias\", \"encoder.encoder.layer.5.output.LayerNorm.weight\", \"encoder.encoder.layer.5.output.LayerNorm.bias\", \"encoder.encoder.layer.6.attention.self.query.weight\", \"encoder.encoder.layer.6.attention.self.query.bias\", \"encoder.encoder.layer.6.attention.self.key.weight\", \"encoder.encoder.layer.6.attention.self.key.bias\", \"encoder.encoder.layer.6.attention.self.value.weight\", \"encoder.encoder.layer.6.attention.self.value.bias\", \"encoder.encoder.layer.6.attention.output.dense.weight\", \"encoder.encoder.layer.6.attention.output.dense.bias\", \"encoder.encoder.layer.6.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.6.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.6.intermediate.dense.weight\", \"encoder.encoder.layer.6.intermediate.dense.bias\", \"encoder.encoder.layer.6.output.dense.weight\", \"encoder.encoder.layer.6.output.dense.bias\", \"encoder.encoder.layer.6.output.LayerNorm.weight\", \"encoder.encoder.layer.6.output.LayerNorm.bias\", \"encoder.encoder.layer.7.attention.self.query.weight\", \"encoder.encoder.layer.7.attention.self.query.bias\", \"encoder.encoder.layer.7.attention.self.key.weight\", \"encoder.encoder.layer.7.attention.self.key.bias\", \"encoder.encoder.layer.7.attention.self.value.weight\", \"encoder.encoder.layer.7.attention.self.value.bias\", \"encoder.encoder.layer.7.attention.output.dense.weight\", \"encoder.encoder.layer.7.attention.output.dense.bias\", \"encoder.encoder.layer.7.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.7.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.7.intermediate.dense.weight\", \"encoder.encoder.layer.7.intermediate.dense.bias\", \"encoder.encoder.layer.7.output.dense.weight\", \"encoder.encoder.layer.7.output.dense.bias\", \"encoder.encoder.layer.7.output.LayerNorm.weight\", \"encoder.encoder.layer.7.output.LayerNorm.bias\", \"encoder.encoder.layer.8.attention.self.query.weight\", \"encoder.encoder.layer.8.attention.self.query.bias\", \"encoder.encoder.layer.8.attention.self.key.weight\", \"encoder.encoder.layer.8.attention.self.key.bias\", \"encoder.encoder.layer.8.attention.self.value.weight\", \"encoder.encoder.layer.8.attention.self.value.bias\", \"encoder.encoder.layer.8.attention.output.dense.weight\", \"encoder.encoder.layer.8.attention.output.dense.bias\", \"encoder.encoder.layer.8.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.8.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.8.intermediate.dense.weight\", \"encoder.encoder.layer.8.intermediate.dense.bias\", \"encoder.encoder.layer.8.output.dense.weight\", \"encoder.encoder.layer.8.output.dense.bias\", \"encoder.encoder.layer.8.output.LayerNorm.weight\", \"encoder.encoder.layer.8.output.LayerNorm.bias\", \"encoder.encoder.layer.9.attention.self.query.weight\", \"encoder.encoder.layer.9.attention.self.query.bias\", \"encoder.encoder.layer.9.attention.self.key.weight\", \"encoder.encoder.layer.9.attention.self.key.bias\", \"encoder.encoder.layer.9.attention.self.value.weight\", \"encoder.encoder.layer.9.attention.self.value.bias\", \"encoder.encoder.layer.9.attention.output.dense.weight\", \"encoder.encoder.layer.9.attention.output.dense.bias\", \"encoder.encoder.layer.9.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.9.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.9.intermediate.dense.weight\", \"encoder.encoder.layer.9.intermediate.dense.bias\", \"encoder.encoder.layer.9.output.dense.weight\", \"encoder.encoder.layer.9.output.dense.bias\", \"encoder.encoder.layer.9.output.LayerNorm.weight\", \"encoder.encoder.layer.9.output.LayerNorm.bias\", \"encoder.encoder.layer.10.attention.self.query.weight\", \"encoder.encoder.layer.10.attention.self.query.bias\", \"encoder.encoder.layer.10.attention.self.key.weight\", \"encoder.encoder.layer.10.attention.self.key.bias\", \"encoder.encoder.layer.10.attention.self.value.weight\", \"encoder.encoder.layer.10.attention.self.value.bias\", \"encoder.encoder.layer.10.attention.output.dense.weight\", \"encoder.encoder.layer.10.attention.output.dense.bias\", \"encoder.encoder.layer.10.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.10.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.10.intermediate.dense.weight\", \"encoder.encoder.layer.10.intermediate.dense.bias\", \"encoder.encoder.layer.10.output.dense.weight\", \"encoder.encoder.layer.10.output.dense.bias\", \"encoder.encoder.layer.10.output.LayerNorm.weight\", \"encoder.encoder.layer.10.output.LayerNorm.bias\", \"encoder.encoder.layer.11.attention.self.query.weight\", \"encoder.encoder.layer.11.attention.self.query.bias\", \"encoder.encoder.layer.11.attention.self.key.weight\", \"encoder.encoder.layer.11.attention.self.key.bias\", \"encoder.encoder.layer.11.attention.self.value.weight\", \"encoder.encoder.layer.11.attention.self.value.bias\", \"encoder.encoder.layer.11.attention.output.dense.weight\", \"encoder.encoder.layer.11.attention.output.dense.bias\", \"encoder.encoder.layer.11.attention.output.LayerNorm.weight\", \"encoder.encoder.layer.11.attention.output.LayerNorm.bias\", \"encoder.encoder.layer.11.intermediate.dense.weight\", \"encoder.encoder.layer.11.intermediate.dense.bias\", \"encoder.encoder.layer.11.output.dense.weight\", \"encoder.encoder.layer.11.output.dense.bias\", \"encoder.encoder.layer.11.output.LayerNorm.weight\", \"encoder.encoder.layer.11.output.LayerNorm.bias\", \"encoder.pooler.dense.weight\", \"encoder.pooler.dense.bias\", \"decoder.bert.embeddings.position_ids\", \"decoder.bert.embeddings.word_embeddings.weight\", \"decoder.bert.embeddings.position_embeddings.weight\", \"decoder.bert.embeddings.token_type_embeddings.weight\", \"decoder.bert.embeddings.LayerNorm.weight\", \"decoder.bert.embeddings.LayerNorm.bias\", \"decoder.bert.encoder.layer.0.attention.self.query.weight\", \"decoder.bert.encoder.layer.0.attention.self.query.bias\", \"decoder.bert.encoder.layer.0.attention.self.key.weight\", \"decoder.bert.encoder.layer.0.attention.self.key.bias\", \"decoder.bert.encoder.layer.0.attention.self.value.weight\", \"decoder.bert.encoder.layer.0.attention.self.value.bias\", \"decoder.bert.encoder.layer.0.attention.output.dense.weight\", \"decoder.bert.encoder.layer.0.attention.output.dense.bias\", \"decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.0.crossattention.self.query.weight\", \"decoder.bert.encoder.layer.0.crossattention.self.query.bias\", \"decoder.bert.encoder.layer.0.crossattention.self.key.weight\", \"decoder.bert.encoder.layer.0.crossattention.self.key.bias\", \"decoder.bert.encoder.layer.0.crossattention.self.value.weight\", \"decoder.bert.encoder.layer.0.crossattention.self.value.bias\", \"decoder.bert.encoder.layer.0.crossattention.output.dense.weight\", \"decoder.bert.encoder.layer.0.crossattention.output.dense.bias\", \"decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.0.intermediate.dense.weight\", \"decoder.bert.encoder.layer.0.intermediate.dense.bias\", \"decoder.bert.encoder.layer.0.output.dense.weight\", \"decoder.bert.encoder.layer.0.output.dense.bias\", \"decoder.bert.encoder.layer.0.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.0.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.1.attention.self.query.weight\", \"decoder.bert.encoder.layer.1.attention.self.query.bias\", \"decoder.bert.encoder.layer.1.attention.self.key.weight\", \"decoder.bert.encoder.layer.1.attention.self.key.bias\", \"decoder.bert.encoder.layer.1.attention.self.value.weight\", \"decoder.bert.encoder.layer.1.attention.self.value.bias\", \"decoder.bert.encoder.layer.1.attention.output.dense.weight\", \"decoder.bert.encoder.layer.1.attention.output.dense.bias\", \"decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.1.crossattention.self.query.weight\", \"decoder.bert.encoder.layer.1.crossattention.self.query.bias\", \"decoder.bert.encoder.layer.1.crossattention.self.key.weight\", \"decoder.bert.encoder.layer.1.crossattention.self.key.bias\", \"decoder.bert.encoder.layer.1.crossattention.self.value.weight\", \"decoder.bert.encoder.layer.1.crossattention.self.value.bias\", \"decoder.bert.encoder.layer.1.crossattention.output.dense.weight\", \"decoder.bert.encoder.layer.1.crossattention.output.dense.bias\", \"decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.1.intermediate.dense.weight\", \"decoder.bert.encoder.layer.1.intermediate.dense.bias\", \"decoder.bert.encoder.layer.1.output.dense.weight\", \"decoder.bert.encoder.layer.1.output.dense.bias\", \"decoder.bert.encoder.layer.1.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.1.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.2.attention.self.query.weight\", \"decoder.bert.encoder.layer.2.attention.self.query.bias\", \"decoder.bert.encoder.layer.2.attention.self.key.weight\", \"decoder.bert.encoder.layer.2.attention.self.key.bias\", \"decoder.bert.encoder.layer.2.attention.self.value.weight\", \"decoder.bert.encoder.layer.2.attention.self.value.bias\", \"decoder.bert.encoder.layer.2.attention.output.dense.weight\", \"decoder.bert.encoder.layer.2.attention.output.dense.bias\", \"decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.2.crossattention.self.query.weight\", \"decoder.bert.encoder.layer.2.crossattention.self.query.bias\", \"decoder.bert.encoder.layer.2.crossattention.self.key.weight\", \"decoder.bert.encoder.layer.2.crossattention.self.key.bias\", \"decoder.bert.encoder.layer.2.crossattention.self.value.weight\", \"decoder.bert.encoder.layer.2.crossattention.self.value.bias\", \"decoder.bert.encoder.layer.2.crossattention.output.dense.weight\", \"decoder.bert.encoder.layer.2.crossattention.output.dense.bias\", \"decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.2.intermediate.dense.weight\", \"decoder.bert.encoder.layer.2.intermediate.dense.bias\", \"decoder.bert.encoder.layer.2.output.dense.weight\", \"decoder.bert.encoder.layer.2.output.dense.bias\", \"decoder.bert.encoder.layer.2.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.2.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.3.attention.self.query.weight\", \"decoder.bert.encoder.layer.3.attention.self.query.bias\", \"decoder.bert.encoder.layer.3.attention.self.key.weight\", \"decoder.bert.encoder.layer.3.attention.self.key.bias\", \"decoder.bert.encoder.layer.3.attention.self.value.weight\", \"decoder.bert.encoder.layer.3.attention.self.value.bias\", \"decoder.bert.encoder.layer.3.attention.output.dense.weight\", \"decoder.bert.encoder.layer.3.attention.output.dense.bias\", \"decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.3.crossattention.self.query.weight\", \"decoder.bert.encoder.layer.3.crossattention.self.query.bias\", \"decoder.bert.encoder.layer.3.crossattention.self.key.weight\", \"decoder.bert.encoder.layer.3.crossattention.self.key.bias\", \"decoder.bert.encoder.layer.3.crossattention.self.value.weight\", \"decoder.bert.encoder.layer.3.crossattention.self.value.bias\", \"decoder.bert.encoder.layer.3.crossattention.output.dense.weight\", \"decoder.bert.encoder.layer.3.crossattention.output.dense.bias\", \"decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.3.intermediate.dense.weight\", \"decoder.bert.encoder.layer.3.intermediate.dense.bias\", \"decoder.bert.encoder.layer.3.output.dense.weight\", \"decoder.bert.encoder.layer.3.output.dense.bias\", \"decoder.bert.encoder.layer.3.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.3.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.4.attention.self.query.weight\", \"decoder.bert.encoder.layer.4.attention.self.query.bias\", \"decoder.bert.encoder.layer.4.attention.self.key.weight\", \"decoder.bert.encoder.layer.4.attention.self.key.bias\", \"decoder.bert.encoder.layer.4.attention.self.value.weight\", \"decoder.bert.encoder.layer.4.attention.self.value.bias\", \"decoder.bert.encoder.layer.4.attention.output.dense.weight\", \"decoder.bert.encoder.layer.4.attention.output.dense.bias\", \"decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.4.crossattention.self.query.weight\", \"decoder.bert.encoder.layer.4.crossattention.self.query.bias\", \"decoder.bert.encoder.layer.4.crossattention.self.key.weight\", \"decoder.bert.encoder.layer.4.crossattention.self.key.bias\", \"decoder.bert.encoder.layer.4.crossattention.self.value.weight\", \"decoder.bert.encoder.layer.4.crossattention.self.value.bias\", \"decoder.bert.encoder.layer.4.crossattention.output.dense.weight\", \"decoder.bert.encoder.layer.4.crossattention.output.dense.bias\", \"decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.4.intermediate.dense.weight\", \"decoder.bert.encoder.layer.4.intermediate.dense.bias\", \"decoder.bert.encoder.layer.4.output.dense.weight\", \"decoder.bert.encoder.layer.4.output.dense.bias\", \"decoder.bert.encoder.layer.4.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.4.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.5.attention.self.query.weight\", \"decoder.bert.encoder.layer.5.attention.self.query.bias\", \"decoder.bert.encoder.layer.5.attention.self.key.weight\", \"decoder.bert.encoder.layer.5.attention.self.key.bias\", \"decoder.bert.encoder.layer.5.attention.self.value.weight\", \"decoder.bert.encoder.layer.5.attention.self.value.bias\", \"decoder.bert.encoder.layer.5.attention.output.dense.weight\", \"decoder.bert.encoder.layer.5.attention.output.dense.bias\", \"decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.5.crossattention.self.query.weight\", \"decoder.bert.encoder.layer.5.crossattention.self.query.bias\", \"decoder.bert.encoder.layer.5.crossattention.self.key.weight\", \"decoder.bert.encoder.layer.5.crossattention.self.key.bias\", \"decoder.bert.encoder.layer.5.crossattention.self.value.weight\", \"decoder.bert.encoder.layer.5.crossattention.self.value.bias\", \"decoder.bert.encoder.layer.5.crossattention.output.dense.weight\", \"decoder.bert.encoder.layer.5.crossattention.output.dense.bias\", \"decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.5.intermediate.dense.weight\", \"decoder.bert.encoder.layer.5.intermediate.dense.bias\", \"decoder.bert.encoder.layer.5.output.dense.weight\", \"decoder.bert.encoder.layer.5.output.dense.bias\", \"decoder.bert.encoder.layer.5.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.5.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.6.attention.self.query.weight\", \"decoder.bert.encoder.layer.6.attention.self.query.bias\", \"decoder.bert.encoder.layer.6.attention.self.key.weight\", \"decoder.bert.encoder.layer.6.attention.self.key.bias\", \"decoder.bert.encoder.layer.6.attention.self.value.weight\", \"decoder.bert.encoder.layer.6.attention.self.value.bias\", \"decoder.bert.encoder.layer.6.attention.output.dense.weight\", \"decoder.bert.encoder.layer.6.attention.output.dense.bias\", \"decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.6.crossattention.self.query.weight\", \"decoder.bert.encoder.layer.6.crossattention.self.query.bias\", \"decoder.bert.encoder.layer.6.crossattention.self.key.weight\", \"decoder.bert.encoder.layer.6.crossattention.self.key.bias\", \"decoder.bert.encoder.layer.6.crossattention.self.value.weight\", \"decoder.bert.encoder.layer.6.crossattention.self.value.bias\", \"decoder.bert.encoder.layer.6.crossattention.output.dense.weight\", \"decoder.bert.encoder.layer.6.crossattention.output.dense.bias\", \"decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.6.intermediate.dense.weight\", \"decoder.bert.encoder.layer.6.intermediate.dense.bias\", \"decoder.bert.encoder.layer.6.output.dense.weight\", \"decoder.bert.encoder.layer.6.output.dense.bias\", \"decoder.bert.encoder.layer.6.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.6.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.7.attention.self.query.weight\", \"decoder.bert.encoder.layer.7.attention.self.query.bias\", \"decoder.bert.encoder.layer.7.attention.self.key.weight\", \"decoder.bert.encoder.layer.7.attention.self.key.bias\", \"decoder.bert.encoder.layer.7.attention.self.value.weight\", \"decoder.bert.encoder.layer.7.attention.self.value.bias\", \"decoder.bert.encoder.layer.7.attention.output.dense.weight\", \"decoder.bert.encoder.layer.7.attention.output.dense.bias\", \"decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.7.crossattention.self.query.weight\", \"decoder.bert.encoder.layer.7.crossattention.self.query.bias\", \"decoder.bert.encoder.layer.7.crossattention.self.key.weight\", \"decoder.bert.encoder.layer.7.crossattention.self.key.bias\", \"decoder.bert.encoder.layer.7.crossattention.self.value.weight\", \"decoder.bert.encoder.layer.7.crossattention.self.value.bias\", \"decoder.bert.encoder.layer.7.crossattention.output.dense.weight\", \"decoder.bert.encoder.layer.7.crossattention.output.dense.bias\", \"decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.7.intermediate.dense.weight\", \"decoder.bert.encoder.layer.7.intermediate.dense.bias\", \"decoder.bert.encoder.layer.7.output.dense.weight\", \"decoder.bert.encoder.layer.7.output.dense.bias\", \"decoder.bert.encoder.layer.7.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.7.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.8.attention.self.query.weight\", \"decoder.bert.encoder.layer.8.attention.self.query.bias\", \"decoder.bert.encoder.layer.8.attention.self.key.weight\", \"decoder.bert.encoder.layer.8.attention.self.key.bias\", \"decoder.bert.encoder.layer.8.attention.self.value.weight\", \"decoder.bert.encoder.layer.8.attention.self.value.bias\", \"decoder.bert.encoder.layer.8.attention.output.dense.weight\", \"decoder.bert.encoder.layer.8.attention.output.dense.bias\", \"decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.8.crossattention.self.query.weight\", \"decoder.bert.encoder.layer.8.crossattention.self.query.bias\", \"decoder.bert.encoder.layer.8.crossattention.self.key.weight\", \"decoder.bert.encoder.layer.8.crossattention.self.key.bias\", \"decoder.bert.encoder.layer.8.crossattention.self.value.weight\", \"decoder.bert.encoder.layer.8.crossattention.self.value.bias\", \"decoder.bert.encoder.layer.8.crossattention.output.dense.weight\", \"decoder.bert.encoder.layer.8.crossattention.output.dense.bias\", \"decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.8.intermediate.dense.weight\", \"decoder.bert.encoder.layer.8.intermediate.dense.bias\", \"decoder.bert.encoder.layer.8.output.dense.weight\", \"decoder.bert.encoder.layer.8.output.dense.bias\", \"decoder.bert.encoder.layer.8.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.8.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.9.attention.self.query.weight\", \"decoder.bert.encoder.layer.9.attention.self.query.bias\", \"decoder.bert.encoder.layer.9.attention.self.key.weight\", \"decoder.bert.encoder.layer.9.attention.self.key.bias\", \"decoder.bert.encoder.layer.9.attention.self.value.weight\", \"decoder.bert.encoder.layer.9.attention.self.value.bias\", \"decoder.bert.encoder.layer.9.attention.output.dense.weight\", \"decoder.bert.encoder.layer.9.attention.output.dense.bias\", \"decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.9.crossattention.self.query.weight\", \"decoder.bert.encoder.layer.9.crossattention.self.query.bias\", \"decoder.bert.encoder.layer.9.crossattention.self.key.weight\", \"decoder.bert.encoder.layer.9.crossattention.self.key.bias\", \"decoder.bert.encoder.layer.9.crossattention.self.value.weight\", \"decoder.bert.encoder.layer.9.crossattention.self.value.bias\", \"decoder.bert.encoder.layer.9.crossattention.output.dense.weight\", \"decoder.bert.encoder.layer.9.crossattention.output.dense.bias\", \"decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.9.intermediate.dense.weight\", \"decoder.bert.encoder.layer.9.intermediate.dense.bias\", \"decoder.bert.encoder.layer.9.output.dense.weight\", \"decoder.bert.encoder.layer.9.output.dense.bias\", \"decoder.bert.encoder.layer.9.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.9.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.10.attention.self.query.weight\", \"decoder.bert.encoder.layer.10.attention.self.query.bias\", \"decoder.bert.encoder.layer.10.attention.self.key.weight\", \"decoder.bert.encoder.layer.10.attention.self.key.bias\", \"decoder.bert.encoder.layer.10.attention.self.value.weight\", \"decoder.bert.encoder.layer.10.attention.self.value.bias\", \"decoder.bert.encoder.layer.10.attention.output.dense.weight\", \"decoder.bert.encoder.layer.10.attention.output.dense.bias\", \"decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.10.crossattention.self.query.weight\", \"decoder.bert.encoder.layer.10.crossattention.self.query.bias\", \"decoder.bert.encoder.layer.10.crossattention.self.key.weight\", \"decoder.bert.encoder.layer.10.crossattention.self.key.bias\", \"decoder.bert.encoder.layer.10.crossattention.self.value.weight\", \"decoder.bert.encoder.layer.10.crossattention.self.value.bias\", \"decoder.bert.encoder.layer.10.crossattention.output.dense.weight\", \"decoder.bert.encoder.layer.10.crossattention.output.dense.bias\", \"decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.10.intermediate.dense.weight\", \"decoder.bert.encoder.layer.10.intermediate.dense.bias\", \"decoder.bert.encoder.layer.10.output.dense.weight\", \"decoder.bert.encoder.layer.10.output.dense.bias\", \"decoder.bert.encoder.layer.10.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.10.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.11.attention.self.query.weight\", \"decoder.bert.encoder.layer.11.attention.self.query.bias\", \"decoder.bert.encoder.layer.11.attention.self.key.weight\", \"decoder.bert.encoder.layer.11.attention.self.key.bias\", \"decoder.bert.encoder.layer.11.attention.self.value.weight\", \"decoder.bert.encoder.layer.11.attention.self.value.bias\", \"decoder.bert.encoder.layer.11.attention.output.dense.weight\", \"decoder.bert.encoder.layer.11.attention.output.dense.bias\", \"decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.11.crossattention.self.query.weight\", \"decoder.bert.encoder.layer.11.crossattention.self.query.bias\", \"decoder.bert.encoder.layer.11.crossattention.self.key.weight\", \"decoder.bert.encoder.layer.11.crossattention.self.key.bias\", \"decoder.bert.encoder.layer.11.crossattention.self.value.weight\", \"decoder.bert.encoder.layer.11.crossattention.self.value.bias\", \"decoder.bert.encoder.layer.11.crossattention.output.dense.weight\", \"decoder.bert.encoder.layer.11.crossattention.output.dense.bias\", \"decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias\", \"decoder.bert.encoder.layer.11.intermediate.dense.weight\", \"decoder.bert.encoder.layer.11.intermediate.dense.bias\", \"decoder.bert.encoder.layer.11.output.dense.weight\", \"decoder.bert.encoder.layer.11.output.dense.bias\", \"decoder.bert.encoder.layer.11.output.LayerNorm.weight\", \"decoder.bert.encoder.layer.11.output.LayerNorm.bias\", \"decoder.cls.predictions.bias\", \"decoder.cls.predictions.transform.dense.weight\", \"decoder.cls.predictions.transform.dense.bias\", \"decoder.cls.predictions.transform.LayerNorm.weight\", \"decoder.cls.predictions.transform.LayerNorm.bias\", \"decoder.cls.predictions.decoder.weight\", \"decoder.cls.predictions.decoder.bias\". \n"
          ]
        }
      ],
      "source": [
        "# infrence\n",
        "!python utils/text_2_emb.py \\\n",
        "--model_type bert \\\n",
        "--bert_model models/bert \\\n",
        "--text_path /path/to/카테고리별답변들.txt \\\n",
        "--output_dir /path/to/카테고리별embedding.pickle \\\n",
        "--gpu 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXvT31FJ6pt1"
      },
      "source": [
        "### 원본 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0jlAQDWUcRx"
      },
      "outputs": [],
      "source": [
        "parlai interactive -m transformer/polyencoder \\\n",
        "    -mf zoo:pretrained_transformers/model_poly/model \\\n",
        "    --encode-candidate-vecs true \\\n",
        "    --eval-candidates fixed  \\\n",
        "    --fixed-candidates-path data/models/pretrained_transformers/convai_trainset_cands.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4rhBueuqhW4"
      },
      "source": [
        "### 다른 git코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubdinMpPqlWU",
        "outputId": "bb54ed92-b536-459d-d907-5a8e149fa488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: You must specify a repository to clone.\n",
            "\n",
            "usage: git clone [<options>] [--] <repo> [<dir>]\n",
            "\n",
            "    -v, --verbose         be more verbose\n",
            "    -q, --quiet           be more quiet\n",
            "    --progress            force progress reporting\n",
            "    --reject-shallow      don't clone shallow repository\n",
            "    -n, --no-checkout     don't create a checkout\n",
            "    --bare                create a bare repository\n",
            "    --mirror              create a mirror repository (implies bare)\n",
            "    -l, --local           to clone from a local repository\n",
            "    --no-hardlinks        don't use local hardlinks, always copy\n",
            "    -s, --shared          setup as shared repository\n",
            "    --recurse-submodules[=<pathspec>]\n",
            "                          initialize submodules in the clone\n",
            "    --recursive ...       alias of --recurse-submodules\n",
            "    -j, --jobs <n>        number of submodules cloned in parallel\n",
            "    --template <template-directory>\n",
            "                          directory from which templates will be used\n",
            "    --reference <repo>    reference repository\n",
            "    --reference-if-able <repo>\n",
            "                          reference repository\n",
            "    --dissociate          use --reference only while cloning\n",
            "    -o, --origin <name>   use <name> instead of 'origin' to track upstream\n",
            "    -b, --branch <branch>\n",
            "                          checkout <branch> instead of the remote's HEAD\n",
            "    -u, --upload-pack <path>\n",
            "                          path to git-upload-pack on the remote\n",
            "    --depth <depth>       create a shallow clone of that depth\n",
            "    --shallow-since <time>\n",
            "                          create a shallow clone since a specific time\n",
            "    --shallow-exclude <revision>\n",
            "                          deepen history of shallow clone, excluding rev\n",
            "    --single-branch       clone only one branch, HEAD or --branch\n",
            "    --no-tags             don't clone any tags, and make later fetches not to follow them\n",
            "    --shallow-submodules  any cloned submodules will be shallow\n",
            "    --separate-git-dir <gitdir>\n",
            "                          separate git dir from working tree\n",
            "    -c, --config <key=value>\n",
            "                          set config inside the new repository\n",
            "    --server-option <server-specific>\n",
            "                          option to transmit\n",
            "    -4, --ipv4            use IPv4 addresses only\n",
            "    -6, --ipv6            use IPv6 addresses only\n",
            "    --filter <args>       object filtering\n",
            "    --remote-submodules   any cloned submodules will use their remote-tracking branch\n",
            "    --sparse              initialize sparse-checkout file to include only files at root\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git clone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JehVX2Mcq9tw"
      },
      "outputs": [],
      "source": [
        "python3 run.py --bert_model bert_model/ --output_dir output_dstc7/ --train_dir dstc7/ --use_pretrain --architecture poly --poly_m 16 --eval"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UsPUdwzL6k5h"
      ],
      "mount_file_id": "1FwZLjKk_QiqPTCa2p_V5lpTVIWl6H8WD",
      "authorship_tag": "ABX9TyPYGCO2naKjtnGX1hYoT4t2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}